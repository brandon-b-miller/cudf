//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-31057947
// Cuda compilation tools, release 11.6, V11.6.124
// Based on NVVM 7.0.1
//

.version 7.6
.target sm_52
.address_size 64

	// .globl	len
.const .align 1 .b8 __nv_static_23__48bda70c_7_shim_cu_len__ZN32_INTERNAL_48bda70c_7_shim_cu_len4cudf7strings6detail19max_string_sentinelE[5] = {247, 191, 191, 191, 0};
.global .align 1 .b8 __nv_static_23__48bda70c_7_shim_cu_len__ZN32_INTERNAL_48bda70c_7_shim_cu_len6thrust6system6detail10sequential3seqE[1];
.global .align 1 .b8 __nv_static_23__48bda70c_7_shim_cu_len__ZN32_INTERNAL_48bda70c_7_shim_cu_len6thrust6system3cpp3parE[1];
.global .align 1 .b8 __nv_static_23__48bda70c_7_shim_cu_len__ZN32_INTERNAL_48bda70c_7_shim_cu_len6thrust8cuda_cub3parE[1];
.global .align 1 .b8 __nv_static_23__48bda70c_7_shim_cu_len__ZN32_INTERNAL_48bda70c_7_shim_cu_len6thrust12placeholders2_1E[1];
.global .align 1 .b8 __nv_static_23__48bda70c_7_shim_cu_len__ZN32_INTERNAL_48bda70c_7_shim_cu_len6thrust12placeholders2_2E[1];
.global .align 1 .b8 __nv_static_23__48bda70c_7_shim_cu_len__ZN32_INTERNAL_48bda70c_7_shim_cu_len6thrust12placeholders2_3E[1];
.global .align 1 .b8 __nv_static_23__48bda70c_7_shim_cu_len__ZN32_INTERNAL_48bda70c_7_shim_cu_len6thrust12placeholders2_4E[1];
.global .align 1 .b8 __nv_static_23__48bda70c_7_shim_cu_len__ZN32_INTERNAL_48bda70c_7_shim_cu_len6thrust12placeholders2_5E[1];
.global .align 1 .b8 __nv_static_23__48bda70c_7_shim_cu_len__ZN32_INTERNAL_48bda70c_7_shim_cu_len6thrust12placeholders2_6E[1];
.global .align 1 .b8 __nv_static_23__48bda70c_7_shim_cu_len__ZN32_INTERNAL_48bda70c_7_shim_cu_len6thrust12placeholders2_7E[1];
.global .align 1 .b8 __nv_static_23__48bda70c_7_shim_cu_len__ZN32_INTERNAL_48bda70c_7_shim_cu_len6thrust12placeholders2_8E[1];
.global .align 1 .b8 __nv_static_23__48bda70c_7_shim_cu_len__ZN32_INTERNAL_48bda70c_7_shim_cu_len6thrust12placeholders2_9E[1];
.global .align 1 .b8 __nv_static_23__48bda70c_7_shim_cu_len__ZN32_INTERNAL_48bda70c_7_shim_cu_len6thrust12placeholders3_10E[1];
.global .align 1 .b8 __nv_static_23__48bda70c_7_shim_cu_len__ZN32_INTERNAL_48bda70c_7_shim_cu_len6thrust3seqE[1];
.global .align 1 .b8 __nv_static_23__48bda70c_7_shim_cu_len__ZN32_INTERNAL_48bda70c_7_shim_cu_len6thrust6deviceE[1];

.visible .func  (.param .b32 func_retval0) len(
	.param .b64 len_param_0,
	.param .b64 len_param_1
)
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<11>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<46>;


	ld.param.u64 	%rd23, [len_param_0];
	ld.param.u64 	%rd1, [len_param_1];
	ld.u32 	%r8, [%rd1+12];
	setp.ne.s32 	%p1, %r8, -1;
	@%p1 bra 	$L__BB0_10;

	ld.s32 	%rd2, [%rd1+8];
	ld.u64 	%rd41, [%rd1];
	setp.eq.s64 	%p2, %rd41, 0;
	setp.eq.s64 	%p3, %rd2, 0;
	mov.u32 	%r8, 0;
	or.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB0_9;

	add.s64 	%rd26, %rd2, -1;
	and.b64  	%rd4, %rd2, 3;
	setp.lt.u64 	%p5, %rd26, 3;
	mov.u64 	%rd45, 0;
	@%p5 bra 	$L__BB0_5;

	sub.s64 	%rd36, %rd4, %rd2;

$L__BB0_4:
	ld.u8 	%rs1, [%rd41];
	and.b16  	%rs2, %rs1, 192;
	setp.ne.s16 	%p6, %rs2, 128;
	selp.u64 	%rd28, 1, 0, %p6;
	add.s64 	%rd29, %rd45, %rd28;
	ld.u8 	%rs3, [%rd41+1];
	and.b16  	%rs4, %rs3, 192;
	setp.ne.s16 	%p7, %rs4, 128;
	selp.u64 	%rd30, 1, 0, %p7;
	add.s64 	%rd31, %rd29, %rd30;
	ld.u8 	%rs5, [%rd41+2];
	and.b16  	%rs6, %rs5, 192;
	setp.ne.s16 	%p8, %rs6, 128;
	selp.u64 	%rd32, 1, 0, %p8;
	add.s64 	%rd33, %rd31, %rd32;
	ld.u8 	%rs7, [%rd41+3];
	and.b16  	%rs8, %rs7, 192;
	setp.ne.s16 	%p9, %rs8, 128;
	selp.u64 	%rd34, 1, 0, %p9;
	add.s64 	%rd45, %rd33, %rd34;
	add.s64 	%rd41, %rd41, 4;
	add.s64 	%rd36, %rd36, 4;
	setp.ne.s64 	%p10, %rd36, 0;
	@%p10 bra 	$L__BB0_4;

$L__BB0_5:
	setp.eq.s64 	%p11, %rd4, 0;
	@%p11 bra 	$L__BB0_8;

	neg.s64 	%rd42, %rd4;

$L__BB0_7:
	.pragma "nounroll";
	ld.u8 	%rs9, [%rd41];
	and.b16  	%rs10, %rs9, 192;
	setp.ne.s16 	%p12, %rs10, 128;
	selp.u64 	%rd35, 1, 0, %p12;
	add.s64 	%rd45, %rd45, %rd35;
	add.s64 	%rd41, %rd41, 1;
	add.s64 	%rd42, %rd42, 1;
	setp.ne.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB0_7;

$L__BB0_8:
	cvt.u32.u64 	%r8, %rd45;

$L__BB0_9:
	st.u32 	[%rd1+12], %r8;

$L__BB0_10:
	st.u32 	[%rd23], %r8;
	mov.u32 	%r6, 0;
	st.param.b32 	[func_retval0+0], %r6;
	ret;

}
	// .globl	startswith
.visible .func  (.param .b32 func_retval0) startswith(
	.param .b64 startswith_param_0,
	.param .b64 startswith_param_1,
	.param .b64 startswith_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd7, [startswith_param_0];
	ld.param.u64 	%rd8, [startswith_param_1];
	ld.param.u64 	%rd9, [startswith_param_2];
	ld.u64 	%rd11, [%rd8];
	ld.u64 	%rd10, [%rd9];
	ld.u32 	%r1, [%rd9+8];
	ld.u32 	%r4, [%rd8+8];
	setp.lt.s32 	%p1, %r4, %r1;
	mov.u16 	%rs2, 0;
	mov.u16 	%rs8, %rs2;
	@%p1 bra 	$L__BB1_5;

	setp.eq.s64 	%p2, %rd11, %rd10;
	setp.lt.s32 	%p3, %r1, 1;
	mov.u16 	%rs3, 1;
	or.pred  	%p4, %p2, %p3;
	mov.u16 	%rs8, %rs3;
	@%p4 bra 	$L__BB1_5;

	mov.u32 	%r7, 0;

$L__BB1_3:
	ld.u8 	%rs5, [%rd10];
	ld.u8 	%rs6, [%rd11];
	setp.ne.s16 	%p5, %rs6, %rs5;
	mov.u16 	%rs8, %rs2;
	@%p5 bra 	$L__BB1_5;

	add.s64 	%rd11, %rd11, 1;
	add.s64 	%rd10, %rd10, 1;
	add.s32 	%r7, %r7, 1;
	setp.lt.s32 	%p6, %r7, %r1;
	mov.u16 	%rs8, %rs3;
	@%p6 bra 	$L__BB1_3;

$L__BB1_5:
	st.u8 	[%rd7], %rs8;
	mov.u32 	%r6, 0;
	st.param.b32 	[func_retval0+0], %r6;
	ret;

}
	// .globl	endswith
.visible .func  (.param .b32 func_retval0) endswith(
	.param .b64 endswith_param_0,
	.param .b64 endswith_param_1,
	.param .b64 endswith_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd8, [endswith_param_0];
	ld.param.u64 	%rd1, [endswith_param_1];
	ld.param.u64 	%rd9, [endswith_param_2];
	ld.u32 	%r1, [%rd1+8];
	ld.u64 	%rd14, [%rd9];
	ld.u32 	%r2, [%rd9+8];
	setp.lt.s32 	%p1, %r1, %r2;
	mov.u16 	%rs2, 0;
	mov.u16 	%rs8, %rs2;
	@%p1 bra 	$L__BB2_5;

	neg.s32 	%r5, %r2;
	cvt.s64.s32 	%rd10, %r5;
	cvt.s64.s32 	%rd11, %r1;
	add.s64 	%rd12, %rd10, %rd11;
	ld.u64 	%rd13, [%rd1];
	add.s64 	%rd15, %rd13, %rd12;
	setp.eq.s64 	%p2, %rd15, %rd14;
	setp.lt.s32 	%p3, %r2, 1;
	mov.u16 	%rs3, 1;
	or.pred  	%p4, %p2, %p3;
	mov.u16 	%rs8, %rs3;
	@%p4 bra 	$L__BB2_5;

	mov.u32 	%r8, 0;

$L__BB2_3:
	ld.u8 	%rs5, [%rd14];
	ld.u8 	%rs6, [%rd15];
	setp.ne.s16 	%p5, %rs6, %rs5;
	mov.u16 	%rs8, %rs2;
	@%p5 bra 	$L__BB2_5;

	add.s64 	%rd15, %rd15, 1;
	add.s64 	%rd14, %rd14, 1;
	add.s32 	%r8, %r8, 1;
	setp.lt.s32 	%p6, %r8, %r2;
	mov.u16 	%rs8, %rs3;
	@%p6 bra 	$L__BB2_3;

$L__BB2_5:
	st.u8 	[%rd8], %rs8;
	mov.u32 	%r7, 0;
	st.param.b32 	[func_retval0+0], %r7;
	ret;

}
	// .globl	contains
.visible .func  (.param .b32 func_retval0) contains(
	.param .b64 contains_param_0,
	.param .b64 contains_param_1,
	.param .b64 contains_param_2
)
{
	.reg .pred 	%p<88>;
	.reg .b16 	%rs<64>;
	.reg .b32 	%r<66>;
	.reg .b64 	%rd<242>;


	ld.param.u64 	%rd109, [contains_param_0];
	ld.param.u64 	%rd110, [contains_param_1];
	ld.param.u64 	%rd111, [contains_param_2];
	ld.u32 	%r1, [%rd111+8];
	ld.u64 	%rd1, [%rd111];
	setp.eq.s64 	%p1, %rd1, 0;
	mov.u32 	%r65, -1;
	@%p1 bra 	$L__BB3_63;

	ld.u32 	%r53, [%rd110+12];
	setp.eq.s32 	%p2, %r53, -1;
	@%p2 bra 	$L__BB3_3;

	ld.u64 	%rd236, [%rd110];
	ld.u32 	%r52, [%rd110+8];
	bra.uni 	$L__BB3_12;

$L__BB3_3:
	ld.u64 	%rd236, [%rd110];
	setp.eq.s64 	%p3, %rd236, 0;
	ld.u32 	%r52, [%rd110+8];
	setp.eq.s32 	%p4, %r52, 0;
	mov.u32 	%r53, 0;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB3_11;

	cvt.s64.s32 	%rd114, %r52;
	add.s64 	%rd115, %rd114, -1;
	setp.lt.u64 	%p6, %rd115, 3;
	mov.u64 	%rd197, 0;
	mov.u64 	%rd193, %rd236;
	@%p6 bra 	$L__BB3_7;

	and.b64  	%rd118, %rd114, 3;
	sub.s64 	%rd188, %rd118, %rd114;
	mov.u64 	%rd193, %rd236;

$L__BB3_6:
	ld.u8 	%rs7, [%rd193];
	and.b16  	%rs8, %rs7, 192;
	setp.ne.s16 	%p7, %rs8, 128;
	selp.u64 	%rd119, 1, 0, %p7;
	add.s64 	%rd120, %rd197, %rd119;
	ld.u8 	%rs9, [%rd193+1];
	and.b16  	%rs10, %rs9, 192;
	setp.ne.s16 	%p8, %rs10, 128;
	selp.u64 	%rd121, 1, 0, %p8;
	add.s64 	%rd122, %rd120, %rd121;
	ld.u8 	%rs11, [%rd193+2];
	and.b16  	%rs12, %rs11, 192;
	setp.ne.s16 	%p9, %rs12, 128;
	selp.u64 	%rd123, 1, 0, %p9;
	add.s64 	%rd124, %rd122, %rd123;
	ld.u8 	%rs13, [%rd193+3];
	and.b16  	%rs14, %rs13, 192;
	setp.ne.s16 	%p10, %rs14, 128;
	selp.u64 	%rd125, 1, 0, %p10;
	add.s64 	%rd197, %rd124, %rd125;
	add.s64 	%rd193, %rd193, 4;
	add.s64 	%rd188, %rd188, 4;
	setp.ne.s64 	%p11, %rd188, 0;
	@%p11 bra 	$L__BB3_6;

$L__BB3_7:
	cvt.u64.u32 	%rd126, %r52;
	and.b64  	%rd127, %rd126, 3;
	setp.eq.s64 	%p12, %rd127, 0;
	@%p12 bra 	$L__BB3_10;

	neg.s64 	%rd194, %rd127;

$L__BB3_9:
	.pragma "nounroll";
	ld.u8 	%rs15, [%rd193];
	and.b16  	%rs16, %rs15, 192;
	setp.ne.s16 	%p13, %rs16, 128;
	selp.u64 	%rd130, 1, 0, %p13;
	add.s64 	%rd197, %rd197, %rd130;
	add.s64 	%rd193, %rd193, 1;
	add.s64 	%rd194, %rd194, 1;
	setp.ne.s64 	%p14, %rd194, 0;
	@%p14 bra 	$L__BB3_9;

$L__BB3_10:
	cvt.u32.u64 	%r53, %rd197;

$L__BB3_11:
	st.u32 	[%rd110+12], %r53;

$L__BB3_12:
	setp.ne.s32 	%p15, %r53, -1;
	mov.u32 	%r57, %r53;
	@%p15 bra 	$L__BB3_22;

	setp.eq.s64 	%p16, %rd236, 0;
	setp.eq.s32 	%p17, %r52, 0;
	mov.u32 	%r57, 0;
	or.pred  	%p18, %p16, %p17;
	@%p18 bra 	$L__BB3_21;

	cvt.s64.s32 	%rd24, %r52;
	add.s64 	%rd133, %rd24, -1;
	and.b64  	%rd25, %rd24, 3;
	setp.lt.u64 	%p19, %rd133, 3;
	mov.u64 	%rd208, 0;
	mov.u64 	%rd204, %rd236;
	@%p19 bra 	$L__BB3_17;

	sub.s64 	%rd199, %rd25, %rd24;
	mov.u64 	%rd204, %rd236;

$L__BB3_16:
	ld.u8 	%rs17, [%rd204];
	and.b16  	%rs18, %rs17, 192;
	setp.ne.s16 	%p20, %rs18, 128;
	selp.u64 	%rd135, 1, 0, %p20;
	add.s64 	%rd136, %rd208, %rd135;
	ld.u8 	%rs19, [%rd204+1];
	and.b16  	%rs20, %rs19, 192;
	setp.ne.s16 	%p21, %rs20, 128;
	selp.u64 	%rd137, 1, 0, %p21;
	add.s64 	%rd138, %rd136, %rd137;
	ld.u8 	%rs21, [%rd204+2];
	and.b16  	%rs22, %rs21, 192;
	setp.ne.s16 	%p22, %rs22, 128;
	selp.u64 	%rd139, 1, 0, %p22;
	add.s64 	%rd140, %rd138, %rd139;
	ld.u8 	%rs23, [%rd204+3];
	and.b16  	%rs24, %rs23, 192;
	setp.ne.s16 	%p23, %rs24, 128;
	selp.u64 	%rd141, 1, 0, %p23;
	add.s64 	%rd208, %rd140, %rd141;
	add.s64 	%rd204, %rd204, 4;
	add.s64 	%rd199, %rd199, 4;
	setp.ne.s64 	%p24, %rd199, 0;
	@%p24 bra 	$L__BB3_16;

$L__BB3_17:
	setp.eq.s64 	%p25, %rd25, 0;
	@%p25 bra 	$L__BB3_20;

	neg.s64 	%rd205, %rd25;

$L__BB3_19:
	.pragma "nounroll";
	ld.u8 	%rs25, [%rd204];
	and.b16  	%rs26, %rs25, 192;
	setp.ne.s16 	%p26, %rs26, 128;
	selp.u64 	%rd142, 1, 0, %p26;
	add.s64 	%rd208, %rd208, %rd142;
	add.s64 	%rd204, %rd204, 1;
	add.s64 	%rd205, %rd205, 1;
	setp.ne.s64 	%p27, %rd205, 0;
	@%p27 bra 	$L__BB3_19;

$L__BB3_20:
	cvt.u32.u64 	%r57, %rd208;

$L__BB3_21:
	st.u32 	[%rd110+12], %r57;

$L__BB3_22:
	cvt.s64.s32 	%rd44, %r52;
	setp.ne.s32 	%p28, %r57, -1;
	@%p28 bra 	$L__BB3_32;

	setp.eq.s64 	%p29, %rd236, 0;
	setp.eq.s32 	%p30, %r52, 0;
	mov.u32 	%r57, 0;
	or.pred  	%p31, %p29, %p30;
	@%p31 bra 	$L__BB3_31;

	add.s64 	%rd145, %rd44, -1;
	and.b64  	%rd45, %rd44, 3;
	setp.lt.u64 	%p32, %rd145, 3;
	mov.u64 	%rd218, 0;
	mov.u64 	%rd213, %rd236;
	@%p32 bra 	$L__BB3_27;

	sub.s64 	%rd209, %rd45, %rd44;
	mov.u64 	%rd213, %rd236;

$L__BB3_26:
	ld.u8 	%rs27, [%rd213];
	and.b16  	%rs28, %rs27, 192;
	setp.ne.s16 	%p33, %rs28, 128;
	selp.u64 	%rd147, 1, 0, %p33;
	add.s64 	%rd148, %rd218, %rd147;
	ld.u8 	%rs29, [%rd213+1];
	and.b16  	%rs30, %rs29, 192;
	setp.ne.s16 	%p34, %rs30, 128;
	selp.u64 	%rd149, 1, 0, %p34;
	add.s64 	%rd150, %rd148, %rd149;
	ld.u8 	%rs31, [%rd213+2];
	and.b16  	%rs32, %rs31, 192;
	setp.ne.s16 	%p35, %rs32, 128;
	selp.u64 	%rd151, 1, 0, %p35;
	add.s64 	%rd152, %rd150, %rd151;
	ld.u8 	%rs33, [%rd213+3];
	and.b16  	%rs34, %rs33, 192;
	setp.ne.s16 	%p36, %rs34, 128;
	selp.u64 	%rd153, 1, 0, %p36;
	add.s64 	%rd218, %rd152, %rd153;
	add.s64 	%rd213, %rd213, 4;
	add.s64 	%rd209, %rd209, 4;
	setp.ne.s64 	%p37, %rd209, 0;
	@%p37 bra 	$L__BB3_26;

$L__BB3_27:
	setp.eq.s64 	%p38, %rd45, 0;
	@%p38 bra 	$L__BB3_30;

	neg.s64 	%rd215, %rd45;

$L__BB3_29:
	.pragma "nounroll";
	ld.u8 	%rs35, [%rd213];
	and.b16  	%rs36, %rs35, 192;
	setp.ne.s16 	%p39, %rs36, 128;
	selp.u64 	%rd154, 1, 0, %p39;
	add.s64 	%rd218, %rd218, %rd154;
	add.s64 	%rd213, %rd213, 1;
	add.s64 	%rd215, %rd215, 1;
	setp.ne.s64 	%p40, %rd215, 0;
	@%p40 bra 	$L__BB3_29;

$L__BB3_30:
	cvt.u32.u64 	%r57, %rd218;

$L__BB3_31:
	st.u32 	[%rd110+12], %r57;

$L__BB3_32:
	setp.eq.s32 	%p41, %r57, %r52;
	mov.u32 	%r60, %r53;
	@%p41 bra 	$L__BB3_36;

	setp.lt.s32 	%p42, %r53, 1;
	mov.u32 	%r60, 0;
	setp.lt.s32 	%p43, %r52, 1;
	or.pred  	%p44, %p42, %p43;
	@%p44 bra 	$L__BB3_36;

	add.s64 	%rd64, %rd236, %rd44;
	mov.u64 	%rd219, %rd236;

$L__BB3_35:
	ld.u8 	%rs37, [%rd219];
	setp.gt.u16 	%p45, %rs37, 239;
	selp.b32 	%r36, 2, 1, %p45;
	setp.gt.u16 	%p46, %rs37, 223;
	selp.u32 	%r37, 1, 0, %p46;
	add.s32 	%r38, %r36, %r37;
	setp.gt.u16 	%p47, %rs37, 191;
	selp.u32 	%r39, 1, 0, %p47;
	add.s32 	%r40, %r38, %r39;
	and.b16  	%rs38, %rs37, 192;
	setp.eq.s16 	%p48, %rs38, 128;
	selp.b32 	%r41, -1, 0, %p48;
	add.s32 	%r42, %r40, %r41;
	setp.ne.s32 	%p49, %r42, 0;
	selp.b32 	%r43, -1, 0, %p49;
	add.s32 	%r53, %r53, %r43;
	add.s32 	%r60, %r42, %r60;
	setp.gt.s32 	%p50, %r53, 0;
	add.s64 	%rd219, %rd219, 1;
	setp.lt.u64 	%p51, %rd219, %rd64;
	and.pred  	%p52, %p50, %p51;
	@%p52 bra 	$L__BB3_35;

$L__BB3_36:
	sub.s32 	%r20, %r60, %r1;
	setp.lt.s32 	%p53, %r20, 0;
	@%p53 bra 	$L__BB3_63;

	mov.u32 	%r61, 0;
	mov.u16 	%rs62, 0;
	mov.u64 	%rd220, 0;
	setp.lt.s32 	%p54, %r1, 1;
	mov.u16 	%rs40, 1;
	mov.u64 	%rd221, %rd236;

$L__BB3_38:
	mov.u16 	%rs63, %rs40;
	@%p54 bra 	$L__BB3_42;

	mov.u32 	%r62, 0;

$L__BB3_40:
	cvt.s64.s32 	%rd156, %r62;
	add.s64 	%rd157, %rd221, %rd156;
	add.s64 	%rd158, %rd1, %rd156;
	ld.u8 	%rs2, [%rd158];
	ld.u8 	%rs3, [%rd157];
	setp.eq.s16 	%p55, %rs3, %rs2;
	add.s32 	%r62, %r62, 1;
	setp.lt.s32 	%p56, %r62, %r1;
	and.pred  	%p57, %p55, %p56;
	@%p57 bra 	$L__BB3_40;

	selp.u16 	%rs63, 1, 0, %p55;

$L__BB3_42:
	setp.eq.s16 	%p59, %rs63, 0;
	@%p59 bra 	$L__BB3_62;
	bra.uni 	$L__BB3_43;

$L__BB3_62:
	add.s64 	%rd221, %rd221, 1;
	add.s32 	%r28, %r61, 1;
	add.s16 	%rs62, %rs62, 1;
	add.s64 	%rd220, %rd220, -1;
	setp.lt.s32 	%p86, %r61, %r20;
	mov.u32 	%r61, %r28;
	@%p86 bra 	$L__BB3_38;
	bra.uni 	$L__BB3_63;

$L__BB3_43:
	setp.ne.s32 	%p60, %r57, -1;
	@%p60 bra 	$L__BB3_53;

	setp.eq.s64 	%p61, %rd236, 0;
	setp.eq.s32 	%p62, %r52, 0;
	mov.u32 	%r57, 0;
	or.pred  	%p63, %p61, %p62;
	@%p63 bra 	$L__BB3_52;

	add.s64 	%rd161, %rd44, -1;
	and.b64  	%rd69, %rd44, 3;
	setp.lt.u64 	%p64, %rd161, 3;
	mov.u64 	%rd231, 0;
	mov.u64 	%rd226, %rd236;
	@%p64 bra 	$L__BB3_48;

	sub.s64 	%rd222, %rd69, %rd44;
	mov.u64 	%rd226, %rd236;

$L__BB3_47:
	ld.u8 	%rs41, [%rd226];
	and.b16  	%rs42, %rs41, 192;
	setp.ne.s16 	%p65, %rs42, 128;
	selp.u64 	%rd163, 1, 0, %p65;
	add.s64 	%rd164, %rd231, %rd163;
	ld.u8 	%rs43, [%rd226+1];
	and.b16  	%rs44, %rs43, 192;
	setp.ne.s16 	%p66, %rs44, 128;
	selp.u64 	%rd165, 1, 0, %p66;
	add.s64 	%rd166, %rd164, %rd165;
	ld.u8 	%rs45, [%rd226+2];
	and.b16  	%rs46, %rs45, 192;
	setp.ne.s16 	%p67, %rs46, 128;
	selp.u64 	%rd167, 1, 0, %p67;
	add.s64 	%rd168, %rd166, %rd167;
	ld.u8 	%rs47, [%rd226+3];
	and.b16  	%rs48, %rs47, 192;
	setp.ne.s16 	%p68, %rs48, 128;
	selp.u64 	%rd169, 1, 0, %p68;
	add.s64 	%rd231, %rd168, %rd169;
	add.s64 	%rd226, %rd226, 4;
	add.s64 	%rd222, %rd222, 4;
	setp.ne.s64 	%p69, %rd222, 0;
	@%p69 bra 	$L__BB3_47;

$L__BB3_48:
	setp.eq.s64 	%p70, %rd69, 0;
	@%p70 bra 	$L__BB3_51;

	neg.s64 	%rd228, %rd69;

$L__BB3_50:
	.pragma "nounroll";
	ld.u8 	%rs49, [%rd226];
	and.b16  	%rs50, %rs49, 192;
	setp.ne.s16 	%p71, %rs50, 128;
	selp.u64 	%rd170, 1, 0, %p71;
	add.s64 	%rd231, %rd231, %rd170;
	add.s64 	%rd226, %rd226, 1;
	add.s64 	%rd228, %rd228, 1;
	setp.ne.s64 	%p72, %rd228, 0;
	@%p72 bra 	$L__BB3_50;

$L__BB3_51:
	cvt.u32.u64 	%r57, %rd231;

$L__BB3_52:
	st.u32 	[%rd110+12], %r57;

$L__BB3_53:
	setp.eq.s32 	%p73, %r57, %r52;
	mov.u32 	%r65, %r61;
	@%p73 bra 	$L__BB3_63;

	setp.eq.s64 	%p74, %rd236, 0;
	setp.eq.s32 	%p75, %r61, 0;
	mov.u32 	%r65, 0;
	or.pred  	%p76, %p75, %p74;
	@%p76 bra 	$L__BB3_63;

	cvt.s64.s32 	%rd88, %r61;
	add.s64 	%rd173, %rd88, -1;
	setp.lt.u64 	%p77, %rd173, 3;
	mov.u64 	%rd241, 0;
	@%p77 bra 	$L__BB3_58;

	cvt.u64.u16 	%rd175, %rs62;
	and.b64  	%rd176, %rd175, 3;
	add.s64 	%rd232, %rd220, %rd176;

$L__BB3_57:
	ld.u8 	%rs51, [%rd236];
	and.b16  	%rs52, %rs51, 192;
	setp.ne.s16 	%p78, %rs52, 128;
	selp.u64 	%rd177, 1, 0, %p78;
	add.s64 	%rd178, %rd241, %rd177;
	ld.u8 	%rs53, [%rd236+1];
	and.b16  	%rs54, %rs53, 192;
	setp.ne.s16 	%p79, %rs54, 128;
	selp.u64 	%rd179, 1, 0, %p79;
	add.s64 	%rd180, %rd178, %rd179;
	ld.u8 	%rs55, [%rd236+2];
	and.b16  	%rs56, %rs55, 192;
	setp.ne.s16 	%p80, %rs56, 128;
	selp.u64 	%rd181, 1, 0, %p80;
	add.s64 	%rd182, %rd180, %rd181;
	ld.u8 	%rs57, [%rd236+3];
	and.b16  	%rs58, %rs57, 192;
	setp.ne.s16 	%p81, %rs58, 128;
	selp.u64 	%rd183, 1, 0, %p81;
	add.s64 	%rd241, %rd182, %rd183;
	add.s64 	%rd236, %rd236, 4;
	add.s64 	%rd232, %rd232, 4;
	setp.ne.s64 	%p82, %rd232, 0;
	@%p82 bra 	$L__BB3_57;

$L__BB3_58:
	and.b64  	%rd184, %rd88, 3;
	setp.eq.s64 	%p83, %rd184, 0;
	@%p83 bra 	$L__BB3_61;

	cvt.u64.u16 	%rd185, %rs62;
	and.b64  	%rd186, %rd185, 3;
	neg.s64 	%rd238, %rd186;

$L__BB3_60:
	.pragma "nounroll";
	ld.u8 	%rs59, [%rd236];
	and.b16  	%rs60, %rs59, 192;
	setp.ne.s16 	%p84, %rs60, 128;
	selp.u64 	%rd187, 1, 0, %p84;
	add.s64 	%rd241, %rd241, %rd187;
	add.s64 	%rd236, %rd236, 1;
	add.s64 	%rd238, %rd238, 1;
	setp.ne.s64 	%p85, %rd238, 0;
	@%p85 bra 	$L__BB3_60;

$L__BB3_61:
	cvt.u32.u64 	%r65, %rd241;

$L__BB3_63:
	setp.ne.s32 	%p87, %r65, -1;
	selp.u16 	%rs61, 1, 0, %p87;
	st.u8 	[%rd109], %rs61;
	mov.u32 	%r50, 0;
	st.param.b32 	[func_retval0+0], %r50;
	ret;

}
	// .globl	find
.visible .func  (.param .b32 func_retval0) find(
	.param .b64 find_param_0,
	.param .b64 find_param_1,
	.param .b64 find_param_2
)
{
	.reg .pred 	%p<87>;
	.reg .b16 	%rs<63>;
	.reg .b32 	%r<66>;
	.reg .b64 	%rd<242>;


	ld.param.u64 	%rd109, [find_param_0];
	ld.param.u64 	%rd110, [find_param_1];
	ld.param.u64 	%rd111, [find_param_2];
	ld.u32 	%r1, [%rd111+8];
	ld.u64 	%rd1, [%rd111];
	setp.eq.s64 	%p1, %rd1, 0;
	mov.u32 	%r65, -1;
	@%p1 bra 	$L__BB4_63;

	ld.u32 	%r53, [%rd110+12];
	setp.eq.s32 	%p2, %r53, -1;
	@%p2 bra 	$L__BB4_3;

	ld.u64 	%rd236, [%rd110];
	ld.u32 	%r52, [%rd110+8];
	bra.uni 	$L__BB4_12;

$L__BB4_3:
	ld.u64 	%rd236, [%rd110];
	setp.eq.s64 	%p3, %rd236, 0;
	ld.u32 	%r52, [%rd110+8];
	setp.eq.s32 	%p4, %r52, 0;
	mov.u32 	%r53, 0;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB4_11;

	cvt.s64.s32 	%rd114, %r52;
	add.s64 	%rd115, %rd114, -1;
	setp.lt.u64 	%p6, %rd115, 3;
	mov.u64 	%rd197, 0;
	mov.u64 	%rd193, %rd236;
	@%p6 bra 	$L__BB4_7;

	and.b64  	%rd118, %rd114, 3;
	sub.s64 	%rd188, %rd118, %rd114;
	mov.u64 	%rd193, %rd236;

$L__BB4_6:
	ld.u8 	%rs7, [%rd193];
	and.b16  	%rs8, %rs7, 192;
	setp.ne.s16 	%p7, %rs8, 128;
	selp.u64 	%rd119, 1, 0, %p7;
	add.s64 	%rd120, %rd197, %rd119;
	ld.u8 	%rs9, [%rd193+1];
	and.b16  	%rs10, %rs9, 192;
	setp.ne.s16 	%p8, %rs10, 128;
	selp.u64 	%rd121, 1, 0, %p8;
	add.s64 	%rd122, %rd120, %rd121;
	ld.u8 	%rs11, [%rd193+2];
	and.b16  	%rs12, %rs11, 192;
	setp.ne.s16 	%p9, %rs12, 128;
	selp.u64 	%rd123, 1, 0, %p9;
	add.s64 	%rd124, %rd122, %rd123;
	ld.u8 	%rs13, [%rd193+3];
	and.b16  	%rs14, %rs13, 192;
	setp.ne.s16 	%p10, %rs14, 128;
	selp.u64 	%rd125, 1, 0, %p10;
	add.s64 	%rd197, %rd124, %rd125;
	add.s64 	%rd193, %rd193, 4;
	add.s64 	%rd188, %rd188, 4;
	setp.ne.s64 	%p11, %rd188, 0;
	@%p11 bra 	$L__BB4_6;

$L__BB4_7:
	cvt.u64.u32 	%rd126, %r52;
	and.b64  	%rd127, %rd126, 3;
	setp.eq.s64 	%p12, %rd127, 0;
	@%p12 bra 	$L__BB4_10;

	neg.s64 	%rd194, %rd127;

$L__BB4_9:
	.pragma "nounroll";
	ld.u8 	%rs15, [%rd193];
	and.b16  	%rs16, %rs15, 192;
	setp.ne.s16 	%p13, %rs16, 128;
	selp.u64 	%rd130, 1, 0, %p13;
	add.s64 	%rd197, %rd197, %rd130;
	add.s64 	%rd193, %rd193, 1;
	add.s64 	%rd194, %rd194, 1;
	setp.ne.s64 	%p14, %rd194, 0;
	@%p14 bra 	$L__BB4_9;

$L__BB4_10:
	cvt.u32.u64 	%r53, %rd197;

$L__BB4_11:
	st.u32 	[%rd110+12], %r53;

$L__BB4_12:
	setp.ne.s32 	%p15, %r53, -1;
	mov.u32 	%r57, %r53;
	@%p15 bra 	$L__BB4_22;

	setp.eq.s64 	%p16, %rd236, 0;
	setp.eq.s32 	%p17, %r52, 0;
	mov.u32 	%r57, 0;
	or.pred  	%p18, %p16, %p17;
	@%p18 bra 	$L__BB4_21;

	cvt.s64.s32 	%rd24, %r52;
	add.s64 	%rd133, %rd24, -1;
	and.b64  	%rd25, %rd24, 3;
	setp.lt.u64 	%p19, %rd133, 3;
	mov.u64 	%rd208, 0;
	mov.u64 	%rd204, %rd236;
	@%p19 bra 	$L__BB4_17;

	sub.s64 	%rd199, %rd25, %rd24;
	mov.u64 	%rd204, %rd236;

$L__BB4_16:
	ld.u8 	%rs17, [%rd204];
	and.b16  	%rs18, %rs17, 192;
	setp.ne.s16 	%p20, %rs18, 128;
	selp.u64 	%rd135, 1, 0, %p20;
	add.s64 	%rd136, %rd208, %rd135;
	ld.u8 	%rs19, [%rd204+1];
	and.b16  	%rs20, %rs19, 192;
	setp.ne.s16 	%p21, %rs20, 128;
	selp.u64 	%rd137, 1, 0, %p21;
	add.s64 	%rd138, %rd136, %rd137;
	ld.u8 	%rs21, [%rd204+2];
	and.b16  	%rs22, %rs21, 192;
	setp.ne.s16 	%p22, %rs22, 128;
	selp.u64 	%rd139, 1, 0, %p22;
	add.s64 	%rd140, %rd138, %rd139;
	ld.u8 	%rs23, [%rd204+3];
	and.b16  	%rs24, %rs23, 192;
	setp.ne.s16 	%p23, %rs24, 128;
	selp.u64 	%rd141, 1, 0, %p23;
	add.s64 	%rd208, %rd140, %rd141;
	add.s64 	%rd204, %rd204, 4;
	add.s64 	%rd199, %rd199, 4;
	setp.ne.s64 	%p24, %rd199, 0;
	@%p24 bra 	$L__BB4_16;

$L__BB4_17:
	setp.eq.s64 	%p25, %rd25, 0;
	@%p25 bra 	$L__BB4_20;

	neg.s64 	%rd205, %rd25;

$L__BB4_19:
	.pragma "nounroll";
	ld.u8 	%rs25, [%rd204];
	and.b16  	%rs26, %rs25, 192;
	setp.ne.s16 	%p26, %rs26, 128;
	selp.u64 	%rd142, 1, 0, %p26;
	add.s64 	%rd208, %rd208, %rd142;
	add.s64 	%rd204, %rd204, 1;
	add.s64 	%rd205, %rd205, 1;
	setp.ne.s64 	%p27, %rd205, 0;
	@%p27 bra 	$L__BB4_19;

$L__BB4_20:
	cvt.u32.u64 	%r57, %rd208;

$L__BB4_21:
	st.u32 	[%rd110+12], %r57;

$L__BB4_22:
	cvt.s64.s32 	%rd44, %r52;
	setp.ne.s32 	%p28, %r57, -1;
	@%p28 bra 	$L__BB4_32;

	setp.eq.s64 	%p29, %rd236, 0;
	setp.eq.s32 	%p30, %r52, 0;
	mov.u32 	%r57, 0;
	or.pred  	%p31, %p29, %p30;
	@%p31 bra 	$L__BB4_31;

	add.s64 	%rd145, %rd44, -1;
	and.b64  	%rd45, %rd44, 3;
	setp.lt.u64 	%p32, %rd145, 3;
	mov.u64 	%rd218, 0;
	mov.u64 	%rd213, %rd236;
	@%p32 bra 	$L__BB4_27;

	sub.s64 	%rd209, %rd45, %rd44;
	mov.u64 	%rd213, %rd236;

$L__BB4_26:
	ld.u8 	%rs27, [%rd213];
	and.b16  	%rs28, %rs27, 192;
	setp.ne.s16 	%p33, %rs28, 128;
	selp.u64 	%rd147, 1, 0, %p33;
	add.s64 	%rd148, %rd218, %rd147;
	ld.u8 	%rs29, [%rd213+1];
	and.b16  	%rs30, %rs29, 192;
	setp.ne.s16 	%p34, %rs30, 128;
	selp.u64 	%rd149, 1, 0, %p34;
	add.s64 	%rd150, %rd148, %rd149;
	ld.u8 	%rs31, [%rd213+2];
	and.b16  	%rs32, %rs31, 192;
	setp.ne.s16 	%p35, %rs32, 128;
	selp.u64 	%rd151, 1, 0, %p35;
	add.s64 	%rd152, %rd150, %rd151;
	ld.u8 	%rs33, [%rd213+3];
	and.b16  	%rs34, %rs33, 192;
	setp.ne.s16 	%p36, %rs34, 128;
	selp.u64 	%rd153, 1, 0, %p36;
	add.s64 	%rd218, %rd152, %rd153;
	add.s64 	%rd213, %rd213, 4;
	add.s64 	%rd209, %rd209, 4;
	setp.ne.s64 	%p37, %rd209, 0;
	@%p37 bra 	$L__BB4_26;

$L__BB4_27:
	setp.eq.s64 	%p38, %rd45, 0;
	@%p38 bra 	$L__BB4_30;

	neg.s64 	%rd215, %rd45;

$L__BB4_29:
	.pragma "nounroll";
	ld.u8 	%rs35, [%rd213];
	and.b16  	%rs36, %rs35, 192;
	setp.ne.s16 	%p39, %rs36, 128;
	selp.u64 	%rd154, 1, 0, %p39;
	add.s64 	%rd218, %rd218, %rd154;
	add.s64 	%rd213, %rd213, 1;
	add.s64 	%rd215, %rd215, 1;
	setp.ne.s64 	%p40, %rd215, 0;
	@%p40 bra 	$L__BB4_29;

$L__BB4_30:
	cvt.u32.u64 	%r57, %rd218;

$L__BB4_31:
	st.u32 	[%rd110+12], %r57;

$L__BB4_32:
	setp.eq.s32 	%p41, %r57, %r52;
	mov.u32 	%r60, %r53;
	@%p41 bra 	$L__BB4_36;

	setp.lt.s32 	%p42, %r53, 1;
	mov.u32 	%r60, 0;
	setp.lt.s32 	%p43, %r52, 1;
	or.pred  	%p44, %p42, %p43;
	@%p44 bra 	$L__BB4_36;

	add.s64 	%rd64, %rd236, %rd44;
	mov.u64 	%rd219, %rd236;

$L__BB4_35:
	ld.u8 	%rs37, [%rd219];
	setp.gt.u16 	%p45, %rs37, 239;
	selp.b32 	%r36, 2, 1, %p45;
	setp.gt.u16 	%p46, %rs37, 223;
	selp.u32 	%r37, 1, 0, %p46;
	add.s32 	%r38, %r36, %r37;
	setp.gt.u16 	%p47, %rs37, 191;
	selp.u32 	%r39, 1, 0, %p47;
	add.s32 	%r40, %r38, %r39;
	and.b16  	%rs38, %rs37, 192;
	setp.eq.s16 	%p48, %rs38, 128;
	selp.b32 	%r41, -1, 0, %p48;
	add.s32 	%r42, %r40, %r41;
	setp.ne.s32 	%p49, %r42, 0;
	selp.b32 	%r43, -1, 0, %p49;
	add.s32 	%r53, %r53, %r43;
	add.s32 	%r60, %r42, %r60;
	setp.gt.s32 	%p50, %r53, 0;
	add.s64 	%rd219, %rd219, 1;
	setp.lt.u64 	%p51, %rd219, %rd64;
	and.pred  	%p52, %p50, %p51;
	@%p52 bra 	$L__BB4_35;

$L__BB4_36:
	sub.s32 	%r20, %r60, %r1;
	setp.lt.s32 	%p53, %r20, 0;
	@%p53 bra 	$L__BB4_63;

	mov.u32 	%r61, 0;
	mov.u16 	%rs61, 0;
	mov.u64 	%rd220, 0;
	setp.lt.s32 	%p54, %r1, 1;
	mov.u16 	%rs40, 1;
	mov.u64 	%rd221, %rd236;

$L__BB4_38:
	mov.u16 	%rs62, %rs40;
	@%p54 bra 	$L__BB4_42;

	mov.u32 	%r62, 0;

$L__BB4_40:
	cvt.s64.s32 	%rd156, %r62;
	add.s64 	%rd157, %rd221, %rd156;
	add.s64 	%rd158, %rd1, %rd156;
	ld.u8 	%rs2, [%rd158];
	ld.u8 	%rs3, [%rd157];
	setp.eq.s16 	%p55, %rs3, %rs2;
	add.s32 	%r62, %r62, 1;
	setp.lt.s32 	%p56, %r62, %r1;
	and.pred  	%p57, %p55, %p56;
	@%p57 bra 	$L__BB4_40;

	selp.u16 	%rs62, 1, 0, %p55;

$L__BB4_42:
	setp.eq.s16 	%p59, %rs62, 0;
	@%p59 bra 	$L__BB4_62;
	bra.uni 	$L__BB4_43;

$L__BB4_62:
	add.s64 	%rd221, %rd221, 1;
	add.s32 	%r28, %r61, 1;
	add.s16 	%rs61, %rs61, 1;
	add.s64 	%rd220, %rd220, -1;
	setp.lt.s32 	%p86, %r61, %r20;
	mov.u32 	%r61, %r28;
	@%p86 bra 	$L__BB4_38;
	bra.uni 	$L__BB4_63;

$L__BB4_43:
	setp.ne.s32 	%p60, %r57, -1;
	@%p60 bra 	$L__BB4_53;

	setp.eq.s64 	%p61, %rd236, 0;
	setp.eq.s32 	%p62, %r52, 0;
	mov.u32 	%r57, 0;
	or.pred  	%p63, %p61, %p62;
	@%p63 bra 	$L__BB4_52;

	add.s64 	%rd161, %rd44, -1;
	and.b64  	%rd69, %rd44, 3;
	setp.lt.u64 	%p64, %rd161, 3;
	mov.u64 	%rd231, 0;
	mov.u64 	%rd226, %rd236;
	@%p64 bra 	$L__BB4_48;

	sub.s64 	%rd222, %rd69, %rd44;
	mov.u64 	%rd226, %rd236;

$L__BB4_47:
	ld.u8 	%rs41, [%rd226];
	and.b16  	%rs42, %rs41, 192;
	setp.ne.s16 	%p65, %rs42, 128;
	selp.u64 	%rd163, 1, 0, %p65;
	add.s64 	%rd164, %rd231, %rd163;
	ld.u8 	%rs43, [%rd226+1];
	and.b16  	%rs44, %rs43, 192;
	setp.ne.s16 	%p66, %rs44, 128;
	selp.u64 	%rd165, 1, 0, %p66;
	add.s64 	%rd166, %rd164, %rd165;
	ld.u8 	%rs45, [%rd226+2];
	and.b16  	%rs46, %rs45, 192;
	setp.ne.s16 	%p67, %rs46, 128;
	selp.u64 	%rd167, 1, 0, %p67;
	add.s64 	%rd168, %rd166, %rd167;
	ld.u8 	%rs47, [%rd226+3];
	and.b16  	%rs48, %rs47, 192;
	setp.ne.s16 	%p68, %rs48, 128;
	selp.u64 	%rd169, 1, 0, %p68;
	add.s64 	%rd231, %rd168, %rd169;
	add.s64 	%rd226, %rd226, 4;
	add.s64 	%rd222, %rd222, 4;
	setp.ne.s64 	%p69, %rd222, 0;
	@%p69 bra 	$L__BB4_47;

$L__BB4_48:
	setp.eq.s64 	%p70, %rd69, 0;
	@%p70 bra 	$L__BB4_51;

	neg.s64 	%rd228, %rd69;

$L__BB4_50:
	.pragma "nounroll";
	ld.u8 	%rs49, [%rd226];
	and.b16  	%rs50, %rs49, 192;
	setp.ne.s16 	%p71, %rs50, 128;
	selp.u64 	%rd170, 1, 0, %p71;
	add.s64 	%rd231, %rd231, %rd170;
	add.s64 	%rd226, %rd226, 1;
	add.s64 	%rd228, %rd228, 1;
	setp.ne.s64 	%p72, %rd228, 0;
	@%p72 bra 	$L__BB4_50;

$L__BB4_51:
	cvt.u32.u64 	%r57, %rd231;

$L__BB4_52:
	st.u32 	[%rd110+12], %r57;

$L__BB4_53:
	setp.eq.s32 	%p73, %r57, %r52;
	mov.u32 	%r65, %r61;
	@%p73 bra 	$L__BB4_63;

	setp.eq.s64 	%p74, %rd236, 0;
	setp.eq.s32 	%p75, %r61, 0;
	mov.u32 	%r65, 0;
	or.pred  	%p76, %p75, %p74;
	@%p76 bra 	$L__BB4_63;

	cvt.s64.s32 	%rd88, %r61;
	add.s64 	%rd173, %rd88, -1;
	setp.lt.u64 	%p77, %rd173, 3;
	mov.u64 	%rd241, 0;
	@%p77 bra 	$L__BB4_58;

	cvt.u64.u16 	%rd175, %rs61;
	and.b64  	%rd176, %rd175, 3;
	add.s64 	%rd232, %rd220, %rd176;

$L__BB4_57:
	ld.u8 	%rs51, [%rd236];
	and.b16  	%rs52, %rs51, 192;
	setp.ne.s16 	%p78, %rs52, 128;
	selp.u64 	%rd177, 1, 0, %p78;
	add.s64 	%rd178, %rd241, %rd177;
	ld.u8 	%rs53, [%rd236+1];
	and.b16  	%rs54, %rs53, 192;
	setp.ne.s16 	%p79, %rs54, 128;
	selp.u64 	%rd179, 1, 0, %p79;
	add.s64 	%rd180, %rd178, %rd179;
	ld.u8 	%rs55, [%rd236+2];
	and.b16  	%rs56, %rs55, 192;
	setp.ne.s16 	%p80, %rs56, 128;
	selp.u64 	%rd181, 1, 0, %p80;
	add.s64 	%rd182, %rd180, %rd181;
	ld.u8 	%rs57, [%rd236+3];
	and.b16  	%rs58, %rs57, 192;
	setp.ne.s16 	%p81, %rs58, 128;
	selp.u64 	%rd183, 1, 0, %p81;
	add.s64 	%rd241, %rd182, %rd183;
	add.s64 	%rd236, %rd236, 4;
	add.s64 	%rd232, %rd232, 4;
	setp.ne.s64 	%p82, %rd232, 0;
	@%p82 bra 	$L__BB4_57;

$L__BB4_58:
	and.b64  	%rd184, %rd88, 3;
	setp.eq.s64 	%p83, %rd184, 0;
	@%p83 bra 	$L__BB4_61;

	cvt.u64.u16 	%rd185, %rs61;
	and.b64  	%rd186, %rd185, 3;
	neg.s64 	%rd238, %rd186;

$L__BB4_60:
	.pragma "nounroll";
	ld.u8 	%rs59, [%rd236];
	and.b16  	%rs60, %rs59, 192;
	setp.ne.s16 	%p84, %rs60, 128;
	selp.u64 	%rd187, 1, 0, %p84;
	add.s64 	%rd241, %rd241, %rd187;
	add.s64 	%rd236, %rd236, 1;
	add.s64 	%rd238, %rd238, 1;
	setp.ne.s64 	%p85, %rd238, 0;
	@%p85 bra 	$L__BB4_60;

$L__BB4_61:
	cvt.u32.u64 	%r65, %rd241;

$L__BB4_63:
	st.u32 	[%rd109], %r65;
	mov.u32 	%r50, 0;
	st.param.b32 	[func_retval0+0], %r50;
	ret;

}
	// .globl	rfind
.visible .func  (.param .b32 func_retval0) rfind(
	.param .b64 rfind_param_0,
	.param .b64 rfind_param_1,
	.param .b64 rfind_param_2
)
{
	.reg .pred 	%p<87>;
	.reg .b16 	%rs<65>;
	.reg .b32 	%r<69>;
	.reg .b64 	%rd<246>;


	ld.param.u64 	%rd111, [rfind_param_0];
	ld.param.u64 	%rd112, [rfind_param_1];
	ld.param.u64 	%rd113, [rfind_param_2];
	ld.u32 	%r1, [%rd113+8];
	ld.u64 	%rd1, [%rd113];
	setp.eq.s64 	%p1, %rd1, 0;
	mov.u32 	%r68, -1;
	@%p1 bra 	$L__BB5_63;

	ld.u32 	%r56, [%rd112+12];
	setp.eq.s32 	%p2, %r56, -1;
	@%p2 bra 	$L__BB5_3;

	ld.u64 	%rd240, [%rd112];
	ld.u32 	%r55, [%rd112+8];
	bra.uni 	$L__BB5_12;

$L__BB5_3:
	ld.u64 	%rd240, [%rd112];
	setp.eq.s64 	%p3, %rd240, 0;
	ld.u32 	%r55, [%rd112+8];
	setp.eq.s32 	%p4, %r55, 0;
	mov.u32 	%r56, 0;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB5_11;

	cvt.s64.s32 	%rd116, %r55;
	add.s64 	%rd117, %rd116, -1;
	setp.lt.u64 	%p6, %rd117, 3;
	mov.u64 	%rd201, 0;
	mov.u64 	%rd197, %rd240;
	@%p6 bra 	$L__BB5_7;

	and.b64  	%rd120, %rd116, 3;
	sub.s64 	%rd192, %rd120, %rd116;
	mov.u64 	%rd197, %rd240;

$L__BB5_6:
	ld.u8 	%rs8, [%rd197];
	and.b16  	%rs9, %rs8, 192;
	setp.ne.s16 	%p7, %rs9, 128;
	selp.u64 	%rd121, 1, 0, %p7;
	add.s64 	%rd122, %rd201, %rd121;
	ld.u8 	%rs10, [%rd197+1];
	and.b16  	%rs11, %rs10, 192;
	setp.ne.s16 	%p8, %rs11, 128;
	selp.u64 	%rd123, 1, 0, %p8;
	add.s64 	%rd124, %rd122, %rd123;
	ld.u8 	%rs12, [%rd197+2];
	and.b16  	%rs13, %rs12, 192;
	setp.ne.s16 	%p9, %rs13, 128;
	selp.u64 	%rd125, 1, 0, %p9;
	add.s64 	%rd126, %rd124, %rd125;
	ld.u8 	%rs14, [%rd197+3];
	and.b16  	%rs15, %rs14, 192;
	setp.ne.s16 	%p10, %rs15, 128;
	selp.u64 	%rd127, 1, 0, %p10;
	add.s64 	%rd201, %rd126, %rd127;
	add.s64 	%rd197, %rd197, 4;
	add.s64 	%rd192, %rd192, 4;
	setp.ne.s64 	%p11, %rd192, 0;
	@%p11 bra 	$L__BB5_6;

$L__BB5_7:
	cvt.u64.u32 	%rd128, %r55;
	and.b64  	%rd129, %rd128, 3;
	setp.eq.s64 	%p12, %rd129, 0;
	@%p12 bra 	$L__BB5_10;

	neg.s64 	%rd198, %rd129;

$L__BB5_9:
	.pragma "nounroll";
	ld.u8 	%rs16, [%rd197];
	and.b16  	%rs17, %rs16, 192;
	setp.ne.s16 	%p13, %rs17, 128;
	selp.u64 	%rd132, 1, 0, %p13;
	add.s64 	%rd201, %rd201, %rd132;
	add.s64 	%rd197, %rd197, 1;
	add.s64 	%rd198, %rd198, 1;
	setp.ne.s64 	%p14, %rd198, 0;
	@%p14 bra 	$L__BB5_9;

$L__BB5_10:
	cvt.u32.u64 	%r56, %rd201;

$L__BB5_11:
	st.u32 	[%rd112+12], %r56;

$L__BB5_12:
	setp.ne.s32 	%p15, %r56, -1;
	mov.u32 	%r60, %r56;
	@%p15 bra 	$L__BB5_22;

	setp.eq.s64 	%p16, %rd240, 0;
	setp.eq.s32 	%p17, %r55, 0;
	mov.u32 	%r60, 0;
	or.pred  	%p18, %p16, %p17;
	@%p18 bra 	$L__BB5_21;

	cvt.s64.s32 	%rd24, %r55;
	add.s64 	%rd135, %rd24, -1;
	and.b64  	%rd25, %rd24, 3;
	setp.lt.u64 	%p19, %rd135, 3;
	mov.u64 	%rd212, 0;
	mov.u64 	%rd208, %rd240;
	@%p19 bra 	$L__BB5_17;

	sub.s64 	%rd203, %rd25, %rd24;
	mov.u64 	%rd208, %rd240;

$L__BB5_16:
	ld.u8 	%rs18, [%rd208];
	and.b16  	%rs19, %rs18, 192;
	setp.ne.s16 	%p20, %rs19, 128;
	selp.u64 	%rd137, 1, 0, %p20;
	add.s64 	%rd138, %rd212, %rd137;
	ld.u8 	%rs20, [%rd208+1];
	and.b16  	%rs21, %rs20, 192;
	setp.ne.s16 	%p21, %rs21, 128;
	selp.u64 	%rd139, 1, 0, %p21;
	add.s64 	%rd140, %rd138, %rd139;
	ld.u8 	%rs22, [%rd208+2];
	and.b16  	%rs23, %rs22, 192;
	setp.ne.s16 	%p22, %rs23, 128;
	selp.u64 	%rd141, 1, 0, %p22;
	add.s64 	%rd142, %rd140, %rd141;
	ld.u8 	%rs24, [%rd208+3];
	and.b16  	%rs25, %rs24, 192;
	setp.ne.s16 	%p23, %rs25, 128;
	selp.u64 	%rd143, 1, 0, %p23;
	add.s64 	%rd212, %rd142, %rd143;
	add.s64 	%rd208, %rd208, 4;
	add.s64 	%rd203, %rd203, 4;
	setp.ne.s64 	%p24, %rd203, 0;
	@%p24 bra 	$L__BB5_16;

$L__BB5_17:
	setp.eq.s64 	%p25, %rd25, 0;
	@%p25 bra 	$L__BB5_20;

	neg.s64 	%rd209, %rd25;

$L__BB5_19:
	.pragma "nounroll";
	ld.u8 	%rs26, [%rd208];
	and.b16  	%rs27, %rs26, 192;
	setp.ne.s16 	%p26, %rs27, 128;
	selp.u64 	%rd144, 1, 0, %p26;
	add.s64 	%rd212, %rd212, %rd144;
	add.s64 	%rd208, %rd208, 1;
	add.s64 	%rd209, %rd209, 1;
	setp.ne.s64 	%p27, %rd209, 0;
	@%p27 bra 	$L__BB5_19;

$L__BB5_20:
	cvt.u32.u64 	%r60, %rd212;

$L__BB5_21:
	st.u32 	[%rd112+12], %r60;

$L__BB5_22:
	cvt.s64.s32 	%rd44, %r55;
	setp.ne.s32 	%p28, %r60, -1;
	@%p28 bra 	$L__BB5_32;

	setp.eq.s64 	%p29, %rd240, 0;
	setp.eq.s32 	%p30, %r55, 0;
	mov.u32 	%r60, 0;
	or.pred  	%p31, %p29, %p30;
	@%p31 bra 	$L__BB5_31;

	add.s64 	%rd147, %rd44, -1;
	and.b64  	%rd45, %rd44, 3;
	setp.lt.u64 	%p32, %rd147, 3;
	mov.u64 	%rd222, 0;
	mov.u64 	%rd217, %rd240;
	@%p32 bra 	$L__BB5_27;

	sub.s64 	%rd213, %rd45, %rd44;
	mov.u64 	%rd217, %rd240;

$L__BB5_26:
	ld.u8 	%rs28, [%rd217];
	and.b16  	%rs29, %rs28, 192;
	setp.ne.s16 	%p33, %rs29, 128;
	selp.u64 	%rd149, 1, 0, %p33;
	add.s64 	%rd150, %rd222, %rd149;
	ld.u8 	%rs30, [%rd217+1];
	and.b16  	%rs31, %rs30, 192;
	setp.ne.s16 	%p34, %rs31, 128;
	selp.u64 	%rd151, 1, 0, %p34;
	add.s64 	%rd152, %rd150, %rd151;
	ld.u8 	%rs32, [%rd217+2];
	and.b16  	%rs33, %rs32, 192;
	setp.ne.s16 	%p35, %rs33, 128;
	selp.u64 	%rd153, 1, 0, %p35;
	add.s64 	%rd154, %rd152, %rd153;
	ld.u8 	%rs34, [%rd217+3];
	and.b16  	%rs35, %rs34, 192;
	setp.ne.s16 	%p36, %rs35, 128;
	selp.u64 	%rd155, 1, 0, %p36;
	add.s64 	%rd222, %rd154, %rd155;
	add.s64 	%rd217, %rd217, 4;
	add.s64 	%rd213, %rd213, 4;
	setp.ne.s64 	%p37, %rd213, 0;
	@%p37 bra 	$L__BB5_26;

$L__BB5_27:
	setp.eq.s64 	%p38, %rd45, 0;
	@%p38 bra 	$L__BB5_30;

	neg.s64 	%rd219, %rd45;

$L__BB5_29:
	.pragma "nounroll";
	ld.u8 	%rs36, [%rd217];
	and.b16  	%rs37, %rs36, 192;
	setp.ne.s16 	%p39, %rs37, 128;
	selp.u64 	%rd156, 1, 0, %p39;
	add.s64 	%rd222, %rd222, %rd156;
	add.s64 	%rd217, %rd217, 1;
	add.s64 	%rd219, %rd219, 1;
	setp.ne.s64 	%p40, %rd219, 0;
	@%p40 bra 	$L__BB5_29;

$L__BB5_30:
	cvt.u32.u64 	%r60, %rd222;

$L__BB5_31:
	st.u32 	[%rd112+12], %r60;

$L__BB5_32:
	setp.eq.s32 	%p41, %r60, %r55;
	mov.u32 	%r63, %r56;
	@%p41 bra 	$L__BB5_36;

	setp.lt.s32 	%p42, %r56, 1;
	mov.u32 	%r63, 0;
	setp.lt.s32 	%p43, %r55, 1;
	or.pred  	%p44, %p42, %p43;
	@%p44 bra 	$L__BB5_36;

	add.s64 	%rd64, %rd240, %rd44;
	mov.u64 	%rd223, %rd240;

$L__BB5_35:
	ld.u8 	%rs38, [%rd223];
	setp.gt.u16 	%p45, %rs38, 239;
	selp.b32 	%r36, 2, 1, %p45;
	setp.gt.u16 	%p46, %rs38, 223;
	selp.u32 	%r37, 1, 0, %p46;
	add.s32 	%r38, %r36, %r37;
	setp.gt.u16 	%p47, %rs38, 191;
	selp.u32 	%r39, 1, 0, %p47;
	add.s32 	%r40, %r38, %r39;
	and.b16  	%rs39, %rs38, 192;
	setp.eq.s16 	%p48, %rs39, 128;
	selp.b32 	%r41, -1, 0, %p48;
	add.s32 	%r42, %r40, %r41;
	setp.ne.s32 	%p49, %r42, 0;
	selp.b32 	%r43, -1, 0, %p49;
	add.s32 	%r56, %r56, %r43;
	add.s32 	%r63, %r42, %r63;
	setp.gt.s32 	%p50, %r56, 0;
	add.s64 	%rd223, %rd223, 1;
	setp.lt.u64 	%p51, %rd223, %rd64;
	and.pred  	%p52, %p50, %p51;
	@%p52 bra 	$L__BB5_35;

$L__BB5_36:
	sub.s32 	%r45, %r63, %r1;
	cvt.s64.s32 	%rd67, %r45;
	add.s64 	%rd225, %rd240, %rd67;
	setp.lt.s32 	%p53, %r45, 0;
	@%p53 bra 	$L__BB5_63;

	cvt.u16.u32 	%rs40, %r63;
	cvt.u16.u32 	%rs41, %r1;
	sub.s16 	%rs63, %rs40, %rs41;
	neg.s64 	%rd224, %rd67;
	mov.u32 	%r64, 0;
	setp.lt.s32 	%p54, %r1, 1;
	mov.u16 	%rs42, 1;

$L__BB5_38:
	mov.u16 	%rs64, %rs42;
	@%p54 bra 	$L__BB5_42;

	mov.u32 	%r65, 0;

$L__BB5_40:
	cvt.s64.s32 	%rd157, %r65;
	add.s64 	%rd158, %rd225, %rd157;
	add.s64 	%rd159, %rd1, %rd157;
	ld.u8 	%rs3, [%rd159];
	ld.u8 	%rs4, [%rd158];
	setp.eq.s16 	%p55, %rs4, %rs3;
	add.s32 	%r65, %r65, 1;
	setp.lt.s32 	%p56, %r65, %r1;
	and.pred  	%p57, %p55, %p56;
	@%p57 bra 	$L__BB5_40;

	selp.u16 	%rs64, 1, 0, %p55;

$L__BB5_42:
	setp.eq.s16 	%p59, %rs64, 0;
	@%p59 bra 	$L__BB5_62;
	bra.uni 	$L__BB5_43;

$L__BB5_62:
	cvt.u32.u64 	%r52, %rd67;
	add.s64 	%rd225, %rd225, -1;
	add.s32 	%r28, %r64, 1;
	setp.gt.s32 	%p86, %r52, %r64;
	add.s16 	%rs63, %rs63, 3;
	add.s64 	%rd224, %rd224, 1;
	mov.u32 	%r64, %r28;
	@%p86 bra 	$L__BB5_38;
	bra.uni 	$L__BB5_63;

$L__BB5_43:
	cvt.u32.u64 	%r48, %rd67;
	sub.s32 	%r23, %r48, %r64;
	setp.ne.s32 	%p60, %r60, -1;
	@%p60 bra 	$L__BB5_53;

	setp.eq.s64 	%p61, %rd240, 0;
	setp.eq.s32 	%p62, %r55, 0;
	mov.u32 	%r60, 0;
	or.pred  	%p63, %p61, %p62;
	@%p63 bra 	$L__BB5_52;

	add.s64 	%rd162, %rd44, -1;
	and.b64  	%rd72, %rd44, 3;
	setp.lt.u64 	%p64, %rd162, 3;
	mov.u64 	%rd235, 0;
	mov.u64 	%rd230, %rd240;
	@%p64 bra 	$L__BB5_48;

	sub.s64 	%rd226, %rd72, %rd44;
	mov.u64 	%rd230, %rd240;

$L__BB5_47:
	ld.u8 	%rs43, [%rd230];
	and.b16  	%rs44, %rs43, 192;
	setp.ne.s16 	%p65, %rs44, 128;
	selp.u64 	%rd164, 1, 0, %p65;
	add.s64 	%rd165, %rd235, %rd164;
	ld.u8 	%rs45, [%rd230+1];
	and.b16  	%rs46, %rs45, 192;
	setp.ne.s16 	%p66, %rs46, 128;
	selp.u64 	%rd166, 1, 0, %p66;
	add.s64 	%rd167, %rd165, %rd166;
	ld.u8 	%rs47, [%rd230+2];
	and.b16  	%rs48, %rs47, 192;
	setp.ne.s16 	%p67, %rs48, 128;
	selp.u64 	%rd168, 1, 0, %p67;
	add.s64 	%rd169, %rd167, %rd168;
	ld.u8 	%rs49, [%rd230+3];
	and.b16  	%rs50, %rs49, 192;
	setp.ne.s16 	%p68, %rs50, 128;
	selp.u64 	%rd170, 1, 0, %p68;
	add.s64 	%rd235, %rd169, %rd170;
	add.s64 	%rd230, %rd230, 4;
	add.s64 	%rd226, %rd226, 4;
	setp.ne.s64 	%p69, %rd226, 0;
	@%p69 bra 	$L__BB5_47;

$L__BB5_48:
	setp.eq.s64 	%p70, %rd72, 0;
	@%p70 bra 	$L__BB5_51;

	neg.s64 	%rd232, %rd72;

$L__BB5_50:
	.pragma "nounroll";
	ld.u8 	%rs51, [%rd230];
	and.b16  	%rs52, %rs51, 192;
	setp.ne.s16 	%p71, %rs52, 128;
	selp.u64 	%rd171, 1, 0, %p71;
	add.s64 	%rd235, %rd235, %rd171;
	add.s64 	%rd230, %rd230, 1;
	add.s64 	%rd232, %rd232, 1;
	setp.ne.s64 	%p72, %rd232, 0;
	@%p72 bra 	$L__BB5_50;

$L__BB5_51:
	cvt.u32.u64 	%r60, %rd235;

$L__BB5_52:
	st.u32 	[%rd112+12], %r60;

$L__BB5_53:
	setp.eq.s32 	%p73, %r60, %r55;
	mov.u32 	%r68, %r23;
	@%p73 bra 	$L__BB5_63;

	setp.eq.s64 	%p74, %rd240, 0;
	setp.eq.s32 	%p75, %r23, 0;
	mov.u32 	%r68, 0;
	or.pred  	%p76, %p75, %p74;
	@%p76 bra 	$L__BB5_63;

	cvt.u64.u32 	%rd174, %r64;
	not.b64 	%rd175, %rd174;
	add.s64 	%rd176, %rd175, %rd67;
	setp.lt.u64 	%p77, %rd176, 3;
	mov.u64 	%rd245, 0;
	@%p77 bra 	$L__BB5_58;

	cvt.u64.u16 	%rd178, %rs63;
	and.b64  	%rd179, %rd178, 3;
	add.s64 	%rd236, %rd224, %rd179;

$L__BB5_57:
	ld.u8 	%rs53, [%rd240];
	and.b16  	%rs54, %rs53, 192;
	setp.ne.s16 	%p78, %rs54, 128;
	selp.u64 	%rd180, 1, 0, %p78;
	add.s64 	%rd181, %rd245, %rd180;
	ld.u8 	%rs55, [%rd240+1];
	and.b16  	%rs56, %rs55, 192;
	setp.ne.s16 	%p79, %rs56, 128;
	selp.u64 	%rd182, 1, 0, %p79;
	add.s64 	%rd183, %rd181, %rd182;
	ld.u8 	%rs57, [%rd240+2];
	and.b16  	%rs58, %rs57, 192;
	setp.ne.s16 	%p80, %rs58, 128;
	selp.u64 	%rd184, 1, 0, %p80;
	add.s64 	%rd185, %rd183, %rd184;
	ld.u8 	%rs59, [%rd240+3];
	and.b16  	%rs60, %rs59, 192;
	setp.ne.s16 	%p81, %rs60, 128;
	selp.u64 	%rd186, 1, 0, %p81;
	add.s64 	%rd245, %rd185, %rd186;
	add.s64 	%rd240, %rd240, 4;
	add.s64 	%rd236, %rd236, 4;
	setp.ne.s64 	%p82, %rd236, 0;
	@%p82 bra 	$L__BB5_57;

$L__BB5_58:
	cvt.u64.u32 	%rd187, %r23;
	and.b64  	%rd188, %rd187, 3;
	setp.eq.s64 	%p83, %rd188, 0;
	@%p83 bra 	$L__BB5_61;

	cvt.u64.u16 	%rd189, %rs63;
	and.b64  	%rd190, %rd189, 3;
	neg.s64 	%rd242, %rd190;

$L__BB5_60:
	.pragma "nounroll";
	ld.u8 	%rs61, [%rd240];
	and.b16  	%rs62, %rs61, 192;
	setp.ne.s16 	%p84, %rs62, 128;
	selp.u64 	%rd191, 1, 0, %p84;
	add.s64 	%rd245, %rd245, %rd191;
	add.s64 	%rd240, %rd240, 1;
	add.s64 	%rd242, %rd242, 1;
	setp.ne.s64 	%p85, %rd242, 0;
	@%p85 bra 	$L__BB5_60;

$L__BB5_61:
	cvt.u32.u64 	%r68, %rd245;

$L__BB5_63:
	st.u32 	[%rd111], %r68;
	mov.u32 	%r53, 0;
	st.param.b32 	[func_retval0+0], %r53;
	ret;

}
	// .globl	eq
.visible .func  (.param .b32 func_retval0) eq(
	.param .b64 eq_param_0,
	.param .b64 eq_param_1,
	.param .b64 eq_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd9, [eq_param_0];
	ld.param.u64 	%rd1, [eq_param_1];
	ld.param.u64 	%rd2, [eq_param_2];
	ld.u32 	%r4, [%rd2+8];
	ld.u32 	%r1, [%rd1+8];
	setp.ne.s32 	%p1, %r1, %r4;
	mov.u16 	%rs2, 0;
	mov.u16 	%rs8, %rs2;
	@%p1 bra 	$L__BB6_5;

	ld.u64 	%rd11, [%rd1];
	ld.u64 	%rd10, [%rd2];
	setp.eq.s64 	%p2, %rd11, %rd10;
	setp.lt.s32 	%p3, %r1, 1;
	mov.u16 	%rs3, 1;
	or.pred  	%p4, %p2, %p3;
	mov.u16 	%rs8, %rs3;
	@%p4 bra 	$L__BB6_5;

	mov.u32 	%r7, 0;

$L__BB6_3:
	ld.u8 	%rs5, [%rd10];
	ld.u8 	%rs6, [%rd11];
	setp.ne.s16 	%p5, %rs6, %rs5;
	mov.u16 	%rs8, %rs2;
	@%p5 bra 	$L__BB6_5;

	add.s64 	%rd11, %rd11, 1;
	add.s64 	%rd10, %rd10, 1;
	add.s32 	%r7, %r7, 1;
	setp.lt.s32 	%p6, %r7, %r1;
	mov.u16 	%rs8, %rs3;
	@%p6 bra 	$L__BB6_3;

$L__BB6_5:
	st.u8 	[%rd9], %rs8;
	mov.u32 	%r6, 0;
	st.param.b32 	[func_retval0+0], %r6;
	ret;

}
	// .globl	ne
.visible .func  (.param .b32 func_retval0) ne(
	.param .b64 ne_param_0,
	.param .b64 ne_param_1,
	.param .b64 ne_param_2
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd7, [ne_param_0];
	ld.param.u64 	%rd8, [ne_param_1];
	ld.param.u64 	%rd9, [ne_param_2];
	ld.u64 	%rd11, [%rd8];
	ld.u64 	%rd10, [%rd9];
	setp.eq.s64 	%p7, %rd11, %rd10;
	ld.u32 	%r1, [%rd8+8];
	ld.u32 	%r2, [%rd9+8];
	setp.eq.s32 	%p8, %r1, %r2;
	and.pred  	%p9, %p8, %p7;
	mov.u32 	%r8, 0;
	mov.u32 	%r15, %r8;
	@%p9 bra 	$L__BB7_7;

	setp.gt.s32 	%p15, %r1, 0;
	setp.gt.s32 	%p16, %r2, 0;
	and.pred  	%p10, %p15, %p16;
	not.pred 	%p11, %p10;
	@%p11 bra 	$L__BB7_6;

	mov.u32 	%r14, 0;

$L__BB7_3:
	ld.u8 	%rs1, [%rd11];
	ld.u8 	%rs2, [%rd10];
	setp.eq.s16 	%p12, %rs1, %rs2;
	@%p12 bra 	$L__BB7_5;
	bra.uni 	$L__BB7_4;

$L__BB7_5:
	add.s64 	%rd11, %rd11, 1;
	add.s64 	%rd10, %rd10, 1;
	add.s32 	%r14, %r14, 1;
	setp.lt.s32 	%p15, %r14, %r1;
	setp.lt.s32 	%p16, %r14, %r2;
	and.pred  	%p13, %p15, %p16;
	@%p13 bra 	$L__BB7_3;

$L__BB7_6:
	selp.s32 	%r12, -1, 0, %p16;
	selp.b32 	%r15, 1, %r12, %p15;
	bra.uni 	$L__BB7_7;

$L__BB7_4:
	cvt.u32.u16 	%r10, %rs2;
	cvt.u32.u16 	%r11, %rs1;
	sub.s32 	%r15, %r11, %r10;

$L__BB7_7:
	setp.ne.s32 	%p14, %r15, 0;
	selp.u16 	%rs3, 1, 0, %p14;
	st.u8 	[%rd7], %rs3;
	st.param.b32 	[func_retval0+0], %r8;
	ret;

}
	// .globl	ge
.visible .func  (.param .b32 func_retval0) ge(
	.param .b64 ge_param_0,
	.param .b64 ge_param_1,
	.param .b64 ge_param_2
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<5>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd7, [ge_param_0];
	ld.param.u64 	%rd8, [ge_param_1];
	ld.param.u64 	%rd9, [ge_param_2];
	ld.u64 	%rd11, [%rd8];
	ld.u64 	%rd10, [%rd9];
	setp.eq.s64 	%p7, %rd11, %rd10;
	ld.u32 	%r1, [%rd8+8];
	ld.u32 	%r2, [%rd9+8];
	setp.eq.s32 	%p8, %r1, %r2;
	and.pred  	%p9, %p8, %p7;
	mov.u32 	%r8, 0;
	mov.u32 	%r16, %r8;
	@%p9 bra 	$L__BB8_7;

	setp.gt.s32 	%p14, %r1, 0;
	setp.gt.s32 	%p15, %r2, 0;
	and.pred  	%p10, %p14, %p15;
	not.pred 	%p11, %p10;
	@%p11 bra 	$L__BB8_6;

	mov.u32 	%r15, 0;

$L__BB8_3:
	ld.u8 	%rs1, [%rd11];
	ld.u8 	%rs2, [%rd10];
	setp.eq.s16 	%p12, %rs1, %rs2;
	@%p12 bra 	$L__BB8_5;
	bra.uni 	$L__BB8_4;

$L__BB8_5:
	add.s64 	%rd11, %rd11, 1;
	add.s64 	%rd10, %rd10, 1;
	add.s32 	%r15, %r15, 1;
	setp.lt.s32 	%p14, %r15, %r1;
	setp.lt.s32 	%p15, %r15, %r2;
	and.pred  	%p13, %p14, %p15;
	@%p13 bra 	$L__BB8_3;

$L__BB8_6:
	selp.s32 	%r12, -1, 0, %p15;
	selp.b32 	%r16, 1, %r12, %p14;
	bra.uni 	$L__BB8_7;

$L__BB8_4:
	cvt.u32.u16 	%r10, %rs2;
	cvt.u32.u16 	%r11, %rs1;
	sub.s32 	%r16, %r11, %r10;

$L__BB8_7:
	shr.u32 	%r13, %r16, 31;
	cvt.u16.u32 	%rs3, %r13;
	xor.b16  	%rs4, %rs3, 1;
	st.u8 	[%rd7], %rs4;
	st.param.b32 	[func_retval0+0], %r8;
	ret;

}
	// .globl	le
.visible .func  (.param .b32 func_retval0) le(
	.param .b64 le_param_0,
	.param .b64 le_param_1,
	.param .b64 le_param_2
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd7, [le_param_0];
	ld.param.u64 	%rd8, [le_param_1];
	ld.param.u64 	%rd9, [le_param_2];
	ld.u64 	%rd11, [%rd8];
	ld.u64 	%rd10, [%rd9];
	setp.eq.s64 	%p7, %rd11, %rd10;
	ld.u32 	%r1, [%rd8+8];
	ld.u32 	%r2, [%rd9+8];
	setp.eq.s32 	%p8, %r1, %r2;
	and.pred  	%p9, %p8, %p7;
	mov.u32 	%r8, 0;
	mov.u32 	%r15, %r8;
	@%p9 bra 	$L__BB9_7;

	setp.gt.s32 	%p15, %r1, 0;
	setp.gt.s32 	%p16, %r2, 0;
	and.pred  	%p10, %p15, %p16;
	not.pred 	%p11, %p10;
	@%p11 bra 	$L__BB9_6;

	mov.u32 	%r14, 0;

$L__BB9_3:
	ld.u8 	%rs1, [%rd11];
	ld.u8 	%rs2, [%rd10];
	setp.eq.s16 	%p12, %rs1, %rs2;
	@%p12 bra 	$L__BB9_5;
	bra.uni 	$L__BB9_4;

$L__BB9_5:
	add.s64 	%rd11, %rd11, 1;
	add.s64 	%rd10, %rd10, 1;
	add.s32 	%r14, %r14, 1;
	setp.lt.s32 	%p15, %r14, %r1;
	setp.lt.s32 	%p16, %r14, %r2;
	and.pred  	%p13, %p15, %p16;
	@%p13 bra 	$L__BB9_3;

$L__BB9_6:
	selp.s32 	%r12, -1, 0, %p16;
	selp.b32 	%r15, 1, %r12, %p15;
	bra.uni 	$L__BB9_7;

$L__BB9_4:
	cvt.u32.u16 	%r10, %rs2;
	cvt.u32.u16 	%r11, %rs1;
	sub.s32 	%r15, %r11, %r10;

$L__BB9_7:
	setp.lt.s32 	%p14, %r15, 1;
	selp.u16 	%rs3, 1, 0, %p14;
	st.u8 	[%rd7], %rs3;
	st.param.b32 	[func_retval0+0], %r8;
	ret;

}
	// .globl	gt
.visible .func  (.param .b32 func_retval0) gt(
	.param .b64 gt_param_0,
	.param .b64 gt_param_1,
	.param .b64 gt_param_2
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd7, [gt_param_0];
	ld.param.u64 	%rd8, [gt_param_1];
	ld.param.u64 	%rd9, [gt_param_2];
	ld.u64 	%rd11, [%rd8];
	ld.u64 	%rd10, [%rd9];
	setp.eq.s64 	%p7, %rd11, %rd10;
	ld.u32 	%r1, [%rd8+8];
	ld.u32 	%r2, [%rd9+8];
	setp.eq.s32 	%p8, %r1, %r2;
	and.pred  	%p9, %p8, %p7;
	mov.u32 	%r8, 0;
	mov.u32 	%r15, %r8;
	@%p9 bra 	$L__BB10_7;

	setp.gt.s32 	%p15, %r1, 0;
	setp.gt.s32 	%p16, %r2, 0;
	and.pred  	%p10, %p15, %p16;
	not.pred 	%p11, %p10;
	@%p11 bra 	$L__BB10_6;

	mov.u32 	%r14, 0;

$L__BB10_3:
	ld.u8 	%rs1, [%rd11];
	ld.u8 	%rs2, [%rd10];
	setp.eq.s16 	%p12, %rs1, %rs2;
	@%p12 bra 	$L__BB10_5;
	bra.uni 	$L__BB10_4;

$L__BB10_5:
	add.s64 	%rd11, %rd11, 1;
	add.s64 	%rd10, %rd10, 1;
	add.s32 	%r14, %r14, 1;
	setp.lt.s32 	%p15, %r14, %r1;
	setp.lt.s32 	%p16, %r14, %r2;
	and.pred  	%p13, %p15, %p16;
	@%p13 bra 	$L__BB10_3;

$L__BB10_6:
	selp.s32 	%r12, -1, 0, %p16;
	selp.b32 	%r15, 1, %r12, %p15;
	bra.uni 	$L__BB10_7;

$L__BB10_4:
	cvt.u32.u16 	%r10, %rs2;
	cvt.u32.u16 	%r11, %rs1;
	sub.s32 	%r15, %r11, %r10;

$L__BB10_7:
	setp.gt.s32 	%p14, %r15, 0;
	selp.u16 	%rs3, 1, 0, %p14;
	st.u8 	[%rd7], %rs3;
	st.param.b32 	[func_retval0+0], %r8;
	ret;

}
	// .globl	lt
.visible .func  (.param .b32 func_retval0) lt(
	.param .b64 lt_param_0,
	.param .b64 lt_param_1,
	.param .b64 lt_param_2
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd7, [lt_param_0];
	ld.param.u64 	%rd8, [lt_param_1];
	ld.param.u64 	%rd9, [lt_param_2];
	ld.u64 	%rd11, [%rd8];
	ld.u64 	%rd10, [%rd9];
	setp.eq.s64 	%p7, %rd11, %rd10;
	ld.u32 	%r1, [%rd8+8];
	ld.u32 	%r2, [%rd9+8];
	setp.eq.s32 	%p8, %r1, %r2;
	and.pred  	%p9, %p8, %p7;
	mov.u32 	%r8, 0;
	mov.u32 	%r16, %r8;
	@%p9 bra 	$L__BB11_7;

	setp.gt.s32 	%p14, %r1, 0;
	setp.gt.s32 	%p15, %r2, 0;
	and.pred  	%p10, %p14, %p15;
	not.pred 	%p11, %p10;
	@%p11 bra 	$L__BB11_6;

	mov.u32 	%r15, 0;

$L__BB11_3:
	ld.u8 	%rs1, [%rd11];
	ld.u8 	%rs2, [%rd10];
	setp.eq.s16 	%p12, %rs1, %rs2;
	@%p12 bra 	$L__BB11_5;
	bra.uni 	$L__BB11_4;

$L__BB11_5:
	add.s64 	%rd11, %rd11, 1;
	add.s64 	%rd10, %rd10, 1;
	add.s32 	%r15, %r15, 1;
	setp.lt.s32 	%p14, %r15, %r1;
	setp.lt.s32 	%p15, %r15, %r2;
	and.pred  	%p13, %p14, %p15;
	@%p13 bra 	$L__BB11_3;

$L__BB11_6:
	selp.s32 	%r12, -1, 0, %p15;
	selp.b32 	%r16, 1, %r12, %p14;
	bra.uni 	$L__BB11_7;

$L__BB11_4:
	cvt.u32.u16 	%r10, %rs2;
	cvt.u32.u16 	%r11, %rs1;
	sub.s32 	%r16, %r11, %r10;

$L__BB11_7:
	shr.u32 	%r13, %r16, 31;
	st.u8 	[%rd7], %r13;
	st.param.b32 	[func_retval0+0], %r8;
	ret;

}
	// .globl	pyislower
.visible .func  (.param .b32 func_retval0) pyislower(
	.param .b64 pyislower_param_0,
	.param .b64 pyislower_param_1,
	.param .b64 pyislower_param_2
)
{
	.reg .pred 	%p<66>;
	.reg .b16 	%rs<52>;
	.reg .b32 	%r<83>;
	.reg .b64 	%rd<128>;


	ld.param.u64 	%rd51, [pyislower_param_0];
	ld.param.u64 	%rd53, [pyislower_param_1];
	ld.param.u64 	%rd52, [pyislower_param_2];
	ld.u64 	%rd1, [%rd53];
	ld.v2.u32 	{%r31, %r74}, [%rd53+8];
	setp.ne.s32 	%p1, %r31, 0;
	selp.u16 	%rs51, 1, 0, %p1;
	setp.ne.s32 	%p2, %r74, -1;
	@%p2 bra 	$L__BB12_8;

	setp.eq.s64 	%p3, %rd1, 0;
	setp.eq.s32 	%p4, %r31, 0;
	mov.u32 	%r74, 0;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB12_8;

	cvt.s64.s32 	%rd2, %r31;
	add.s64 	%rd56, %rd2, -1;
	and.b64  	%rd112, %rd2, 3;
	setp.lt.u64 	%p6, %rd56, 3;
	mov.u64 	%rd113, 0;
	mov.u64 	%rd109, %rd1;
	@%p6 bra 	$L__BB12_5;

	sub.s64 	%rd106, %rd2, %rd112;
	mov.u64 	%rd109, %rd1;

$L__BB12_4:
	ld.u8 	%rs5, [%rd109];
	and.b16  	%rs6, %rs5, 192;
	setp.ne.s16 	%p7, %rs6, 128;
	selp.u64 	%rd58, 1, 0, %p7;
	add.s64 	%rd59, %rd113, %rd58;
	ld.u8 	%rs7, [%rd109+1];
	and.b16  	%rs8, %rs7, 192;
	setp.ne.s16 	%p8, %rs8, 128;
	selp.u64 	%rd60, 1, 0, %p8;
	add.s64 	%rd61, %rd59, %rd60;
	ld.u8 	%rs9, [%rd109+2];
	and.b16  	%rs10, %rs9, 192;
	setp.ne.s16 	%p9, %rs10, 128;
	selp.u64 	%rd62, 1, 0, %p9;
	add.s64 	%rd63, %rd61, %rd62;
	ld.u8 	%rs11, [%rd109+3];
	and.b16  	%rs12, %rs11, 192;
	setp.ne.s16 	%p10, %rs12, 128;
	selp.u64 	%rd64, 1, 0, %p10;
	add.s64 	%rd113, %rd63, %rd64;
	add.s64 	%rd109, %rd109, 4;
	add.s64 	%rd106, %rd106, -4;
	setp.ne.s64 	%p11, %rd106, 0;
	@%p11 bra 	$L__BB12_4;

$L__BB12_5:
	setp.eq.s64 	%p12, %rd112, 0;
	@%p12 bra 	$L__BB12_7;

$L__BB12_6:
	.pragma "nounroll";
	ld.u8 	%rs13, [%rd109];
	and.b16  	%rs14, %rs13, 192;
	setp.ne.s16 	%p13, %rs14, 128;
	selp.u64 	%rd65, 1, 0, %p13;
	add.s64 	%rd113, %rd113, %rd65;
	add.s64 	%rd109, %rd109, 1;
	add.s64 	%rd112, %rd112, -1;
	setp.ne.s64 	%p14, %rd112, 0;
	@%p14 bra 	$L__BB12_6;

$L__BB12_7:
	cvt.u32.u64 	%r74, %rd113;

$L__BB12_8:
	setp.eq.s32 	%p15, %r31, 0;
	mov.u32 	%r34, 0;
	mov.u32 	%r82, %r34;
	@%p15 bra 	$L__BB12_43;

	cvt.u64.u32 	%rd66, %r31;
	and.b64  	%rd21, %rd66, 3;
	setp.eq.s64 	%p17, %rd1, 0;
	mov.u16 	%rs45, 0;
	cvt.s64.s32 	%rd86, %r31;
	add.s64 	%rd87, %rd86, -1;
	setp.lt.u64 	%p34, %rd87, 3;
	mov.u64 	%rd85, 0;
	setp.eq.s64 	%p40, %rd21, 0;
	setp.eq.s64 	%p26, %rd21, 1;
	mov.u16 	%rs50, %rs51;
	mov.u32 	%r75, %r34;
	mov.u32 	%r76, %r34;

$L__BB12_10:
	setp.ne.s32 	%p16, %r74, -1;
	or.pred  	%p18, %p16, %p17;
	selp.b32 	%r77, %r74, 0, %p16;
	@%p18 bra 	$L__BB12_19;

	mov.u64 	%rd120, 0;
	mov.u64 	%rd118, %rd1;
	@%p34 bra 	$L__BB12_14;

	and.b64  	%rd73, %rd86, 3;
	sub.s64 	%rd114, %rd73, %rd86;
	mov.u64 	%rd118, %rd1;

$L__BB12_13:
	ld.u8 	%rs15, [%rd118];
	and.b16  	%rs16, %rs15, 192;
	setp.ne.s16 	%p20, %rs16, 128;
	selp.u64 	%rd74, 1, 0, %p20;
	add.s64 	%rd75, %rd120, %rd74;
	ld.u8 	%rs17, [%rd118+1];
	and.b16  	%rs18, %rs17, 192;
	setp.ne.s16 	%p21, %rs18, 128;
	selp.u64 	%rd76, 1, 0, %p21;
	add.s64 	%rd77, %rd75, %rd76;
	ld.u8 	%rs19, [%rd118+2];
	and.b16  	%rs20, %rs19, 192;
	setp.ne.s16 	%p22, %rs20, 128;
	selp.u64 	%rd78, 1, 0, %p22;
	add.s64 	%rd79, %rd77, %rd78;
	ld.u8 	%rs21, [%rd118+3];
	and.b16  	%rs22, %rs21, 192;
	setp.ne.s16 	%p23, %rs22, 128;
	selp.u64 	%rd80, 1, 0, %p23;
	add.s64 	%rd120, %rd79, %rd80;
	add.s64 	%rd118, %rd118, 4;
	add.s64 	%rd114, %rd114, 4;
	setp.ne.s64 	%p24, %rd114, 0;
	@%p24 bra 	$L__BB12_13;

$L__BB12_14:
	@%p40 bra 	$L__BB12_18;

	ld.u8 	%rs23, [%rd118];
	and.b16  	%rs24, %rs23, 192;
	setp.ne.s16 	%p27, %rs24, 128;
	selp.u64 	%rd81, 1, 0, %p27;
	add.s64 	%rd120, %rd120, %rd81;
	@%p26 bra 	$L__BB12_18;

	setp.eq.s64 	%p28, %rd21, 2;
	ld.u8 	%rs25, [%rd118+1];
	and.b16  	%rs26, %rs25, 192;
	setp.ne.s16 	%p29, %rs26, 128;
	selp.u64 	%rd82, 1, 0, %p29;
	add.s64 	%rd120, %rd120, %rd82;
	@%p28 bra 	$L__BB12_18;

	ld.u8 	%rs27, [%rd118+2];
	and.b16  	%rs28, %rs27, 192;
	setp.ne.s16 	%p30, %rs28, 128;
	selp.u64 	%rd83, 1, 0, %p30;
	add.s64 	%rd120, %rd120, %rd83;

$L__BB12_18:
	cvt.u32.u64 	%r77, %rd120;

$L__BB12_19:
	setp.ne.s32 	%p31, %r77, -1;
	or.pred  	%p33, %p31, %p17;
	selp.b32 	%r74, %r77, 0, %p31;
	@%p33 bra 	$L__BB12_28;

	mov.u64 	%rd125, %rd1;
	mov.u64 	%rd127, %rd85;
	@%p34 bra 	$L__BB12_23;

	and.b64  	%rd90, %rd86, 3;
	sub.s64 	%rd123, %rd86, %rd90;
	mov.u64 	%rd125, %rd1;
	mov.u64 	%rd127, %rd85;

$L__BB12_22:
	ld.u8 	%rs29, [%rd125];
	and.b16  	%rs30, %rs29, 192;
	setp.ne.s16 	%p35, %rs30, 128;
	selp.u64 	%rd91, 1, 0, %p35;
	add.s64 	%rd92, %rd127, %rd91;
	ld.u8 	%rs31, [%rd125+1];
	and.b16  	%rs32, %rs31, 192;
	setp.ne.s16 	%p36, %rs32, 128;
	selp.u64 	%rd93, 1, 0, %p36;
	add.s64 	%rd94, %rd92, %rd93;
	ld.u8 	%rs33, [%rd125+2];
	and.b16  	%rs34, %rs33, 192;
	setp.ne.s16 	%p37, %rs34, 128;
	selp.u64 	%rd95, 1, 0, %p37;
	add.s64 	%rd96, %rd94, %rd95;
	ld.u8 	%rs35, [%rd125+3];
	and.b16  	%rs36, %rs35, 192;
	setp.ne.s16 	%p38, %rs36, 128;
	selp.u64 	%rd97, 1, 0, %p38;
	add.s64 	%rd127, %rd96, %rd97;
	add.s64 	%rd125, %rd125, 4;
	add.s64 	%rd123, %rd123, -4;
	setp.ne.s64 	%p39, %rd123, 0;
	@%p39 bra 	$L__BB12_22;

$L__BB12_23:
	@%p40 bra 	$L__BB12_27;

	ld.u8 	%rs37, [%rd125];
	and.b16  	%rs38, %rs37, 192;
	setp.ne.s16 	%p42, %rs38, 128;
	selp.u64 	%rd98, 1, 0, %p42;
	add.s64 	%rd127, %rd127, %rd98;
	@%p26 bra 	$L__BB12_27;

	setp.eq.s64 	%p43, %rd21, 2;
	ld.u8 	%rs39, [%rd125+1];
	and.b16  	%rs40, %rs39, 192;
	setp.ne.s16 	%p44, %rs40, 128;
	selp.u64 	%rd99, 1, 0, %p44;
	add.s64 	%rd127, %rd127, %rd99;
	@%p43 bra 	$L__BB12_27;

	ld.u8 	%rs41, [%rd125+2];
	and.b16  	%rs42, %rs41, 192;
	setp.ne.s16 	%p45, %rs42, 128;
	selp.u64 	%rd100, 1, 0, %p45;
	add.s64 	%rd127, %rd127, %rd100;

$L__BB12_27:
	cvt.u32.u64 	%r74, %rd127;

$L__BB12_28:
	setp.eq.s32 	%p46, %r75, %r77;
	mov.u16 	%rs51, %rs50;
	@%p46 bra 	$L__BB12_43;

	cvt.s64.s32 	%rd101, %r76;
	add.s64 	%rd50, %rd1, %rd101;
	ld.u8 	%rs43, [%rd50];
	setp.gt.u16 	%p47, %rs43, 239;
	selp.b32 	%r38, 2, 1, %p47;
	setp.gt.u16 	%p48, %rs43, 223;
	selp.u32 	%r39, 1, 0, %p48;
	add.s32 	%r40, %r38, %r39;
	setp.gt.u16 	%p49, %rs43, 191;
	selp.u32 	%r41, 1, 0, %p49;
	add.s32 	%r42, %r40, %r41;
	and.b16  	%rs44, %rs43, 192;
	setp.eq.s16 	%p50, %rs44, 128;
	selp.b32 	%r43, -1, 0, %p50;
	add.s32 	%r15, %r42, %r43;
	cvt.u32.u16 	%r79, %rs43;
	setp.lt.s32 	%p51, %r15, 2;
	@%p51 bra 	$L__BB12_33;

	ld.u8 	%r44, [%rd50+1];
	prmt.b32 	%r79, %r79, %r44, 8452;
	setp.eq.s32 	%p52, %r15, 2;
	@%p52 bra 	$L__BB12_33;

	ld.u8 	%r45, [%rd50+2];
	prmt.b32 	%r79, %r79, %r45, 8452;
	setp.lt.s32 	%p53, %r15, 4;
	@%p53 bra 	$L__BB12_33;

	ld.u8 	%r46, [%rd50+3];
	prmt.b32 	%r79, %r79, %r46, 8452;

$L__BB12_33:
	setp.lt.u32 	%p54, %r79, 128;
	mov.u32 	%r80, %r79;
	@%p54 bra 	$L__BB12_40;

	setp.lt.u32 	%p55, %r79, 57344;
	@%p55 bra 	$L__BB12_39;
	bra.uni 	$L__BB12_35;

$L__BB12_39:
	and.b32  	%r63, %r79, 7936;
	shr.u32 	%r64, %r63, 2;
	and.b32  	%r65, %r79, 63;
	or.b32  	%r80, %r64, %r65;
	bra.uni 	$L__BB12_40;

$L__BB12_35:
	setp.lt.u32 	%p56, %r79, 15728640;
	@%p56 bra 	$L__BB12_38;
	bra.uni 	$L__BB12_36;

$L__BB12_38:
	and.b32  	%r57, %r79, 983040;
	shr.u32 	%r58, %r57, 4;
	and.b32  	%r59, %r79, 16128;
	shr.u32 	%r60, %r59, 2;
	and.b32  	%r61, %r79, 63;
	or.b32  	%r62, %r60, %r61;
	or.b32  	%r80, %r62, %r58;
	bra.uni 	$L__BB12_40;

$L__BB12_36:
	setp.gt.u32 	%p57, %r79, -134217728;
	mov.u32 	%r80, 0;
	@%p57 bra 	$L__BB12_40;

	and.b32  	%r48, %r79, 50331648;
	shr.u32 	%r49, %r48, 6;
	and.b32  	%r50, %r79, 4128768;
	shr.u32 	%r51, %r50, 4;
	and.b32  	%r52, %r79, 16128;
	shr.u32 	%r53, %r52, 2;
	and.b32  	%r54, %r79, 63;
	or.b32  	%r55, %r51, %r54;
	or.b32  	%r56, %r55, %r49;
	or.b32  	%r80, %r56, %r53;

$L__BB12_40:
	setp.gt.u32 	%p58, %r80, 65535;
	mov.u32 	%r81, 0;
	@%p58 bra 	$L__BB12_42;

	cvt.u64.u32 	%rd102, %r80;
	add.s64 	%rd103, %rd52, %rd102;
	ld.u8 	%r81, [%rd103];

$L__BB12_42:
	and.b32  	%r67, %r81, 96;
	setp.eq.s32 	%p59, %r67, 0;
	bfe.u32 	%r68, %r81, 6, 1;
	cvt.u16.u32 	%rs46, %r68;
	selp.b16 	%rs50, %rs50, %rs46, %p59;
	and.b16  	%rs47, %rs50, 255;
	setp.ne.s32 	%p60, %r67, 0;
	selp.u32 	%r69, 1, 0, %p60;
	add.s32 	%r82, %r82, %r69;
	add.s32 	%r70, %r15, %r76;
	setp.lt.s32 	%p61, %r76, %r31;
	selp.b32 	%r76, %r70, %r76, %p61;
	add.s32 	%r75, %r75, 1;
	setp.ne.s16 	%p62, %rs47, 0;
	mov.u16 	%rs51, %rs45;
	@%p62 bra 	$L__BB12_10;

$L__BB12_43:
	setp.ne.s32 	%p63, %r82, 0;
	and.b16  	%rs48, %rs51, 255;
	setp.ne.s16 	%p64, %rs48, 0;
	and.pred  	%p65, %p64, %p63;
	selp.u16 	%rs49, 1, 0, %p65;
	st.u8 	[%rd51], %rs49;
	st.param.b32 	[func_retval0+0], %r34;
	ret;

}
	// .globl	pyisupper
.visible .func  (.param .b32 func_retval0) pyisupper(
	.param .b64 pyisupper_param_0,
	.param .b64 pyisupper_param_1,
	.param .b64 pyisupper_param_2
)
{
	.reg .pred 	%p<66>;
	.reg .b16 	%rs<52>;
	.reg .b32 	%r<83>;
	.reg .b64 	%rd<128>;


	ld.param.u64 	%rd51, [pyisupper_param_0];
	ld.param.u64 	%rd53, [pyisupper_param_1];
	ld.param.u64 	%rd52, [pyisupper_param_2];
	ld.u64 	%rd1, [%rd53];
	ld.v2.u32 	{%r31, %r74}, [%rd53+8];
	setp.ne.s32 	%p1, %r31, 0;
	selp.u16 	%rs51, 1, 0, %p1;
	setp.ne.s32 	%p2, %r74, -1;
	@%p2 bra 	$L__BB13_8;

	setp.eq.s64 	%p3, %rd1, 0;
	setp.eq.s32 	%p4, %r31, 0;
	mov.u32 	%r74, 0;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB13_8;

	cvt.s64.s32 	%rd2, %r31;
	add.s64 	%rd56, %rd2, -1;
	and.b64  	%rd112, %rd2, 3;
	setp.lt.u64 	%p6, %rd56, 3;
	mov.u64 	%rd113, 0;
	mov.u64 	%rd109, %rd1;
	@%p6 bra 	$L__BB13_5;

	sub.s64 	%rd106, %rd2, %rd112;
	mov.u64 	%rd109, %rd1;

$L__BB13_4:
	ld.u8 	%rs5, [%rd109];
	and.b16  	%rs6, %rs5, 192;
	setp.ne.s16 	%p7, %rs6, 128;
	selp.u64 	%rd58, 1, 0, %p7;
	add.s64 	%rd59, %rd113, %rd58;
	ld.u8 	%rs7, [%rd109+1];
	and.b16  	%rs8, %rs7, 192;
	setp.ne.s16 	%p8, %rs8, 128;
	selp.u64 	%rd60, 1, 0, %p8;
	add.s64 	%rd61, %rd59, %rd60;
	ld.u8 	%rs9, [%rd109+2];
	and.b16  	%rs10, %rs9, 192;
	setp.ne.s16 	%p9, %rs10, 128;
	selp.u64 	%rd62, 1, 0, %p9;
	add.s64 	%rd63, %rd61, %rd62;
	ld.u8 	%rs11, [%rd109+3];
	and.b16  	%rs12, %rs11, 192;
	setp.ne.s16 	%p10, %rs12, 128;
	selp.u64 	%rd64, 1, 0, %p10;
	add.s64 	%rd113, %rd63, %rd64;
	add.s64 	%rd109, %rd109, 4;
	add.s64 	%rd106, %rd106, -4;
	setp.ne.s64 	%p11, %rd106, 0;
	@%p11 bra 	$L__BB13_4;

$L__BB13_5:
	setp.eq.s64 	%p12, %rd112, 0;
	@%p12 bra 	$L__BB13_7;

$L__BB13_6:
	.pragma "nounroll";
	ld.u8 	%rs13, [%rd109];
	and.b16  	%rs14, %rs13, 192;
	setp.ne.s16 	%p13, %rs14, 128;
	selp.u64 	%rd65, 1, 0, %p13;
	add.s64 	%rd113, %rd113, %rd65;
	add.s64 	%rd109, %rd109, 1;
	add.s64 	%rd112, %rd112, -1;
	setp.ne.s64 	%p14, %rd112, 0;
	@%p14 bra 	$L__BB13_6;

$L__BB13_7:
	cvt.u32.u64 	%r74, %rd113;

$L__BB13_8:
	setp.eq.s32 	%p15, %r31, 0;
	mov.u32 	%r34, 0;
	mov.u32 	%r82, %r34;
	@%p15 bra 	$L__BB13_43;

	cvt.u64.u32 	%rd66, %r31;
	and.b64  	%rd21, %rd66, 3;
	setp.eq.s64 	%p17, %rd1, 0;
	mov.u16 	%rs45, 0;
	cvt.s64.s32 	%rd86, %r31;
	add.s64 	%rd87, %rd86, -1;
	setp.lt.u64 	%p34, %rd87, 3;
	mov.u64 	%rd85, 0;
	setp.eq.s64 	%p40, %rd21, 0;
	setp.eq.s64 	%p26, %rd21, 1;
	mov.u16 	%rs50, %rs51;
	mov.u32 	%r75, %r34;
	mov.u32 	%r76, %r34;

$L__BB13_10:
	setp.ne.s32 	%p16, %r74, -1;
	or.pred  	%p18, %p16, %p17;
	selp.b32 	%r77, %r74, 0, %p16;
	@%p18 bra 	$L__BB13_19;

	mov.u64 	%rd120, 0;
	mov.u64 	%rd118, %rd1;
	@%p34 bra 	$L__BB13_14;

	and.b64  	%rd73, %rd86, 3;
	sub.s64 	%rd114, %rd73, %rd86;
	mov.u64 	%rd118, %rd1;

$L__BB13_13:
	ld.u8 	%rs15, [%rd118];
	and.b16  	%rs16, %rs15, 192;
	setp.ne.s16 	%p20, %rs16, 128;
	selp.u64 	%rd74, 1, 0, %p20;
	add.s64 	%rd75, %rd120, %rd74;
	ld.u8 	%rs17, [%rd118+1];
	and.b16  	%rs18, %rs17, 192;
	setp.ne.s16 	%p21, %rs18, 128;
	selp.u64 	%rd76, 1, 0, %p21;
	add.s64 	%rd77, %rd75, %rd76;
	ld.u8 	%rs19, [%rd118+2];
	and.b16  	%rs20, %rs19, 192;
	setp.ne.s16 	%p22, %rs20, 128;
	selp.u64 	%rd78, 1, 0, %p22;
	add.s64 	%rd79, %rd77, %rd78;
	ld.u8 	%rs21, [%rd118+3];
	and.b16  	%rs22, %rs21, 192;
	setp.ne.s16 	%p23, %rs22, 128;
	selp.u64 	%rd80, 1, 0, %p23;
	add.s64 	%rd120, %rd79, %rd80;
	add.s64 	%rd118, %rd118, 4;
	add.s64 	%rd114, %rd114, 4;
	setp.ne.s64 	%p24, %rd114, 0;
	@%p24 bra 	$L__BB13_13;

$L__BB13_14:
	@%p40 bra 	$L__BB13_18;

	ld.u8 	%rs23, [%rd118];
	and.b16  	%rs24, %rs23, 192;
	setp.ne.s16 	%p27, %rs24, 128;
	selp.u64 	%rd81, 1, 0, %p27;
	add.s64 	%rd120, %rd120, %rd81;
	@%p26 bra 	$L__BB13_18;

	setp.eq.s64 	%p28, %rd21, 2;
	ld.u8 	%rs25, [%rd118+1];
	and.b16  	%rs26, %rs25, 192;
	setp.ne.s16 	%p29, %rs26, 128;
	selp.u64 	%rd82, 1, 0, %p29;
	add.s64 	%rd120, %rd120, %rd82;
	@%p28 bra 	$L__BB13_18;

	ld.u8 	%rs27, [%rd118+2];
	and.b16  	%rs28, %rs27, 192;
	setp.ne.s16 	%p30, %rs28, 128;
	selp.u64 	%rd83, 1, 0, %p30;
	add.s64 	%rd120, %rd120, %rd83;

$L__BB13_18:
	cvt.u32.u64 	%r77, %rd120;

$L__BB13_19:
	setp.ne.s32 	%p31, %r77, -1;
	or.pred  	%p33, %p31, %p17;
	selp.b32 	%r74, %r77, 0, %p31;
	@%p33 bra 	$L__BB13_28;

	mov.u64 	%rd125, %rd1;
	mov.u64 	%rd127, %rd85;
	@%p34 bra 	$L__BB13_23;

	and.b64  	%rd90, %rd86, 3;
	sub.s64 	%rd123, %rd86, %rd90;
	mov.u64 	%rd125, %rd1;
	mov.u64 	%rd127, %rd85;

$L__BB13_22:
	ld.u8 	%rs29, [%rd125];
	and.b16  	%rs30, %rs29, 192;
	setp.ne.s16 	%p35, %rs30, 128;
	selp.u64 	%rd91, 1, 0, %p35;
	add.s64 	%rd92, %rd127, %rd91;
	ld.u8 	%rs31, [%rd125+1];
	and.b16  	%rs32, %rs31, 192;
	setp.ne.s16 	%p36, %rs32, 128;
	selp.u64 	%rd93, 1, 0, %p36;
	add.s64 	%rd94, %rd92, %rd93;
	ld.u8 	%rs33, [%rd125+2];
	and.b16  	%rs34, %rs33, 192;
	setp.ne.s16 	%p37, %rs34, 128;
	selp.u64 	%rd95, 1, 0, %p37;
	add.s64 	%rd96, %rd94, %rd95;
	ld.u8 	%rs35, [%rd125+3];
	and.b16  	%rs36, %rs35, 192;
	setp.ne.s16 	%p38, %rs36, 128;
	selp.u64 	%rd97, 1, 0, %p38;
	add.s64 	%rd127, %rd96, %rd97;
	add.s64 	%rd125, %rd125, 4;
	add.s64 	%rd123, %rd123, -4;
	setp.ne.s64 	%p39, %rd123, 0;
	@%p39 bra 	$L__BB13_22;

$L__BB13_23:
	@%p40 bra 	$L__BB13_27;

	ld.u8 	%rs37, [%rd125];
	and.b16  	%rs38, %rs37, 192;
	setp.ne.s16 	%p42, %rs38, 128;
	selp.u64 	%rd98, 1, 0, %p42;
	add.s64 	%rd127, %rd127, %rd98;
	@%p26 bra 	$L__BB13_27;

	setp.eq.s64 	%p43, %rd21, 2;
	ld.u8 	%rs39, [%rd125+1];
	and.b16  	%rs40, %rs39, 192;
	setp.ne.s16 	%p44, %rs40, 128;
	selp.u64 	%rd99, 1, 0, %p44;
	add.s64 	%rd127, %rd127, %rd99;
	@%p43 bra 	$L__BB13_27;

	ld.u8 	%rs41, [%rd125+2];
	and.b16  	%rs42, %rs41, 192;
	setp.ne.s16 	%p45, %rs42, 128;
	selp.u64 	%rd100, 1, 0, %p45;
	add.s64 	%rd127, %rd127, %rd100;

$L__BB13_27:
	cvt.u32.u64 	%r74, %rd127;

$L__BB13_28:
	setp.eq.s32 	%p46, %r75, %r77;
	mov.u16 	%rs51, %rs50;
	@%p46 bra 	$L__BB13_43;

	cvt.s64.s32 	%rd101, %r76;
	add.s64 	%rd50, %rd1, %rd101;
	ld.u8 	%rs43, [%rd50];
	setp.gt.u16 	%p47, %rs43, 239;
	selp.b32 	%r38, 2, 1, %p47;
	setp.gt.u16 	%p48, %rs43, 223;
	selp.u32 	%r39, 1, 0, %p48;
	add.s32 	%r40, %r38, %r39;
	setp.gt.u16 	%p49, %rs43, 191;
	selp.u32 	%r41, 1, 0, %p49;
	add.s32 	%r42, %r40, %r41;
	and.b16  	%rs44, %rs43, 192;
	setp.eq.s16 	%p50, %rs44, 128;
	selp.b32 	%r43, -1, 0, %p50;
	add.s32 	%r15, %r42, %r43;
	cvt.u32.u16 	%r79, %rs43;
	setp.lt.s32 	%p51, %r15, 2;
	@%p51 bra 	$L__BB13_33;

	ld.u8 	%r44, [%rd50+1];
	prmt.b32 	%r79, %r79, %r44, 8452;
	setp.eq.s32 	%p52, %r15, 2;
	@%p52 bra 	$L__BB13_33;

	ld.u8 	%r45, [%rd50+2];
	prmt.b32 	%r79, %r79, %r45, 8452;
	setp.lt.s32 	%p53, %r15, 4;
	@%p53 bra 	$L__BB13_33;

	ld.u8 	%r46, [%rd50+3];
	prmt.b32 	%r79, %r79, %r46, 8452;

$L__BB13_33:
	setp.lt.u32 	%p54, %r79, 128;
	mov.u32 	%r80, %r79;
	@%p54 bra 	$L__BB13_40;

	setp.lt.u32 	%p55, %r79, 57344;
	@%p55 bra 	$L__BB13_39;
	bra.uni 	$L__BB13_35;

$L__BB13_39:
	and.b32  	%r63, %r79, 7936;
	shr.u32 	%r64, %r63, 2;
	and.b32  	%r65, %r79, 63;
	or.b32  	%r80, %r64, %r65;
	bra.uni 	$L__BB13_40;

$L__BB13_35:
	setp.lt.u32 	%p56, %r79, 15728640;
	@%p56 bra 	$L__BB13_38;
	bra.uni 	$L__BB13_36;

$L__BB13_38:
	and.b32  	%r57, %r79, 983040;
	shr.u32 	%r58, %r57, 4;
	and.b32  	%r59, %r79, 16128;
	shr.u32 	%r60, %r59, 2;
	and.b32  	%r61, %r79, 63;
	or.b32  	%r62, %r60, %r61;
	or.b32  	%r80, %r62, %r58;
	bra.uni 	$L__BB13_40;

$L__BB13_36:
	setp.gt.u32 	%p57, %r79, -134217728;
	mov.u32 	%r80, 0;
	@%p57 bra 	$L__BB13_40;

	and.b32  	%r48, %r79, 50331648;
	shr.u32 	%r49, %r48, 6;
	and.b32  	%r50, %r79, 4128768;
	shr.u32 	%r51, %r50, 4;
	and.b32  	%r52, %r79, 16128;
	shr.u32 	%r53, %r52, 2;
	and.b32  	%r54, %r79, 63;
	or.b32  	%r55, %r51, %r54;
	or.b32  	%r56, %r55, %r49;
	or.b32  	%r80, %r56, %r53;

$L__BB13_40:
	setp.gt.u32 	%p58, %r80, 65535;
	mov.u32 	%r81, 0;
	@%p58 bra 	$L__BB13_42;

	cvt.u64.u32 	%rd102, %r80;
	add.s64 	%rd103, %rd52, %rd102;
	ld.u8 	%r81, [%rd103];

$L__BB13_42:
	and.b32  	%r67, %r81, 96;
	setp.eq.s32 	%p59, %r67, 0;
	bfe.u32 	%r68, %r81, 5, 1;
	cvt.u16.u32 	%rs46, %r68;
	selp.b16 	%rs50, %rs50, %rs46, %p59;
	and.b16  	%rs47, %rs50, 255;
	setp.ne.s32 	%p60, %r67, 0;
	selp.u32 	%r69, 1, 0, %p60;
	add.s32 	%r82, %r82, %r69;
	add.s32 	%r70, %r15, %r76;
	setp.lt.s32 	%p61, %r76, %r31;
	selp.b32 	%r76, %r70, %r76, %p61;
	add.s32 	%r75, %r75, 1;
	setp.ne.s16 	%p62, %rs47, 0;
	mov.u16 	%rs51, %rs45;
	@%p62 bra 	$L__BB13_10;

$L__BB13_43:
	setp.ne.s32 	%p63, %r82, 0;
	and.b16  	%rs48, %rs51, 255;
	setp.ne.s16 	%p64, %rs48, 0;
	and.pred  	%p65, %p64, %p63;
	selp.u16 	%rs49, 1, 0, %p65;
	st.u8 	[%rd51], %rs49;
	st.param.b32 	[func_retval0+0], %r34;
	ret;

}
	// .globl	pyisspace
.visible .func  (.param .b32 func_retval0) pyisspace(
	.param .b64 pyisspace_param_0,
	.param .b64 pyisspace_param_1,
	.param .b64 pyisspace_param_2
)
{
	.reg .pred 	%p<67>;
	.reg .b16 	%rs<52>;
	.reg .b32 	%r<83>;
	.reg .b64 	%rd<128>;


	ld.param.u64 	%rd51, [pyisspace_param_0];
	ld.param.u64 	%rd53, [pyisspace_param_1];
	ld.param.u64 	%rd52, [pyisspace_param_2];
	ld.u64 	%rd1, [%rd53];
	ld.v2.u32 	{%r31, %r74}, [%rd53+8];
	setp.ne.s32 	%p1, %r31, 0;
	selp.u16 	%rs51, 1, 0, %p1;
	setp.ne.s32 	%p2, %r74, -1;
	@%p2 bra 	$L__BB14_8;

	setp.eq.s64 	%p3, %rd1, 0;
	setp.eq.s32 	%p4, %r31, 0;
	mov.u32 	%r74, 0;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB14_8;

	cvt.s64.s32 	%rd2, %r31;
	add.s64 	%rd56, %rd2, -1;
	and.b64  	%rd112, %rd2, 3;
	setp.lt.u64 	%p6, %rd56, 3;
	mov.u64 	%rd113, 0;
	mov.u64 	%rd109, %rd1;
	@%p6 bra 	$L__BB14_5;

	sub.s64 	%rd106, %rd2, %rd112;
	mov.u64 	%rd109, %rd1;

$L__BB14_4:
	ld.u8 	%rs5, [%rd109];
	and.b16  	%rs6, %rs5, 192;
	setp.ne.s16 	%p7, %rs6, 128;
	selp.u64 	%rd58, 1, 0, %p7;
	add.s64 	%rd59, %rd113, %rd58;
	ld.u8 	%rs7, [%rd109+1];
	and.b16  	%rs8, %rs7, 192;
	setp.ne.s16 	%p8, %rs8, 128;
	selp.u64 	%rd60, 1, 0, %p8;
	add.s64 	%rd61, %rd59, %rd60;
	ld.u8 	%rs9, [%rd109+2];
	and.b16  	%rs10, %rs9, 192;
	setp.ne.s16 	%p9, %rs10, 128;
	selp.u64 	%rd62, 1, 0, %p9;
	add.s64 	%rd63, %rd61, %rd62;
	ld.u8 	%rs11, [%rd109+3];
	and.b16  	%rs12, %rs11, 192;
	setp.ne.s16 	%p10, %rs12, 128;
	selp.u64 	%rd64, 1, 0, %p10;
	add.s64 	%rd113, %rd63, %rd64;
	add.s64 	%rd109, %rd109, 4;
	add.s64 	%rd106, %rd106, -4;
	setp.ne.s64 	%p11, %rd106, 0;
	@%p11 bra 	$L__BB14_4;

$L__BB14_5:
	setp.eq.s64 	%p12, %rd112, 0;
	@%p12 bra 	$L__BB14_7;

$L__BB14_6:
	.pragma "nounroll";
	ld.u8 	%rs13, [%rd109];
	and.b16  	%rs14, %rs13, 192;
	setp.ne.s16 	%p13, %rs14, 128;
	selp.u64 	%rd65, 1, 0, %p13;
	add.s64 	%rd113, %rd113, %rd65;
	add.s64 	%rd109, %rd109, 1;
	add.s64 	%rd112, %rd112, -1;
	setp.ne.s64 	%p14, %rd112, 0;
	@%p14 bra 	$L__BB14_6;

$L__BB14_7:
	cvt.u32.u64 	%r74, %rd113;

$L__BB14_8:
	setp.eq.s32 	%p15, %r31, 0;
	mov.u32 	%r34, 0;
	mov.u32 	%r82, %r34;
	@%p15 bra 	$L__BB14_43;

	cvt.u64.u32 	%rd66, %r31;
	and.b64  	%rd21, %rd66, 3;
	setp.eq.s64 	%p17, %rd1, 0;
	mov.u16 	%rs45, 0;
	cvt.s64.s32 	%rd86, %r31;
	add.s64 	%rd87, %rd86, -1;
	setp.lt.u64 	%p34, %rd87, 3;
	mov.u64 	%rd85, 0;
	setp.eq.s64 	%p25, %rd21, 0;
	mov.u16 	%rs50, %rs51;
	mov.u32 	%r75, %r34;
	mov.u32 	%r76, %r34;

$L__BB14_10:
	setp.ne.s32 	%p16, %r74, -1;
	or.pred  	%p18, %p16, %p17;
	selp.b32 	%r77, %r74, 0, %p16;
	@%p18 bra 	$L__BB14_19;

	mov.u64 	%rd120, 0;
	mov.u64 	%rd118, %rd1;
	@%p34 bra 	$L__BB14_14;

	and.b64  	%rd73, %rd86, 3;
	sub.s64 	%rd114, %rd73, %rd86;
	mov.u64 	%rd118, %rd1;

$L__BB14_13:
	ld.u8 	%rs15, [%rd118];
	and.b16  	%rs16, %rs15, 192;
	setp.ne.s16 	%p20, %rs16, 128;
	selp.u64 	%rd74, 1, 0, %p20;
	add.s64 	%rd75, %rd120, %rd74;
	ld.u8 	%rs17, [%rd118+1];
	and.b16  	%rs18, %rs17, 192;
	setp.ne.s16 	%p21, %rs18, 128;
	selp.u64 	%rd76, 1, 0, %p21;
	add.s64 	%rd77, %rd75, %rd76;
	ld.u8 	%rs19, [%rd118+2];
	and.b16  	%rs20, %rs19, 192;
	setp.ne.s16 	%p22, %rs20, 128;
	selp.u64 	%rd78, 1, 0, %p22;
	add.s64 	%rd79, %rd77, %rd78;
	ld.u8 	%rs21, [%rd118+3];
	and.b16  	%rs22, %rs21, 192;
	setp.ne.s16 	%p23, %rs22, 128;
	selp.u64 	%rd80, 1, 0, %p23;
	add.s64 	%rd120, %rd79, %rd80;
	add.s64 	%rd118, %rd118, 4;
	add.s64 	%rd114, %rd114, 4;
	setp.ne.s64 	%p24, %rd114, 0;
	@%p24 bra 	$L__BB14_13;

$L__BB14_14:
	@%p25 bra 	$L__BB14_18;

	setp.eq.s64 	%p26, %rd21, 1;
	ld.u8 	%rs23, [%rd118];
	and.b16  	%rs24, %rs23, 192;
	setp.ne.s16 	%p27, %rs24, 128;
	selp.u64 	%rd81, 1, 0, %p27;
	add.s64 	%rd120, %rd120, %rd81;
	@%p26 bra 	$L__BB14_18;

	setp.eq.s64 	%p28, %rd21, 2;
	ld.u8 	%rs25, [%rd118+1];
	and.b16  	%rs26, %rs25, 192;
	setp.ne.s16 	%p29, %rs26, 128;
	selp.u64 	%rd82, 1, 0, %p29;
	add.s64 	%rd120, %rd120, %rd82;
	@%p28 bra 	$L__BB14_18;

	ld.u8 	%rs27, [%rd118+2];
	and.b16  	%rs28, %rs27, 192;
	setp.ne.s16 	%p30, %rs28, 128;
	selp.u64 	%rd83, 1, 0, %p30;
	add.s64 	%rd120, %rd120, %rd83;

$L__BB14_18:
	cvt.u32.u64 	%r77, %rd120;

$L__BB14_19:
	setp.ne.s32 	%p31, %r77, -1;
	or.pred  	%p33, %p31, %p17;
	selp.b32 	%r74, %r77, 0, %p31;
	@%p33 bra 	$L__BB14_28;

	mov.u64 	%rd125, %rd1;
	mov.u64 	%rd127, %rd85;
	@%p34 bra 	$L__BB14_23;

	and.b64  	%rd90, %rd86, 3;
	sub.s64 	%rd123, %rd86, %rd90;
	mov.u64 	%rd125, %rd1;
	mov.u64 	%rd127, %rd85;

$L__BB14_22:
	ld.u8 	%rs29, [%rd125];
	and.b16  	%rs30, %rs29, 192;
	setp.ne.s16 	%p35, %rs30, 128;
	selp.u64 	%rd91, 1, 0, %p35;
	add.s64 	%rd92, %rd127, %rd91;
	ld.u8 	%rs31, [%rd125+1];
	and.b16  	%rs32, %rs31, 192;
	setp.ne.s16 	%p36, %rs32, 128;
	selp.u64 	%rd93, 1, 0, %p36;
	add.s64 	%rd94, %rd92, %rd93;
	ld.u8 	%rs33, [%rd125+2];
	and.b16  	%rs34, %rs33, 192;
	setp.ne.s16 	%p37, %rs34, 128;
	selp.u64 	%rd95, 1, 0, %p37;
	add.s64 	%rd96, %rd94, %rd95;
	ld.u8 	%rs35, [%rd125+3];
	and.b16  	%rs36, %rs35, 192;
	setp.ne.s16 	%p38, %rs36, 128;
	selp.u64 	%rd97, 1, 0, %p38;
	add.s64 	%rd127, %rd96, %rd97;
	add.s64 	%rd125, %rd125, 4;
	add.s64 	%rd123, %rd123, -4;
	setp.ne.s64 	%p39, %rd123, 0;
	@%p39 bra 	$L__BB14_22;

$L__BB14_23:
	@%p25 bra 	$L__BB14_27;

	setp.eq.s64 	%p41, %rd21, 1;
	ld.u8 	%rs37, [%rd125];
	and.b16  	%rs38, %rs37, 192;
	setp.ne.s16 	%p42, %rs38, 128;
	selp.u64 	%rd98, 1, 0, %p42;
	add.s64 	%rd127, %rd127, %rd98;
	@%p41 bra 	$L__BB14_27;

	setp.eq.s64 	%p43, %rd21, 2;
	ld.u8 	%rs39, [%rd125+1];
	and.b16  	%rs40, %rs39, 192;
	setp.ne.s16 	%p44, %rs40, 128;
	selp.u64 	%rd99, 1, 0, %p44;
	add.s64 	%rd127, %rd127, %rd99;
	@%p43 bra 	$L__BB14_27;

	ld.u8 	%rs41, [%rd125+2];
	and.b16  	%rs42, %rs41, 192;
	setp.ne.s16 	%p45, %rs42, 128;
	selp.u64 	%rd100, 1, 0, %p45;
	add.s64 	%rd127, %rd127, %rd100;

$L__BB14_27:
	cvt.u32.u64 	%r74, %rd127;

$L__BB14_28:
	setp.eq.s32 	%p46, %r75, %r77;
	mov.u16 	%rs51, %rs50;
	@%p46 bra 	$L__BB14_43;

	cvt.s64.s32 	%rd101, %r76;
	add.s64 	%rd50, %rd1, %rd101;
	ld.u8 	%rs43, [%rd50];
	setp.gt.u16 	%p47, %rs43, 239;
	selp.b32 	%r38, 2, 1, %p47;
	setp.gt.u16 	%p48, %rs43, 223;
	selp.u32 	%r39, 1, 0, %p48;
	add.s32 	%r40, %r38, %r39;
	setp.gt.u16 	%p49, %rs43, 191;
	selp.u32 	%r41, 1, 0, %p49;
	add.s32 	%r42, %r40, %r41;
	and.b16  	%rs44, %rs43, 192;
	setp.eq.s16 	%p50, %rs44, 128;
	selp.b32 	%r43, -1, 0, %p50;
	add.s32 	%r15, %r42, %r43;
	cvt.u32.u16 	%r79, %rs43;
	setp.lt.s32 	%p51, %r15, 2;
	@%p51 bra 	$L__BB14_33;

	ld.u8 	%r44, [%rd50+1];
	prmt.b32 	%r79, %r79, %r44, 8452;
	setp.eq.s32 	%p52, %r15, 2;
	@%p52 bra 	$L__BB14_33;

	ld.u8 	%r45, [%rd50+2];
	prmt.b32 	%r79, %r79, %r45, 8452;
	setp.lt.s32 	%p53, %r15, 4;
	@%p53 bra 	$L__BB14_33;

	ld.u8 	%r46, [%rd50+3];
	prmt.b32 	%r79, %r79, %r46, 8452;

$L__BB14_33:
	setp.lt.u32 	%p54, %r79, 128;
	mov.u32 	%r80, %r79;
	@%p54 bra 	$L__BB14_40;

	setp.lt.u32 	%p55, %r79, 57344;
	@%p55 bra 	$L__BB14_39;
	bra.uni 	$L__BB14_35;

$L__BB14_39:
	and.b32  	%r63, %r79, 7936;
	shr.u32 	%r64, %r63, 2;
	and.b32  	%r65, %r79, 63;
	or.b32  	%r80, %r64, %r65;
	bra.uni 	$L__BB14_40;

$L__BB14_35:
	setp.lt.u32 	%p56, %r79, 15728640;
	@%p56 bra 	$L__BB14_38;
	bra.uni 	$L__BB14_36;

$L__BB14_38:
	and.b32  	%r57, %r79, 983040;
	shr.u32 	%r58, %r57, 4;
	and.b32  	%r59, %r79, 16128;
	shr.u32 	%r60, %r59, 2;
	and.b32  	%r61, %r79, 63;
	or.b32  	%r62, %r60, %r61;
	or.b32  	%r80, %r62, %r58;
	bra.uni 	$L__BB14_40;

$L__BB14_36:
	setp.gt.u32 	%p57, %r79, -134217728;
	mov.u32 	%r80, 0;
	@%p57 bra 	$L__BB14_40;

	and.b32  	%r48, %r79, 50331648;
	shr.u32 	%r49, %r48, 6;
	and.b32  	%r50, %r79, 4128768;
	shr.u32 	%r51, %r50, 4;
	and.b32  	%r52, %r79, 16128;
	shr.u32 	%r53, %r52, 2;
	and.b32  	%r54, %r79, 63;
	or.b32  	%r55, %r51, %r54;
	or.b32  	%r56, %r55, %r49;
	or.b32  	%r80, %r56, %r53;

$L__BB14_40:
	setp.gt.u32 	%p58, %r80, 65535;
	mov.u32 	%r81, 0;
	@%p58 bra 	$L__BB14_42;

	cvt.u64.u32 	%rd102, %r80;
	add.s64 	%rd103, %rd52, %rd102;
	ld.u8 	%r81, [%rd103];

$L__BB14_42:
	and.b32  	%r67, %r81, 127;
	setp.ne.s32 	%p59, %r67, 0;
	setp.eq.s32 	%p60, %r81, 0;
	or.pred  	%p61, %p60, %p59;
	bfe.u32 	%r68, %r81, 4, 1;
	cvt.u16.u32 	%rs46, %r68;
	selp.b16 	%rs50, %rs46, %rs50, %p61;
	and.b16  	%rs47, %rs50, 255;
	selp.u32 	%r69, 1, 0, %p61;
	add.s32 	%r82, %r82, %r69;
	add.s32 	%r70, %r15, %r76;
	setp.lt.s32 	%p62, %r76, %r31;
	selp.b32 	%r76, %r70, %r76, %p62;
	add.s32 	%r75, %r75, 1;
	setp.ne.s16 	%p63, %rs47, 0;
	mov.u16 	%rs51, %rs45;
	@%p63 bra 	$L__BB14_10;

$L__BB14_43:
	setp.ne.s32 	%p64, %r82, 0;
	and.b16  	%rs48, %rs51, 255;
	setp.ne.s16 	%p65, %rs48, 0;
	and.pred  	%p66, %p65, %p64;
	selp.u16 	%rs49, 1, 0, %p66;
	st.u8 	[%rd51], %rs49;
	st.param.b32 	[func_retval0+0], %r34;
	ret;

}
	// .globl	pyisdecimal
.visible .func  (.param .b32 func_retval0) pyisdecimal(
	.param .b64 pyisdecimal_param_0,
	.param .b64 pyisdecimal_param_1,
	.param .b64 pyisdecimal_param_2
)
{
	.reg .pred 	%p<67>;
	.reg .b16 	%rs<53>;
	.reg .b32 	%r<82>;
	.reg .b64 	%rd<128>;


	ld.param.u64 	%rd51, [pyisdecimal_param_0];
	ld.param.u64 	%rd53, [pyisdecimal_param_1];
	ld.param.u64 	%rd52, [pyisdecimal_param_2];
	ld.u64 	%rd1, [%rd53];
	ld.v2.u32 	{%r31, %r73}, [%rd53+8];
	setp.ne.s32 	%p1, %r31, 0;
	selp.u16 	%rs52, 1, 0, %p1;
	setp.ne.s32 	%p2, %r73, -1;
	@%p2 bra 	$L__BB15_8;

	setp.eq.s64 	%p3, %rd1, 0;
	setp.eq.s32 	%p4, %r31, 0;
	mov.u32 	%r73, 0;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB15_8;

	cvt.s64.s32 	%rd2, %r31;
	add.s64 	%rd56, %rd2, -1;
	and.b64  	%rd112, %rd2, 3;
	setp.lt.u64 	%p6, %rd56, 3;
	mov.u64 	%rd113, 0;
	mov.u64 	%rd109, %rd1;
	@%p6 bra 	$L__BB15_5;

	sub.s64 	%rd106, %rd2, %rd112;
	mov.u64 	%rd109, %rd1;

$L__BB15_4:
	ld.u8 	%rs5, [%rd109];
	and.b16  	%rs6, %rs5, 192;
	setp.ne.s16 	%p7, %rs6, 128;
	selp.u64 	%rd58, 1, 0, %p7;
	add.s64 	%rd59, %rd113, %rd58;
	ld.u8 	%rs7, [%rd109+1];
	and.b16  	%rs8, %rs7, 192;
	setp.ne.s16 	%p8, %rs8, 128;
	selp.u64 	%rd60, 1, 0, %p8;
	add.s64 	%rd61, %rd59, %rd60;
	ld.u8 	%rs9, [%rd109+2];
	and.b16  	%rs10, %rs9, 192;
	setp.ne.s16 	%p9, %rs10, 128;
	selp.u64 	%rd62, 1, 0, %p9;
	add.s64 	%rd63, %rd61, %rd62;
	ld.u8 	%rs11, [%rd109+3];
	and.b16  	%rs12, %rs11, 192;
	setp.ne.s16 	%p10, %rs12, 128;
	selp.u64 	%rd64, 1, 0, %p10;
	add.s64 	%rd113, %rd63, %rd64;
	add.s64 	%rd109, %rd109, 4;
	add.s64 	%rd106, %rd106, -4;
	setp.ne.s64 	%p11, %rd106, 0;
	@%p11 bra 	$L__BB15_4;

$L__BB15_5:
	setp.eq.s64 	%p12, %rd112, 0;
	@%p12 bra 	$L__BB15_7;

$L__BB15_6:
	.pragma "nounroll";
	ld.u8 	%rs13, [%rd109];
	and.b16  	%rs14, %rs13, 192;
	setp.ne.s16 	%p13, %rs14, 128;
	selp.u64 	%rd65, 1, 0, %p13;
	add.s64 	%rd113, %rd113, %rd65;
	add.s64 	%rd109, %rd109, 1;
	add.s64 	%rd112, %rd112, -1;
	setp.ne.s64 	%p14, %rd112, 0;
	@%p14 bra 	$L__BB15_6;

$L__BB15_7:
	cvt.u32.u64 	%r73, %rd113;

$L__BB15_8:
	setp.eq.s32 	%p15, %r31, 0;
	mov.u32 	%r34, 0;
	mov.u32 	%r81, %r34;
	@%p15 bra 	$L__BB15_43;

	cvt.u64.u32 	%rd66, %r31;
	and.b64  	%rd21, %rd66, 3;
	setp.eq.s64 	%p17, %rd1, 0;
	mov.u16 	%rs45, 0;
	cvt.s64.s32 	%rd86, %r31;
	add.s64 	%rd87, %rd86, -1;
	setp.lt.u64 	%p34, %rd87, 3;
	mov.u64 	%rd85, 0;
	setp.eq.s64 	%p25, %rd21, 0;
	mov.u16 	%rs51, %rs52;
	mov.u32 	%r74, %r34;
	mov.u32 	%r75, %r34;

$L__BB15_10:
	setp.ne.s32 	%p16, %r73, -1;
	or.pred  	%p18, %p16, %p17;
	selp.b32 	%r76, %r73, 0, %p16;
	@%p18 bra 	$L__BB15_19;

	mov.u64 	%rd120, 0;
	mov.u64 	%rd118, %rd1;
	@%p34 bra 	$L__BB15_14;

	and.b64  	%rd73, %rd86, 3;
	sub.s64 	%rd114, %rd73, %rd86;
	mov.u64 	%rd118, %rd1;

$L__BB15_13:
	ld.u8 	%rs15, [%rd118];
	and.b16  	%rs16, %rs15, 192;
	setp.ne.s16 	%p20, %rs16, 128;
	selp.u64 	%rd74, 1, 0, %p20;
	add.s64 	%rd75, %rd120, %rd74;
	ld.u8 	%rs17, [%rd118+1];
	and.b16  	%rs18, %rs17, 192;
	setp.ne.s16 	%p21, %rs18, 128;
	selp.u64 	%rd76, 1, 0, %p21;
	add.s64 	%rd77, %rd75, %rd76;
	ld.u8 	%rs19, [%rd118+2];
	and.b16  	%rs20, %rs19, 192;
	setp.ne.s16 	%p22, %rs20, 128;
	selp.u64 	%rd78, 1, 0, %p22;
	add.s64 	%rd79, %rd77, %rd78;
	ld.u8 	%rs21, [%rd118+3];
	and.b16  	%rs22, %rs21, 192;
	setp.ne.s16 	%p23, %rs22, 128;
	selp.u64 	%rd80, 1, 0, %p23;
	add.s64 	%rd120, %rd79, %rd80;
	add.s64 	%rd118, %rd118, 4;
	add.s64 	%rd114, %rd114, 4;
	setp.ne.s64 	%p24, %rd114, 0;
	@%p24 bra 	$L__BB15_13;

$L__BB15_14:
	@%p25 bra 	$L__BB15_18;

	setp.eq.s64 	%p26, %rd21, 1;
	ld.u8 	%rs23, [%rd118];
	and.b16  	%rs24, %rs23, 192;
	setp.ne.s16 	%p27, %rs24, 128;
	selp.u64 	%rd81, 1, 0, %p27;
	add.s64 	%rd120, %rd120, %rd81;
	@%p26 bra 	$L__BB15_18;

	setp.eq.s64 	%p28, %rd21, 2;
	ld.u8 	%rs25, [%rd118+1];
	and.b16  	%rs26, %rs25, 192;
	setp.ne.s16 	%p29, %rs26, 128;
	selp.u64 	%rd82, 1, 0, %p29;
	add.s64 	%rd120, %rd120, %rd82;
	@%p28 bra 	$L__BB15_18;

	ld.u8 	%rs27, [%rd118+2];
	and.b16  	%rs28, %rs27, 192;
	setp.ne.s16 	%p30, %rs28, 128;
	selp.u64 	%rd83, 1, 0, %p30;
	add.s64 	%rd120, %rd120, %rd83;

$L__BB15_18:
	cvt.u32.u64 	%r76, %rd120;

$L__BB15_19:
	setp.ne.s32 	%p31, %r76, -1;
	or.pred  	%p33, %p31, %p17;
	selp.b32 	%r73, %r76, 0, %p31;
	@%p33 bra 	$L__BB15_28;

	mov.u64 	%rd125, %rd1;
	mov.u64 	%rd127, %rd85;
	@%p34 bra 	$L__BB15_23;

	and.b64  	%rd90, %rd86, 3;
	sub.s64 	%rd123, %rd86, %rd90;
	mov.u64 	%rd125, %rd1;
	mov.u64 	%rd127, %rd85;

$L__BB15_22:
	ld.u8 	%rs29, [%rd125];
	and.b16  	%rs30, %rs29, 192;
	setp.ne.s16 	%p35, %rs30, 128;
	selp.u64 	%rd91, 1, 0, %p35;
	add.s64 	%rd92, %rd127, %rd91;
	ld.u8 	%rs31, [%rd125+1];
	and.b16  	%rs32, %rs31, 192;
	setp.ne.s16 	%p36, %rs32, 128;
	selp.u64 	%rd93, 1, 0, %p36;
	add.s64 	%rd94, %rd92, %rd93;
	ld.u8 	%rs33, [%rd125+2];
	and.b16  	%rs34, %rs33, 192;
	setp.ne.s16 	%p37, %rs34, 128;
	selp.u64 	%rd95, 1, 0, %p37;
	add.s64 	%rd96, %rd94, %rd95;
	ld.u8 	%rs35, [%rd125+3];
	and.b16  	%rs36, %rs35, 192;
	setp.ne.s16 	%p38, %rs36, 128;
	selp.u64 	%rd97, 1, 0, %p38;
	add.s64 	%rd127, %rd96, %rd97;
	add.s64 	%rd125, %rd125, 4;
	add.s64 	%rd123, %rd123, -4;
	setp.ne.s64 	%p39, %rd123, 0;
	@%p39 bra 	$L__BB15_22;

$L__BB15_23:
	@%p25 bra 	$L__BB15_27;

	setp.eq.s64 	%p41, %rd21, 1;
	ld.u8 	%rs37, [%rd125];
	and.b16  	%rs38, %rs37, 192;
	setp.ne.s16 	%p42, %rs38, 128;
	selp.u64 	%rd98, 1, 0, %p42;
	add.s64 	%rd127, %rd127, %rd98;
	@%p41 bra 	$L__BB15_27;

	setp.eq.s64 	%p43, %rd21, 2;
	ld.u8 	%rs39, [%rd125+1];
	and.b16  	%rs40, %rs39, 192;
	setp.ne.s16 	%p44, %rs40, 128;
	selp.u64 	%rd99, 1, 0, %p44;
	add.s64 	%rd127, %rd127, %rd99;
	@%p43 bra 	$L__BB15_27;

	ld.u8 	%rs41, [%rd125+2];
	and.b16  	%rs42, %rs41, 192;
	setp.ne.s16 	%p45, %rs42, 128;
	selp.u64 	%rd100, 1, 0, %p45;
	add.s64 	%rd127, %rd127, %rd100;

$L__BB15_27:
	cvt.u32.u64 	%r73, %rd127;

$L__BB15_28:
	setp.eq.s32 	%p46, %r74, %r76;
	mov.u16 	%rs52, %rs51;
	@%p46 bra 	$L__BB15_43;

	cvt.s64.s32 	%rd101, %r75;
	add.s64 	%rd50, %rd1, %rd101;
	ld.u8 	%rs43, [%rd50];
	setp.gt.u16 	%p47, %rs43, 239;
	selp.b32 	%r38, 2, 1, %p47;
	setp.gt.u16 	%p48, %rs43, 223;
	selp.u32 	%r39, 1, 0, %p48;
	add.s32 	%r40, %r38, %r39;
	setp.gt.u16 	%p49, %rs43, 191;
	selp.u32 	%r41, 1, 0, %p49;
	add.s32 	%r42, %r40, %r41;
	and.b16  	%rs44, %rs43, 192;
	setp.eq.s16 	%p50, %rs44, 128;
	selp.b32 	%r43, -1, 0, %p50;
	add.s32 	%r15, %r42, %r43;
	cvt.u32.u16 	%r78, %rs43;
	setp.lt.s32 	%p51, %r15, 2;
	@%p51 bra 	$L__BB15_33;

	ld.u8 	%r44, [%rd50+1];
	prmt.b32 	%r78, %r78, %r44, 8452;
	setp.eq.s32 	%p52, %r15, 2;
	@%p52 bra 	$L__BB15_33;

	ld.u8 	%r45, [%rd50+2];
	prmt.b32 	%r78, %r78, %r45, 8452;
	setp.lt.s32 	%p53, %r15, 4;
	@%p53 bra 	$L__BB15_33;

	ld.u8 	%r46, [%rd50+3];
	prmt.b32 	%r78, %r78, %r46, 8452;

$L__BB15_33:
	setp.lt.u32 	%p54, %r78, 128;
	mov.u32 	%r79, %r78;
	@%p54 bra 	$L__BB15_40;

	setp.lt.u32 	%p55, %r78, 57344;
	@%p55 bra 	$L__BB15_39;
	bra.uni 	$L__BB15_35;

$L__BB15_39:
	and.b32  	%r63, %r78, 7936;
	shr.u32 	%r64, %r63, 2;
	and.b32  	%r65, %r78, 63;
	or.b32  	%r79, %r64, %r65;
	bra.uni 	$L__BB15_40;

$L__BB15_35:
	setp.lt.u32 	%p56, %r78, 15728640;
	@%p56 bra 	$L__BB15_38;
	bra.uni 	$L__BB15_36;

$L__BB15_38:
	and.b32  	%r57, %r78, 983040;
	shr.u32 	%r58, %r57, 4;
	and.b32  	%r59, %r78, 16128;
	shr.u32 	%r60, %r59, 2;
	and.b32  	%r61, %r78, 63;
	or.b32  	%r62, %r60, %r61;
	or.b32  	%r79, %r62, %r58;
	bra.uni 	$L__BB15_40;

$L__BB15_36:
	setp.gt.u32 	%p57, %r78, -134217728;
	mov.u32 	%r79, 0;
	@%p57 bra 	$L__BB15_40;

	and.b32  	%r48, %r78, 50331648;
	shr.u32 	%r49, %r48, 6;
	and.b32  	%r50, %r78, 4128768;
	shr.u32 	%r51, %r50, 4;
	and.b32  	%r52, %r78, 16128;
	shr.u32 	%r53, %r52, 2;
	and.b32  	%r54, %r78, 63;
	or.b32  	%r55, %r51, %r54;
	or.b32  	%r56, %r55, %r49;
	or.b32  	%r79, %r56, %r53;

$L__BB15_40:
	setp.gt.u32 	%p58, %r79, 65535;
	mov.u32 	%r80, 0;
	@%p58 bra 	$L__BB15_42;

	cvt.u64.u32 	%rd102, %r79;
	add.s64 	%rd103, %rd52, %rd102;
	ld.u8 	%r80, [%rd103];

$L__BB15_42:
	and.b32  	%r67, %r80, 127;
	setp.ne.s32 	%p59, %r67, 0;
	setp.eq.s32 	%p60, %r80, 0;
	or.pred  	%p61, %p60, %p59;
	cvt.u16.u32 	%rs46, %r80;
	and.b16  	%rs47, %rs46, 1;
	selp.b16 	%rs51, %rs47, %rs51, %p61;
	and.b16  	%rs48, %rs51, 255;
	selp.u32 	%r68, 1, 0, %p61;
	add.s32 	%r81, %r81, %r68;
	add.s32 	%r69, %r15, %r75;
	setp.lt.s32 	%p62, %r75, %r31;
	selp.b32 	%r75, %r69, %r75, %p62;
	add.s32 	%r74, %r74, 1;
	setp.ne.s16 	%p63, %rs48, 0;
	mov.u16 	%rs52, %rs45;
	@%p63 bra 	$L__BB15_10;

$L__BB15_43:
	setp.ne.s32 	%p64, %r81, 0;
	and.b16  	%rs49, %rs52, 255;
	setp.ne.s16 	%p65, %rs49, 0;
	and.pred  	%p66, %p65, %p64;
	selp.u16 	%rs50, 1, 0, %p66;
	st.u8 	[%rd51], %rs50;
	st.param.b32 	[func_retval0+0], %r34;
	ret;

}
	// .globl	pyisnumeric
.visible .func  (.param .b32 func_retval0) pyisnumeric(
	.param .b64 pyisnumeric_param_0,
	.param .b64 pyisnumeric_param_1,
	.param .b64 pyisnumeric_param_2
)
{
	.reg .pred 	%p<67>;
	.reg .b16 	%rs<52>;
	.reg .b32 	%r<83>;
	.reg .b64 	%rd<128>;


	ld.param.u64 	%rd51, [pyisnumeric_param_0];
	ld.param.u64 	%rd53, [pyisnumeric_param_1];
	ld.param.u64 	%rd52, [pyisnumeric_param_2];
	ld.u64 	%rd1, [%rd53];
	ld.v2.u32 	{%r31, %r74}, [%rd53+8];
	setp.ne.s32 	%p1, %r31, 0;
	selp.u16 	%rs51, 1, 0, %p1;
	setp.ne.s32 	%p2, %r74, -1;
	@%p2 bra 	$L__BB16_8;

	setp.eq.s64 	%p3, %rd1, 0;
	setp.eq.s32 	%p4, %r31, 0;
	mov.u32 	%r74, 0;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB16_8;

	cvt.s64.s32 	%rd2, %r31;
	add.s64 	%rd56, %rd2, -1;
	and.b64  	%rd112, %rd2, 3;
	setp.lt.u64 	%p6, %rd56, 3;
	mov.u64 	%rd113, 0;
	mov.u64 	%rd109, %rd1;
	@%p6 bra 	$L__BB16_5;

	sub.s64 	%rd106, %rd2, %rd112;
	mov.u64 	%rd109, %rd1;

$L__BB16_4:
	ld.u8 	%rs5, [%rd109];
	and.b16  	%rs6, %rs5, 192;
	setp.ne.s16 	%p7, %rs6, 128;
	selp.u64 	%rd58, 1, 0, %p7;
	add.s64 	%rd59, %rd113, %rd58;
	ld.u8 	%rs7, [%rd109+1];
	and.b16  	%rs8, %rs7, 192;
	setp.ne.s16 	%p8, %rs8, 128;
	selp.u64 	%rd60, 1, 0, %p8;
	add.s64 	%rd61, %rd59, %rd60;
	ld.u8 	%rs9, [%rd109+2];
	and.b16  	%rs10, %rs9, 192;
	setp.ne.s16 	%p9, %rs10, 128;
	selp.u64 	%rd62, 1, 0, %p9;
	add.s64 	%rd63, %rd61, %rd62;
	ld.u8 	%rs11, [%rd109+3];
	and.b16  	%rs12, %rs11, 192;
	setp.ne.s16 	%p10, %rs12, 128;
	selp.u64 	%rd64, 1, 0, %p10;
	add.s64 	%rd113, %rd63, %rd64;
	add.s64 	%rd109, %rd109, 4;
	add.s64 	%rd106, %rd106, -4;
	setp.ne.s64 	%p11, %rd106, 0;
	@%p11 bra 	$L__BB16_4;

$L__BB16_5:
	setp.eq.s64 	%p12, %rd112, 0;
	@%p12 bra 	$L__BB16_7;

$L__BB16_6:
	.pragma "nounroll";
	ld.u8 	%rs13, [%rd109];
	and.b16  	%rs14, %rs13, 192;
	setp.ne.s16 	%p13, %rs14, 128;
	selp.u64 	%rd65, 1, 0, %p13;
	add.s64 	%rd113, %rd113, %rd65;
	add.s64 	%rd109, %rd109, 1;
	add.s64 	%rd112, %rd112, -1;
	setp.ne.s64 	%p14, %rd112, 0;
	@%p14 bra 	$L__BB16_6;

$L__BB16_7:
	cvt.u32.u64 	%r74, %rd113;

$L__BB16_8:
	setp.eq.s32 	%p15, %r31, 0;
	mov.u32 	%r34, 0;
	mov.u32 	%r82, %r34;
	@%p15 bra 	$L__BB16_43;

	cvt.u64.u32 	%rd66, %r31;
	and.b64  	%rd21, %rd66, 3;
	setp.eq.s64 	%p17, %rd1, 0;
	mov.u16 	%rs45, 0;
	cvt.s64.s32 	%rd86, %r31;
	add.s64 	%rd87, %rd86, -1;
	setp.lt.u64 	%p34, %rd87, 3;
	mov.u64 	%rd85, 0;
	setp.eq.s64 	%p25, %rd21, 0;
	mov.u16 	%rs50, %rs51;
	mov.u32 	%r75, %r34;
	mov.u32 	%r76, %r34;

$L__BB16_10:
	setp.ne.s32 	%p16, %r74, -1;
	or.pred  	%p18, %p16, %p17;
	selp.b32 	%r77, %r74, 0, %p16;
	@%p18 bra 	$L__BB16_19;

	mov.u64 	%rd120, 0;
	mov.u64 	%rd118, %rd1;
	@%p34 bra 	$L__BB16_14;

	and.b64  	%rd73, %rd86, 3;
	sub.s64 	%rd114, %rd73, %rd86;
	mov.u64 	%rd118, %rd1;

$L__BB16_13:
	ld.u8 	%rs15, [%rd118];
	and.b16  	%rs16, %rs15, 192;
	setp.ne.s16 	%p20, %rs16, 128;
	selp.u64 	%rd74, 1, 0, %p20;
	add.s64 	%rd75, %rd120, %rd74;
	ld.u8 	%rs17, [%rd118+1];
	and.b16  	%rs18, %rs17, 192;
	setp.ne.s16 	%p21, %rs18, 128;
	selp.u64 	%rd76, 1, 0, %p21;
	add.s64 	%rd77, %rd75, %rd76;
	ld.u8 	%rs19, [%rd118+2];
	and.b16  	%rs20, %rs19, 192;
	setp.ne.s16 	%p22, %rs20, 128;
	selp.u64 	%rd78, 1, 0, %p22;
	add.s64 	%rd79, %rd77, %rd78;
	ld.u8 	%rs21, [%rd118+3];
	and.b16  	%rs22, %rs21, 192;
	setp.ne.s16 	%p23, %rs22, 128;
	selp.u64 	%rd80, 1, 0, %p23;
	add.s64 	%rd120, %rd79, %rd80;
	add.s64 	%rd118, %rd118, 4;
	add.s64 	%rd114, %rd114, 4;
	setp.ne.s64 	%p24, %rd114, 0;
	@%p24 bra 	$L__BB16_13;

$L__BB16_14:
	@%p25 bra 	$L__BB16_18;

	setp.eq.s64 	%p26, %rd21, 1;
	ld.u8 	%rs23, [%rd118];
	and.b16  	%rs24, %rs23, 192;
	setp.ne.s16 	%p27, %rs24, 128;
	selp.u64 	%rd81, 1, 0, %p27;
	add.s64 	%rd120, %rd120, %rd81;
	@%p26 bra 	$L__BB16_18;

	setp.eq.s64 	%p28, %rd21, 2;
	ld.u8 	%rs25, [%rd118+1];
	and.b16  	%rs26, %rs25, 192;
	setp.ne.s16 	%p29, %rs26, 128;
	selp.u64 	%rd82, 1, 0, %p29;
	add.s64 	%rd120, %rd120, %rd82;
	@%p28 bra 	$L__BB16_18;

	ld.u8 	%rs27, [%rd118+2];
	and.b16  	%rs28, %rs27, 192;
	setp.ne.s16 	%p30, %rs28, 128;
	selp.u64 	%rd83, 1, 0, %p30;
	add.s64 	%rd120, %rd120, %rd83;

$L__BB16_18:
	cvt.u32.u64 	%r77, %rd120;

$L__BB16_19:
	setp.ne.s32 	%p31, %r77, -1;
	or.pred  	%p33, %p31, %p17;
	selp.b32 	%r74, %r77, 0, %p31;
	@%p33 bra 	$L__BB16_28;

	mov.u64 	%rd125, %rd1;
	mov.u64 	%rd127, %rd85;
	@%p34 bra 	$L__BB16_23;

	and.b64  	%rd90, %rd86, 3;
	sub.s64 	%rd123, %rd86, %rd90;
	mov.u64 	%rd125, %rd1;
	mov.u64 	%rd127, %rd85;

$L__BB16_22:
	ld.u8 	%rs29, [%rd125];
	and.b16  	%rs30, %rs29, 192;
	setp.ne.s16 	%p35, %rs30, 128;
	selp.u64 	%rd91, 1, 0, %p35;
	add.s64 	%rd92, %rd127, %rd91;
	ld.u8 	%rs31, [%rd125+1];
	and.b16  	%rs32, %rs31, 192;
	setp.ne.s16 	%p36, %rs32, 128;
	selp.u64 	%rd93, 1, 0, %p36;
	add.s64 	%rd94, %rd92, %rd93;
	ld.u8 	%rs33, [%rd125+2];
	and.b16  	%rs34, %rs33, 192;
	setp.ne.s16 	%p37, %rs34, 128;
	selp.u64 	%rd95, 1, 0, %p37;
	add.s64 	%rd96, %rd94, %rd95;
	ld.u8 	%rs35, [%rd125+3];
	and.b16  	%rs36, %rs35, 192;
	setp.ne.s16 	%p38, %rs36, 128;
	selp.u64 	%rd97, 1, 0, %p38;
	add.s64 	%rd127, %rd96, %rd97;
	add.s64 	%rd125, %rd125, 4;
	add.s64 	%rd123, %rd123, -4;
	setp.ne.s64 	%p39, %rd123, 0;
	@%p39 bra 	$L__BB16_22;

$L__BB16_23:
	@%p25 bra 	$L__BB16_27;

	setp.eq.s64 	%p41, %rd21, 1;
	ld.u8 	%rs37, [%rd125];
	and.b16  	%rs38, %rs37, 192;
	setp.ne.s16 	%p42, %rs38, 128;
	selp.u64 	%rd98, 1, 0, %p42;
	add.s64 	%rd127, %rd127, %rd98;
	@%p41 bra 	$L__BB16_27;

	setp.eq.s64 	%p43, %rd21, 2;
	ld.u8 	%rs39, [%rd125+1];
	and.b16  	%rs40, %rs39, 192;
	setp.ne.s16 	%p44, %rs40, 128;
	selp.u64 	%rd99, 1, 0, %p44;
	add.s64 	%rd127, %rd127, %rd99;
	@%p43 bra 	$L__BB16_27;

	ld.u8 	%rs41, [%rd125+2];
	and.b16  	%rs42, %rs41, 192;
	setp.ne.s16 	%p45, %rs42, 128;
	selp.u64 	%rd100, 1, 0, %p45;
	add.s64 	%rd127, %rd127, %rd100;

$L__BB16_27:
	cvt.u32.u64 	%r74, %rd127;

$L__BB16_28:
	setp.eq.s32 	%p46, %r75, %r77;
	mov.u16 	%rs51, %rs50;
	@%p46 bra 	$L__BB16_43;

	cvt.s64.s32 	%rd101, %r76;
	add.s64 	%rd50, %rd1, %rd101;
	ld.u8 	%rs43, [%rd50];
	setp.gt.u16 	%p47, %rs43, 239;
	selp.b32 	%r38, 2, 1, %p47;
	setp.gt.u16 	%p48, %rs43, 223;
	selp.u32 	%r39, 1, 0, %p48;
	add.s32 	%r40, %r38, %r39;
	setp.gt.u16 	%p49, %rs43, 191;
	selp.u32 	%r41, 1, 0, %p49;
	add.s32 	%r42, %r40, %r41;
	and.b16  	%rs44, %rs43, 192;
	setp.eq.s16 	%p50, %rs44, 128;
	selp.b32 	%r43, -1, 0, %p50;
	add.s32 	%r15, %r42, %r43;
	cvt.u32.u16 	%r79, %rs43;
	setp.lt.s32 	%p51, %r15, 2;
	@%p51 bra 	$L__BB16_33;

	ld.u8 	%r44, [%rd50+1];
	prmt.b32 	%r79, %r79, %r44, 8452;
	setp.eq.s32 	%p52, %r15, 2;
	@%p52 bra 	$L__BB16_33;

	ld.u8 	%r45, [%rd50+2];
	prmt.b32 	%r79, %r79, %r45, 8452;
	setp.lt.s32 	%p53, %r15, 4;
	@%p53 bra 	$L__BB16_33;

	ld.u8 	%r46, [%rd50+3];
	prmt.b32 	%r79, %r79, %r46, 8452;

$L__BB16_33:
	setp.lt.u32 	%p54, %r79, 128;
	mov.u32 	%r80, %r79;
	@%p54 bra 	$L__BB16_40;

	setp.lt.u32 	%p55, %r79, 57344;
	@%p55 bra 	$L__BB16_39;
	bra.uni 	$L__BB16_35;

$L__BB16_39:
	and.b32  	%r63, %r79, 7936;
	shr.u32 	%r64, %r63, 2;
	and.b32  	%r65, %r79, 63;
	or.b32  	%r80, %r64, %r65;
	bra.uni 	$L__BB16_40;

$L__BB16_35:
	setp.lt.u32 	%p56, %r79, 15728640;
	@%p56 bra 	$L__BB16_38;
	bra.uni 	$L__BB16_36;

$L__BB16_38:
	and.b32  	%r57, %r79, 983040;
	shr.u32 	%r58, %r57, 4;
	and.b32  	%r59, %r79, 16128;
	shr.u32 	%r60, %r59, 2;
	and.b32  	%r61, %r79, 63;
	or.b32  	%r62, %r60, %r61;
	or.b32  	%r80, %r62, %r58;
	bra.uni 	$L__BB16_40;

$L__BB16_36:
	setp.gt.u32 	%p57, %r79, -134217728;
	mov.u32 	%r80, 0;
	@%p57 bra 	$L__BB16_40;

	and.b32  	%r48, %r79, 50331648;
	shr.u32 	%r49, %r48, 6;
	and.b32  	%r50, %r79, 4128768;
	shr.u32 	%r51, %r50, 4;
	and.b32  	%r52, %r79, 16128;
	shr.u32 	%r53, %r52, 2;
	and.b32  	%r54, %r79, 63;
	or.b32  	%r55, %r51, %r54;
	or.b32  	%r56, %r55, %r49;
	or.b32  	%r80, %r56, %r53;

$L__BB16_40:
	setp.gt.u32 	%p58, %r80, 65535;
	mov.u32 	%r81, 0;
	@%p58 bra 	$L__BB16_42;

	cvt.u64.u32 	%rd102, %r80;
	add.s64 	%rd103, %rd52, %rd102;
	ld.u8 	%r81, [%rd103];

$L__BB16_42:
	and.b32  	%r67, %r81, 127;
	setp.ne.s32 	%p59, %r67, 0;
	setp.eq.s32 	%p60, %r81, 0;
	or.pred  	%p61, %p60, %p59;
	bfe.u32 	%r68, %r81, 1, 1;
	cvt.u16.u32 	%rs46, %r68;
	selp.b16 	%rs50, %rs46, %rs50, %p61;
	and.b16  	%rs47, %rs50, 255;
	selp.u32 	%r69, 1, 0, %p61;
	add.s32 	%r82, %r82, %r69;
	add.s32 	%r70, %r15, %r76;
	setp.lt.s32 	%p62, %r76, %r31;
	selp.b32 	%r76, %r70, %r76, %p62;
	add.s32 	%r75, %r75, 1;
	setp.ne.s16 	%p63, %rs47, 0;
	mov.u16 	%rs51, %rs45;
	@%p63 bra 	$L__BB16_10;

$L__BB16_43:
	setp.ne.s32 	%p64, %r82, 0;
	and.b16  	%rs48, %rs51, 255;
	setp.ne.s16 	%p65, %rs48, 0;
	and.pred  	%p66, %p65, %p64;
	selp.u16 	%rs49, 1, 0, %p66;
	st.u8 	[%rd51], %rs49;
	st.param.b32 	[func_retval0+0], %r34;
	ret;

}
	// .globl	pyisdigit
.visible .func  (.param .b32 func_retval0) pyisdigit(
	.param .b64 pyisdigit_param_0,
	.param .b64 pyisdigit_param_1,
	.param .b64 pyisdigit_param_2
)
{
	.reg .pred 	%p<67>;
	.reg .b16 	%rs<52>;
	.reg .b32 	%r<83>;
	.reg .b64 	%rd<128>;


	ld.param.u64 	%rd51, [pyisdigit_param_0];
	ld.param.u64 	%rd53, [pyisdigit_param_1];
	ld.param.u64 	%rd52, [pyisdigit_param_2];
	ld.u64 	%rd1, [%rd53];
	ld.v2.u32 	{%r31, %r74}, [%rd53+8];
	setp.ne.s32 	%p1, %r31, 0;
	selp.u16 	%rs51, 1, 0, %p1;
	setp.ne.s32 	%p2, %r74, -1;
	@%p2 bra 	$L__BB17_8;

	setp.eq.s64 	%p3, %rd1, 0;
	setp.eq.s32 	%p4, %r31, 0;
	mov.u32 	%r74, 0;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB17_8;

	cvt.s64.s32 	%rd2, %r31;
	add.s64 	%rd56, %rd2, -1;
	and.b64  	%rd112, %rd2, 3;
	setp.lt.u64 	%p6, %rd56, 3;
	mov.u64 	%rd113, 0;
	mov.u64 	%rd109, %rd1;
	@%p6 bra 	$L__BB17_5;

	sub.s64 	%rd106, %rd2, %rd112;
	mov.u64 	%rd109, %rd1;

$L__BB17_4:
	ld.u8 	%rs5, [%rd109];
	and.b16  	%rs6, %rs5, 192;
	setp.ne.s16 	%p7, %rs6, 128;
	selp.u64 	%rd58, 1, 0, %p7;
	add.s64 	%rd59, %rd113, %rd58;
	ld.u8 	%rs7, [%rd109+1];
	and.b16  	%rs8, %rs7, 192;
	setp.ne.s16 	%p8, %rs8, 128;
	selp.u64 	%rd60, 1, 0, %p8;
	add.s64 	%rd61, %rd59, %rd60;
	ld.u8 	%rs9, [%rd109+2];
	and.b16  	%rs10, %rs9, 192;
	setp.ne.s16 	%p9, %rs10, 128;
	selp.u64 	%rd62, 1, 0, %p9;
	add.s64 	%rd63, %rd61, %rd62;
	ld.u8 	%rs11, [%rd109+3];
	and.b16  	%rs12, %rs11, 192;
	setp.ne.s16 	%p10, %rs12, 128;
	selp.u64 	%rd64, 1, 0, %p10;
	add.s64 	%rd113, %rd63, %rd64;
	add.s64 	%rd109, %rd109, 4;
	add.s64 	%rd106, %rd106, -4;
	setp.ne.s64 	%p11, %rd106, 0;
	@%p11 bra 	$L__BB17_4;

$L__BB17_5:
	setp.eq.s64 	%p12, %rd112, 0;
	@%p12 bra 	$L__BB17_7;

$L__BB17_6:
	.pragma "nounroll";
	ld.u8 	%rs13, [%rd109];
	and.b16  	%rs14, %rs13, 192;
	setp.ne.s16 	%p13, %rs14, 128;
	selp.u64 	%rd65, 1, 0, %p13;
	add.s64 	%rd113, %rd113, %rd65;
	add.s64 	%rd109, %rd109, 1;
	add.s64 	%rd112, %rd112, -1;
	setp.ne.s64 	%p14, %rd112, 0;
	@%p14 bra 	$L__BB17_6;

$L__BB17_7:
	cvt.u32.u64 	%r74, %rd113;

$L__BB17_8:
	setp.eq.s32 	%p15, %r31, 0;
	mov.u32 	%r34, 0;
	mov.u32 	%r82, %r34;
	@%p15 bra 	$L__BB17_43;

	cvt.u64.u32 	%rd66, %r31;
	and.b64  	%rd21, %rd66, 3;
	setp.eq.s64 	%p17, %rd1, 0;
	mov.u16 	%rs45, 0;
	cvt.s64.s32 	%rd86, %r31;
	add.s64 	%rd87, %rd86, -1;
	setp.lt.u64 	%p34, %rd87, 3;
	mov.u64 	%rd85, 0;
	setp.eq.s64 	%p25, %rd21, 0;
	mov.u16 	%rs50, %rs51;
	mov.u32 	%r75, %r34;
	mov.u32 	%r76, %r34;

$L__BB17_10:
	setp.ne.s32 	%p16, %r74, -1;
	or.pred  	%p18, %p16, %p17;
	selp.b32 	%r77, %r74, 0, %p16;
	@%p18 bra 	$L__BB17_19;

	mov.u64 	%rd120, 0;
	mov.u64 	%rd118, %rd1;
	@%p34 bra 	$L__BB17_14;

	and.b64  	%rd73, %rd86, 3;
	sub.s64 	%rd114, %rd73, %rd86;
	mov.u64 	%rd118, %rd1;

$L__BB17_13:
	ld.u8 	%rs15, [%rd118];
	and.b16  	%rs16, %rs15, 192;
	setp.ne.s16 	%p20, %rs16, 128;
	selp.u64 	%rd74, 1, 0, %p20;
	add.s64 	%rd75, %rd120, %rd74;
	ld.u8 	%rs17, [%rd118+1];
	and.b16  	%rs18, %rs17, 192;
	setp.ne.s16 	%p21, %rs18, 128;
	selp.u64 	%rd76, 1, 0, %p21;
	add.s64 	%rd77, %rd75, %rd76;
	ld.u8 	%rs19, [%rd118+2];
	and.b16  	%rs20, %rs19, 192;
	setp.ne.s16 	%p22, %rs20, 128;
	selp.u64 	%rd78, 1, 0, %p22;
	add.s64 	%rd79, %rd77, %rd78;
	ld.u8 	%rs21, [%rd118+3];
	and.b16  	%rs22, %rs21, 192;
	setp.ne.s16 	%p23, %rs22, 128;
	selp.u64 	%rd80, 1, 0, %p23;
	add.s64 	%rd120, %rd79, %rd80;
	add.s64 	%rd118, %rd118, 4;
	add.s64 	%rd114, %rd114, 4;
	setp.ne.s64 	%p24, %rd114, 0;
	@%p24 bra 	$L__BB17_13;

$L__BB17_14:
	@%p25 bra 	$L__BB17_18;

	setp.eq.s64 	%p26, %rd21, 1;
	ld.u8 	%rs23, [%rd118];
	and.b16  	%rs24, %rs23, 192;
	setp.ne.s16 	%p27, %rs24, 128;
	selp.u64 	%rd81, 1, 0, %p27;
	add.s64 	%rd120, %rd120, %rd81;
	@%p26 bra 	$L__BB17_18;

	setp.eq.s64 	%p28, %rd21, 2;
	ld.u8 	%rs25, [%rd118+1];
	and.b16  	%rs26, %rs25, 192;
	setp.ne.s16 	%p29, %rs26, 128;
	selp.u64 	%rd82, 1, 0, %p29;
	add.s64 	%rd120, %rd120, %rd82;
	@%p28 bra 	$L__BB17_18;

	ld.u8 	%rs27, [%rd118+2];
	and.b16  	%rs28, %rs27, 192;
	setp.ne.s16 	%p30, %rs28, 128;
	selp.u64 	%rd83, 1, 0, %p30;
	add.s64 	%rd120, %rd120, %rd83;

$L__BB17_18:
	cvt.u32.u64 	%r77, %rd120;

$L__BB17_19:
	setp.ne.s32 	%p31, %r77, -1;
	or.pred  	%p33, %p31, %p17;
	selp.b32 	%r74, %r77, 0, %p31;
	@%p33 bra 	$L__BB17_28;

	mov.u64 	%rd125, %rd1;
	mov.u64 	%rd127, %rd85;
	@%p34 bra 	$L__BB17_23;

	and.b64  	%rd90, %rd86, 3;
	sub.s64 	%rd123, %rd86, %rd90;
	mov.u64 	%rd125, %rd1;
	mov.u64 	%rd127, %rd85;

$L__BB17_22:
	ld.u8 	%rs29, [%rd125];
	and.b16  	%rs30, %rs29, 192;
	setp.ne.s16 	%p35, %rs30, 128;
	selp.u64 	%rd91, 1, 0, %p35;
	add.s64 	%rd92, %rd127, %rd91;
	ld.u8 	%rs31, [%rd125+1];
	and.b16  	%rs32, %rs31, 192;
	setp.ne.s16 	%p36, %rs32, 128;
	selp.u64 	%rd93, 1, 0, %p36;
	add.s64 	%rd94, %rd92, %rd93;
	ld.u8 	%rs33, [%rd125+2];
	and.b16  	%rs34, %rs33, 192;
	setp.ne.s16 	%p37, %rs34, 128;
	selp.u64 	%rd95, 1, 0, %p37;
	add.s64 	%rd96, %rd94, %rd95;
	ld.u8 	%rs35, [%rd125+3];
	and.b16  	%rs36, %rs35, 192;
	setp.ne.s16 	%p38, %rs36, 128;
	selp.u64 	%rd97, 1, 0, %p38;
	add.s64 	%rd127, %rd96, %rd97;
	add.s64 	%rd125, %rd125, 4;
	add.s64 	%rd123, %rd123, -4;
	setp.ne.s64 	%p39, %rd123, 0;
	@%p39 bra 	$L__BB17_22;

$L__BB17_23:
	@%p25 bra 	$L__BB17_27;

	setp.eq.s64 	%p41, %rd21, 1;
	ld.u8 	%rs37, [%rd125];
	and.b16  	%rs38, %rs37, 192;
	setp.ne.s16 	%p42, %rs38, 128;
	selp.u64 	%rd98, 1, 0, %p42;
	add.s64 	%rd127, %rd127, %rd98;
	@%p41 bra 	$L__BB17_27;

	setp.eq.s64 	%p43, %rd21, 2;
	ld.u8 	%rs39, [%rd125+1];
	and.b16  	%rs40, %rs39, 192;
	setp.ne.s16 	%p44, %rs40, 128;
	selp.u64 	%rd99, 1, 0, %p44;
	add.s64 	%rd127, %rd127, %rd99;
	@%p43 bra 	$L__BB17_27;

	ld.u8 	%rs41, [%rd125+2];
	and.b16  	%rs42, %rs41, 192;
	setp.ne.s16 	%p45, %rs42, 128;
	selp.u64 	%rd100, 1, 0, %p45;
	add.s64 	%rd127, %rd127, %rd100;

$L__BB17_27:
	cvt.u32.u64 	%r74, %rd127;

$L__BB17_28:
	setp.eq.s32 	%p46, %r75, %r77;
	mov.u16 	%rs51, %rs50;
	@%p46 bra 	$L__BB17_43;

	cvt.s64.s32 	%rd101, %r76;
	add.s64 	%rd50, %rd1, %rd101;
	ld.u8 	%rs43, [%rd50];
	setp.gt.u16 	%p47, %rs43, 239;
	selp.b32 	%r38, 2, 1, %p47;
	setp.gt.u16 	%p48, %rs43, 223;
	selp.u32 	%r39, 1, 0, %p48;
	add.s32 	%r40, %r38, %r39;
	setp.gt.u16 	%p49, %rs43, 191;
	selp.u32 	%r41, 1, 0, %p49;
	add.s32 	%r42, %r40, %r41;
	and.b16  	%rs44, %rs43, 192;
	setp.eq.s16 	%p50, %rs44, 128;
	selp.b32 	%r43, -1, 0, %p50;
	add.s32 	%r15, %r42, %r43;
	cvt.u32.u16 	%r79, %rs43;
	setp.lt.s32 	%p51, %r15, 2;
	@%p51 bra 	$L__BB17_33;

	ld.u8 	%r44, [%rd50+1];
	prmt.b32 	%r79, %r79, %r44, 8452;
	setp.eq.s32 	%p52, %r15, 2;
	@%p52 bra 	$L__BB17_33;

	ld.u8 	%r45, [%rd50+2];
	prmt.b32 	%r79, %r79, %r45, 8452;
	setp.lt.s32 	%p53, %r15, 4;
	@%p53 bra 	$L__BB17_33;

	ld.u8 	%r46, [%rd50+3];
	prmt.b32 	%r79, %r79, %r46, 8452;

$L__BB17_33:
	setp.lt.u32 	%p54, %r79, 128;
	mov.u32 	%r80, %r79;
	@%p54 bra 	$L__BB17_40;

	setp.lt.u32 	%p55, %r79, 57344;
	@%p55 bra 	$L__BB17_39;
	bra.uni 	$L__BB17_35;

$L__BB17_39:
	and.b32  	%r63, %r79, 7936;
	shr.u32 	%r64, %r63, 2;
	and.b32  	%r65, %r79, 63;
	or.b32  	%r80, %r64, %r65;
	bra.uni 	$L__BB17_40;

$L__BB17_35:
	setp.lt.u32 	%p56, %r79, 15728640;
	@%p56 bra 	$L__BB17_38;
	bra.uni 	$L__BB17_36;

$L__BB17_38:
	and.b32  	%r57, %r79, 983040;
	shr.u32 	%r58, %r57, 4;
	and.b32  	%r59, %r79, 16128;
	shr.u32 	%r60, %r59, 2;
	and.b32  	%r61, %r79, 63;
	or.b32  	%r62, %r60, %r61;
	or.b32  	%r80, %r62, %r58;
	bra.uni 	$L__BB17_40;

$L__BB17_36:
	setp.gt.u32 	%p57, %r79, -134217728;
	mov.u32 	%r80, 0;
	@%p57 bra 	$L__BB17_40;

	and.b32  	%r48, %r79, 50331648;
	shr.u32 	%r49, %r48, 6;
	and.b32  	%r50, %r79, 4128768;
	shr.u32 	%r51, %r50, 4;
	and.b32  	%r52, %r79, 16128;
	shr.u32 	%r53, %r52, 2;
	and.b32  	%r54, %r79, 63;
	or.b32  	%r55, %r51, %r54;
	or.b32  	%r56, %r55, %r49;
	or.b32  	%r80, %r56, %r53;

$L__BB17_40:
	setp.gt.u32 	%p58, %r80, 65535;
	mov.u32 	%r81, 0;
	@%p58 bra 	$L__BB17_42;

	cvt.u64.u32 	%rd102, %r80;
	add.s64 	%rd103, %rd52, %rd102;
	ld.u8 	%r81, [%rd103];

$L__BB17_42:
	and.b32  	%r67, %r81, 127;
	setp.ne.s32 	%p59, %r67, 0;
	setp.eq.s32 	%p60, %r81, 0;
	or.pred  	%p61, %p60, %p59;
	bfe.u32 	%r68, %r81, 2, 1;
	cvt.u16.u32 	%rs46, %r68;
	selp.b16 	%rs50, %rs46, %rs50, %p61;
	and.b16  	%rs47, %rs50, 255;
	selp.u32 	%r69, 1, 0, %p61;
	add.s32 	%r82, %r82, %r69;
	add.s32 	%r70, %r15, %r76;
	setp.lt.s32 	%p62, %r76, %r31;
	selp.b32 	%r76, %r70, %r76, %p62;
	add.s32 	%r75, %r75, 1;
	setp.ne.s16 	%p63, %rs47, 0;
	mov.u16 	%rs51, %rs45;
	@%p63 bra 	$L__BB17_10;

$L__BB17_43:
	setp.ne.s32 	%p64, %r82, 0;
	and.b16  	%rs48, %rs51, 255;
	setp.ne.s16 	%p65, %rs48, 0;
	and.pred  	%p66, %p65, %p64;
	selp.u16 	%rs49, 1, 0, %p66;
	st.u8 	[%rd51], %rs49;
	st.param.b32 	[func_retval0+0], %r34;
	ret;

}
	// .globl	pyisalnum
.visible .func  (.param .b32 func_retval0) pyisalnum(
	.param .b64 pyisalnum_param_0,
	.param .b64 pyisalnum_param_1,
	.param .b64 pyisalnum_param_2
)
{
	.reg .pred 	%p<68>;
	.reg .b16 	%rs<52>;
	.reg .b32 	%r<83>;
	.reg .b64 	%rd<128>;


	ld.param.u64 	%rd51, [pyisalnum_param_0];
	ld.param.u64 	%rd53, [pyisalnum_param_1];
	ld.param.u64 	%rd52, [pyisalnum_param_2];
	ld.u64 	%rd1, [%rd53];
	ld.v2.u32 	{%r31, %r74}, [%rd53+8];
	setp.ne.s32 	%p1, %r31, 0;
	selp.u16 	%rs51, 1, 0, %p1;
	setp.ne.s32 	%p2, %r74, -1;
	@%p2 bra 	$L__BB18_8;

	setp.eq.s64 	%p3, %rd1, 0;
	setp.eq.s32 	%p4, %r31, 0;
	mov.u32 	%r74, 0;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB18_8;

	cvt.s64.s32 	%rd2, %r31;
	add.s64 	%rd56, %rd2, -1;
	and.b64  	%rd112, %rd2, 3;
	setp.lt.u64 	%p6, %rd56, 3;
	mov.u64 	%rd113, 0;
	mov.u64 	%rd109, %rd1;
	@%p6 bra 	$L__BB18_5;

	sub.s64 	%rd106, %rd2, %rd112;
	mov.u64 	%rd109, %rd1;

$L__BB18_4:
	ld.u8 	%rs5, [%rd109];
	and.b16  	%rs6, %rs5, 192;
	setp.ne.s16 	%p7, %rs6, 128;
	selp.u64 	%rd58, 1, 0, %p7;
	add.s64 	%rd59, %rd113, %rd58;
	ld.u8 	%rs7, [%rd109+1];
	and.b16  	%rs8, %rs7, 192;
	setp.ne.s16 	%p8, %rs8, 128;
	selp.u64 	%rd60, 1, 0, %p8;
	add.s64 	%rd61, %rd59, %rd60;
	ld.u8 	%rs9, [%rd109+2];
	and.b16  	%rs10, %rs9, 192;
	setp.ne.s16 	%p9, %rs10, 128;
	selp.u64 	%rd62, 1, 0, %p9;
	add.s64 	%rd63, %rd61, %rd62;
	ld.u8 	%rs11, [%rd109+3];
	and.b16  	%rs12, %rs11, 192;
	setp.ne.s16 	%p10, %rs12, 128;
	selp.u64 	%rd64, 1, 0, %p10;
	add.s64 	%rd113, %rd63, %rd64;
	add.s64 	%rd109, %rd109, 4;
	add.s64 	%rd106, %rd106, -4;
	setp.ne.s64 	%p11, %rd106, 0;
	@%p11 bra 	$L__BB18_4;

$L__BB18_5:
	setp.eq.s64 	%p12, %rd112, 0;
	@%p12 bra 	$L__BB18_7;

$L__BB18_6:
	.pragma "nounroll";
	ld.u8 	%rs13, [%rd109];
	and.b16  	%rs14, %rs13, 192;
	setp.ne.s16 	%p13, %rs14, 128;
	selp.u64 	%rd65, 1, 0, %p13;
	add.s64 	%rd113, %rd113, %rd65;
	add.s64 	%rd109, %rd109, 1;
	add.s64 	%rd112, %rd112, -1;
	setp.ne.s64 	%p14, %rd112, 0;
	@%p14 bra 	$L__BB18_6;

$L__BB18_7:
	cvt.u32.u64 	%r74, %rd113;

$L__BB18_8:
	setp.eq.s32 	%p15, %r31, 0;
	mov.u32 	%r34, 0;
	mov.u32 	%r82, %r34;
	@%p15 bra 	$L__BB18_43;

	cvt.u64.u32 	%rd66, %r31;
	and.b64  	%rd21, %rd66, 3;
	setp.eq.s64 	%p17, %rd1, 0;
	mov.u16 	%rs45, 0;
	cvt.s64.s32 	%rd86, %r31;
	add.s64 	%rd87, %rd86, -1;
	setp.lt.u64 	%p34, %rd87, 3;
	mov.u64 	%rd85, 0;
	setp.eq.s64 	%p25, %rd21, 0;
	mov.u16 	%rs50, %rs51;
	mov.u32 	%r75, %r34;
	mov.u32 	%r76, %r34;

$L__BB18_10:
	setp.ne.s32 	%p16, %r74, -1;
	or.pred  	%p18, %p16, %p17;
	selp.b32 	%r77, %r74, 0, %p16;
	@%p18 bra 	$L__BB18_19;

	mov.u64 	%rd120, 0;
	mov.u64 	%rd118, %rd1;
	@%p34 bra 	$L__BB18_14;

	and.b64  	%rd73, %rd86, 3;
	sub.s64 	%rd114, %rd73, %rd86;
	mov.u64 	%rd118, %rd1;

$L__BB18_13:
	ld.u8 	%rs15, [%rd118];
	and.b16  	%rs16, %rs15, 192;
	setp.ne.s16 	%p20, %rs16, 128;
	selp.u64 	%rd74, 1, 0, %p20;
	add.s64 	%rd75, %rd120, %rd74;
	ld.u8 	%rs17, [%rd118+1];
	and.b16  	%rs18, %rs17, 192;
	setp.ne.s16 	%p21, %rs18, 128;
	selp.u64 	%rd76, 1, 0, %p21;
	add.s64 	%rd77, %rd75, %rd76;
	ld.u8 	%rs19, [%rd118+2];
	and.b16  	%rs20, %rs19, 192;
	setp.ne.s16 	%p22, %rs20, 128;
	selp.u64 	%rd78, 1, 0, %p22;
	add.s64 	%rd79, %rd77, %rd78;
	ld.u8 	%rs21, [%rd118+3];
	and.b16  	%rs22, %rs21, 192;
	setp.ne.s16 	%p23, %rs22, 128;
	selp.u64 	%rd80, 1, 0, %p23;
	add.s64 	%rd120, %rd79, %rd80;
	add.s64 	%rd118, %rd118, 4;
	add.s64 	%rd114, %rd114, 4;
	setp.ne.s64 	%p24, %rd114, 0;
	@%p24 bra 	$L__BB18_13;

$L__BB18_14:
	@%p25 bra 	$L__BB18_18;

	setp.eq.s64 	%p26, %rd21, 1;
	ld.u8 	%rs23, [%rd118];
	and.b16  	%rs24, %rs23, 192;
	setp.ne.s16 	%p27, %rs24, 128;
	selp.u64 	%rd81, 1, 0, %p27;
	add.s64 	%rd120, %rd120, %rd81;
	@%p26 bra 	$L__BB18_18;

	setp.eq.s64 	%p28, %rd21, 2;
	ld.u8 	%rs25, [%rd118+1];
	and.b16  	%rs26, %rs25, 192;
	setp.ne.s16 	%p29, %rs26, 128;
	selp.u64 	%rd82, 1, 0, %p29;
	add.s64 	%rd120, %rd120, %rd82;
	@%p28 bra 	$L__BB18_18;

	ld.u8 	%rs27, [%rd118+2];
	and.b16  	%rs28, %rs27, 192;
	setp.ne.s16 	%p30, %rs28, 128;
	selp.u64 	%rd83, 1, 0, %p30;
	add.s64 	%rd120, %rd120, %rd83;

$L__BB18_18:
	cvt.u32.u64 	%r77, %rd120;

$L__BB18_19:
	setp.ne.s32 	%p31, %r77, -1;
	or.pred  	%p33, %p31, %p17;
	selp.b32 	%r74, %r77, 0, %p31;
	@%p33 bra 	$L__BB18_28;

	mov.u64 	%rd125, %rd1;
	mov.u64 	%rd127, %rd85;
	@%p34 bra 	$L__BB18_23;

	and.b64  	%rd90, %rd86, 3;
	sub.s64 	%rd123, %rd86, %rd90;
	mov.u64 	%rd125, %rd1;
	mov.u64 	%rd127, %rd85;

$L__BB18_22:
	ld.u8 	%rs29, [%rd125];
	and.b16  	%rs30, %rs29, 192;
	setp.ne.s16 	%p35, %rs30, 128;
	selp.u64 	%rd91, 1, 0, %p35;
	add.s64 	%rd92, %rd127, %rd91;
	ld.u8 	%rs31, [%rd125+1];
	and.b16  	%rs32, %rs31, 192;
	setp.ne.s16 	%p36, %rs32, 128;
	selp.u64 	%rd93, 1, 0, %p36;
	add.s64 	%rd94, %rd92, %rd93;
	ld.u8 	%rs33, [%rd125+2];
	and.b16  	%rs34, %rs33, 192;
	setp.ne.s16 	%p37, %rs34, 128;
	selp.u64 	%rd95, 1, 0, %p37;
	add.s64 	%rd96, %rd94, %rd95;
	ld.u8 	%rs35, [%rd125+3];
	and.b16  	%rs36, %rs35, 192;
	setp.ne.s16 	%p38, %rs36, 128;
	selp.u64 	%rd97, 1, 0, %p38;
	add.s64 	%rd127, %rd96, %rd97;
	add.s64 	%rd125, %rd125, 4;
	add.s64 	%rd123, %rd123, -4;
	setp.ne.s64 	%p39, %rd123, 0;
	@%p39 bra 	$L__BB18_22;

$L__BB18_23:
	@%p25 bra 	$L__BB18_27;

	setp.eq.s64 	%p41, %rd21, 1;
	ld.u8 	%rs37, [%rd125];
	and.b16  	%rs38, %rs37, 192;
	setp.ne.s16 	%p42, %rs38, 128;
	selp.u64 	%rd98, 1, 0, %p42;
	add.s64 	%rd127, %rd127, %rd98;
	@%p41 bra 	$L__BB18_27;

	setp.eq.s64 	%p43, %rd21, 2;
	ld.u8 	%rs39, [%rd125+1];
	and.b16  	%rs40, %rs39, 192;
	setp.ne.s16 	%p44, %rs40, 128;
	selp.u64 	%rd99, 1, 0, %p44;
	add.s64 	%rd127, %rd127, %rd99;
	@%p43 bra 	$L__BB18_27;

	ld.u8 	%rs41, [%rd125+2];
	and.b16  	%rs42, %rs41, 192;
	setp.ne.s16 	%p45, %rs42, 128;
	selp.u64 	%rd100, 1, 0, %p45;
	add.s64 	%rd127, %rd127, %rd100;

$L__BB18_27:
	cvt.u32.u64 	%r74, %rd127;

$L__BB18_28:
	setp.eq.s32 	%p46, %r75, %r77;
	mov.u16 	%rs51, %rs50;
	@%p46 bra 	$L__BB18_43;

	cvt.s64.s32 	%rd101, %r76;
	add.s64 	%rd50, %rd1, %rd101;
	ld.u8 	%rs43, [%rd50];
	setp.gt.u16 	%p47, %rs43, 239;
	selp.b32 	%r38, 2, 1, %p47;
	setp.gt.u16 	%p48, %rs43, 223;
	selp.u32 	%r39, 1, 0, %p48;
	add.s32 	%r40, %r38, %r39;
	setp.gt.u16 	%p49, %rs43, 191;
	selp.u32 	%r41, 1, 0, %p49;
	add.s32 	%r42, %r40, %r41;
	and.b16  	%rs44, %rs43, 192;
	setp.eq.s16 	%p50, %rs44, 128;
	selp.b32 	%r43, -1, 0, %p50;
	add.s32 	%r15, %r42, %r43;
	cvt.u32.u16 	%r79, %rs43;
	setp.lt.s32 	%p51, %r15, 2;
	@%p51 bra 	$L__BB18_33;

	ld.u8 	%r44, [%rd50+1];
	prmt.b32 	%r79, %r79, %r44, 8452;
	setp.eq.s32 	%p52, %r15, 2;
	@%p52 bra 	$L__BB18_33;

	ld.u8 	%r45, [%rd50+2];
	prmt.b32 	%r79, %r79, %r45, 8452;
	setp.lt.s32 	%p53, %r15, 4;
	@%p53 bra 	$L__BB18_33;

	ld.u8 	%r46, [%rd50+3];
	prmt.b32 	%r79, %r79, %r46, 8452;

$L__BB18_33:
	setp.lt.u32 	%p54, %r79, 128;
	mov.u32 	%r80, %r79;
	@%p54 bra 	$L__BB18_40;

	setp.lt.u32 	%p55, %r79, 57344;
	@%p55 bra 	$L__BB18_39;
	bra.uni 	$L__BB18_35;

$L__BB18_39:
	and.b32  	%r63, %r79, 7936;
	shr.u32 	%r64, %r63, 2;
	and.b32  	%r65, %r79, 63;
	or.b32  	%r80, %r64, %r65;
	bra.uni 	$L__BB18_40;

$L__BB18_35:
	setp.lt.u32 	%p56, %r79, 15728640;
	@%p56 bra 	$L__BB18_38;
	bra.uni 	$L__BB18_36;

$L__BB18_38:
	and.b32  	%r57, %r79, 983040;
	shr.u32 	%r58, %r57, 4;
	and.b32  	%r59, %r79, 16128;
	shr.u32 	%r60, %r59, 2;
	and.b32  	%r61, %r79, 63;
	or.b32  	%r62, %r60, %r61;
	or.b32  	%r80, %r62, %r58;
	bra.uni 	$L__BB18_40;

$L__BB18_36:
	setp.gt.u32 	%p57, %r79, -134217728;
	mov.u32 	%r80, 0;
	@%p57 bra 	$L__BB18_40;

	and.b32  	%r48, %r79, 50331648;
	shr.u32 	%r49, %r48, 6;
	and.b32  	%r50, %r79, 4128768;
	shr.u32 	%r51, %r50, 4;
	and.b32  	%r52, %r79, 16128;
	shr.u32 	%r53, %r52, 2;
	and.b32  	%r54, %r79, 63;
	or.b32  	%r55, %r51, %r54;
	or.b32  	%r56, %r55, %r49;
	or.b32  	%r80, %r56, %r53;

$L__BB18_40:
	setp.gt.u32 	%p58, %r80, 65535;
	mov.u32 	%r81, 0;
	@%p58 bra 	$L__BB18_42;

	cvt.u64.u32 	%rd102, %r80;
	add.s64 	%rd103, %rd52, %rd102;
	ld.u8 	%r81, [%rd103];

$L__BB18_42:
	and.b32  	%r67, %r81, 127;
	setp.ne.s32 	%p59, %r67, 0;
	setp.eq.s32 	%p60, %r81, 0;
	or.pred  	%p61, %p60, %p59;
	and.b32  	%r68, %r81, 15;
	setp.ne.s32 	%p62, %r68, 0;
	selp.u16 	%rs46, 1, 0, %p62;
	selp.b16 	%rs50, %rs46, %rs50, %p61;
	and.b16  	%rs47, %rs50, 255;
	selp.u32 	%r69, 1, 0, %p61;
	add.s32 	%r82, %r82, %r69;
	add.s32 	%r70, %r15, %r76;
	setp.lt.s32 	%p63, %r76, %r31;
	selp.b32 	%r76, %r70, %r76, %p63;
	add.s32 	%r75, %r75, 1;
	setp.ne.s16 	%p64, %rs47, 0;
	mov.u16 	%rs51, %rs45;
	@%p64 bra 	$L__BB18_10;

$L__BB18_43:
	setp.ne.s32 	%p65, %r82, 0;
	and.b16  	%rs48, %rs51, 255;
	setp.ne.s16 	%p66, %rs48, 0;
	and.pred  	%p67, %p66, %p65;
	selp.u16 	%rs49, 1, 0, %p67;
	st.u8 	[%rd51], %rs49;
	st.param.b32 	[func_retval0+0], %r34;
	ret;

}
	// .globl	pyisalpha
.visible .func  (.param .b32 func_retval0) pyisalpha(
	.param .b64 pyisalpha_param_0,
	.param .b64 pyisalpha_param_1,
	.param .b64 pyisalpha_param_2
)
{
	.reg .pred 	%p<67>;
	.reg .b16 	%rs<52>;
	.reg .b32 	%r<83>;
	.reg .b64 	%rd<128>;


	ld.param.u64 	%rd51, [pyisalpha_param_0];
	ld.param.u64 	%rd53, [pyisalpha_param_1];
	ld.param.u64 	%rd52, [pyisalpha_param_2];
	ld.u64 	%rd1, [%rd53];
	ld.v2.u32 	{%r31, %r74}, [%rd53+8];
	setp.ne.s32 	%p1, %r31, 0;
	selp.u16 	%rs51, 1, 0, %p1;
	setp.ne.s32 	%p2, %r74, -1;
	@%p2 bra 	$L__BB19_8;

	setp.eq.s64 	%p3, %rd1, 0;
	setp.eq.s32 	%p4, %r31, 0;
	mov.u32 	%r74, 0;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB19_8;

	cvt.s64.s32 	%rd2, %r31;
	add.s64 	%rd56, %rd2, -1;
	and.b64  	%rd112, %rd2, 3;
	setp.lt.u64 	%p6, %rd56, 3;
	mov.u64 	%rd113, 0;
	mov.u64 	%rd109, %rd1;
	@%p6 bra 	$L__BB19_5;

	sub.s64 	%rd106, %rd2, %rd112;
	mov.u64 	%rd109, %rd1;

$L__BB19_4:
	ld.u8 	%rs5, [%rd109];
	and.b16  	%rs6, %rs5, 192;
	setp.ne.s16 	%p7, %rs6, 128;
	selp.u64 	%rd58, 1, 0, %p7;
	add.s64 	%rd59, %rd113, %rd58;
	ld.u8 	%rs7, [%rd109+1];
	and.b16  	%rs8, %rs7, 192;
	setp.ne.s16 	%p8, %rs8, 128;
	selp.u64 	%rd60, 1, 0, %p8;
	add.s64 	%rd61, %rd59, %rd60;
	ld.u8 	%rs9, [%rd109+2];
	and.b16  	%rs10, %rs9, 192;
	setp.ne.s16 	%p9, %rs10, 128;
	selp.u64 	%rd62, 1, 0, %p9;
	add.s64 	%rd63, %rd61, %rd62;
	ld.u8 	%rs11, [%rd109+3];
	and.b16  	%rs12, %rs11, 192;
	setp.ne.s16 	%p10, %rs12, 128;
	selp.u64 	%rd64, 1, 0, %p10;
	add.s64 	%rd113, %rd63, %rd64;
	add.s64 	%rd109, %rd109, 4;
	add.s64 	%rd106, %rd106, -4;
	setp.ne.s64 	%p11, %rd106, 0;
	@%p11 bra 	$L__BB19_4;

$L__BB19_5:
	setp.eq.s64 	%p12, %rd112, 0;
	@%p12 bra 	$L__BB19_7;

$L__BB19_6:
	.pragma "nounroll";
	ld.u8 	%rs13, [%rd109];
	and.b16  	%rs14, %rs13, 192;
	setp.ne.s16 	%p13, %rs14, 128;
	selp.u64 	%rd65, 1, 0, %p13;
	add.s64 	%rd113, %rd113, %rd65;
	add.s64 	%rd109, %rd109, 1;
	add.s64 	%rd112, %rd112, -1;
	setp.ne.s64 	%p14, %rd112, 0;
	@%p14 bra 	$L__BB19_6;

$L__BB19_7:
	cvt.u32.u64 	%r74, %rd113;

$L__BB19_8:
	setp.eq.s32 	%p15, %r31, 0;
	mov.u32 	%r34, 0;
	mov.u32 	%r82, %r34;
	@%p15 bra 	$L__BB19_43;

	cvt.u64.u32 	%rd66, %r31;
	and.b64  	%rd21, %rd66, 3;
	setp.eq.s64 	%p17, %rd1, 0;
	mov.u16 	%rs45, 0;
	cvt.s64.s32 	%rd86, %r31;
	add.s64 	%rd87, %rd86, -1;
	setp.lt.u64 	%p34, %rd87, 3;
	mov.u64 	%rd85, 0;
	setp.eq.s64 	%p25, %rd21, 0;
	mov.u16 	%rs50, %rs51;
	mov.u32 	%r75, %r34;
	mov.u32 	%r76, %r34;

$L__BB19_10:
	setp.ne.s32 	%p16, %r74, -1;
	or.pred  	%p18, %p16, %p17;
	selp.b32 	%r77, %r74, 0, %p16;
	@%p18 bra 	$L__BB19_19;

	mov.u64 	%rd120, 0;
	mov.u64 	%rd118, %rd1;
	@%p34 bra 	$L__BB19_14;

	and.b64  	%rd73, %rd86, 3;
	sub.s64 	%rd114, %rd73, %rd86;
	mov.u64 	%rd118, %rd1;

$L__BB19_13:
	ld.u8 	%rs15, [%rd118];
	and.b16  	%rs16, %rs15, 192;
	setp.ne.s16 	%p20, %rs16, 128;
	selp.u64 	%rd74, 1, 0, %p20;
	add.s64 	%rd75, %rd120, %rd74;
	ld.u8 	%rs17, [%rd118+1];
	and.b16  	%rs18, %rs17, 192;
	setp.ne.s16 	%p21, %rs18, 128;
	selp.u64 	%rd76, 1, 0, %p21;
	add.s64 	%rd77, %rd75, %rd76;
	ld.u8 	%rs19, [%rd118+2];
	and.b16  	%rs20, %rs19, 192;
	setp.ne.s16 	%p22, %rs20, 128;
	selp.u64 	%rd78, 1, 0, %p22;
	add.s64 	%rd79, %rd77, %rd78;
	ld.u8 	%rs21, [%rd118+3];
	and.b16  	%rs22, %rs21, 192;
	setp.ne.s16 	%p23, %rs22, 128;
	selp.u64 	%rd80, 1, 0, %p23;
	add.s64 	%rd120, %rd79, %rd80;
	add.s64 	%rd118, %rd118, 4;
	add.s64 	%rd114, %rd114, 4;
	setp.ne.s64 	%p24, %rd114, 0;
	@%p24 bra 	$L__BB19_13;

$L__BB19_14:
	@%p25 bra 	$L__BB19_18;

	setp.eq.s64 	%p26, %rd21, 1;
	ld.u8 	%rs23, [%rd118];
	and.b16  	%rs24, %rs23, 192;
	setp.ne.s16 	%p27, %rs24, 128;
	selp.u64 	%rd81, 1, 0, %p27;
	add.s64 	%rd120, %rd120, %rd81;
	@%p26 bra 	$L__BB19_18;

	setp.eq.s64 	%p28, %rd21, 2;
	ld.u8 	%rs25, [%rd118+1];
	and.b16  	%rs26, %rs25, 192;
	setp.ne.s16 	%p29, %rs26, 128;
	selp.u64 	%rd82, 1, 0, %p29;
	add.s64 	%rd120, %rd120, %rd82;
	@%p28 bra 	$L__BB19_18;

	ld.u8 	%rs27, [%rd118+2];
	and.b16  	%rs28, %rs27, 192;
	setp.ne.s16 	%p30, %rs28, 128;
	selp.u64 	%rd83, 1, 0, %p30;
	add.s64 	%rd120, %rd120, %rd83;

$L__BB19_18:
	cvt.u32.u64 	%r77, %rd120;

$L__BB19_19:
	setp.ne.s32 	%p31, %r77, -1;
	or.pred  	%p33, %p31, %p17;
	selp.b32 	%r74, %r77, 0, %p31;
	@%p33 bra 	$L__BB19_28;

	mov.u64 	%rd125, %rd1;
	mov.u64 	%rd127, %rd85;
	@%p34 bra 	$L__BB19_23;

	and.b64  	%rd90, %rd86, 3;
	sub.s64 	%rd123, %rd86, %rd90;
	mov.u64 	%rd125, %rd1;
	mov.u64 	%rd127, %rd85;

$L__BB19_22:
	ld.u8 	%rs29, [%rd125];
	and.b16  	%rs30, %rs29, 192;
	setp.ne.s16 	%p35, %rs30, 128;
	selp.u64 	%rd91, 1, 0, %p35;
	add.s64 	%rd92, %rd127, %rd91;
	ld.u8 	%rs31, [%rd125+1];
	and.b16  	%rs32, %rs31, 192;
	setp.ne.s16 	%p36, %rs32, 128;
	selp.u64 	%rd93, 1, 0, %p36;
	add.s64 	%rd94, %rd92, %rd93;
	ld.u8 	%rs33, [%rd125+2];
	and.b16  	%rs34, %rs33, 192;
	setp.ne.s16 	%p37, %rs34, 128;
	selp.u64 	%rd95, 1, 0, %p37;
	add.s64 	%rd96, %rd94, %rd95;
	ld.u8 	%rs35, [%rd125+3];
	and.b16  	%rs36, %rs35, 192;
	setp.ne.s16 	%p38, %rs36, 128;
	selp.u64 	%rd97, 1, 0, %p38;
	add.s64 	%rd127, %rd96, %rd97;
	add.s64 	%rd125, %rd125, 4;
	add.s64 	%rd123, %rd123, -4;
	setp.ne.s64 	%p39, %rd123, 0;
	@%p39 bra 	$L__BB19_22;

$L__BB19_23:
	@%p25 bra 	$L__BB19_27;

	setp.eq.s64 	%p41, %rd21, 1;
	ld.u8 	%rs37, [%rd125];
	and.b16  	%rs38, %rs37, 192;
	setp.ne.s16 	%p42, %rs38, 128;
	selp.u64 	%rd98, 1, 0, %p42;
	add.s64 	%rd127, %rd127, %rd98;
	@%p41 bra 	$L__BB19_27;

	setp.eq.s64 	%p43, %rd21, 2;
	ld.u8 	%rs39, [%rd125+1];
	and.b16  	%rs40, %rs39, 192;
	setp.ne.s16 	%p44, %rs40, 128;
	selp.u64 	%rd99, 1, 0, %p44;
	add.s64 	%rd127, %rd127, %rd99;
	@%p43 bra 	$L__BB19_27;

	ld.u8 	%rs41, [%rd125+2];
	and.b16  	%rs42, %rs41, 192;
	setp.ne.s16 	%p45, %rs42, 128;
	selp.u64 	%rd100, 1, 0, %p45;
	add.s64 	%rd127, %rd127, %rd100;

$L__BB19_27:
	cvt.u32.u64 	%r74, %rd127;

$L__BB19_28:
	setp.eq.s32 	%p46, %r75, %r77;
	mov.u16 	%rs51, %rs50;
	@%p46 bra 	$L__BB19_43;

	cvt.s64.s32 	%rd101, %r76;
	add.s64 	%rd50, %rd1, %rd101;
	ld.u8 	%rs43, [%rd50];
	setp.gt.u16 	%p47, %rs43, 239;
	selp.b32 	%r38, 2, 1, %p47;
	setp.gt.u16 	%p48, %rs43, 223;
	selp.u32 	%r39, 1, 0, %p48;
	add.s32 	%r40, %r38, %r39;
	setp.gt.u16 	%p49, %rs43, 191;
	selp.u32 	%r41, 1, 0, %p49;
	add.s32 	%r42, %r40, %r41;
	and.b16  	%rs44, %rs43, 192;
	setp.eq.s16 	%p50, %rs44, 128;
	selp.b32 	%r43, -1, 0, %p50;
	add.s32 	%r15, %r42, %r43;
	cvt.u32.u16 	%r79, %rs43;
	setp.lt.s32 	%p51, %r15, 2;
	@%p51 bra 	$L__BB19_33;

	ld.u8 	%r44, [%rd50+1];
	prmt.b32 	%r79, %r79, %r44, 8452;
	setp.eq.s32 	%p52, %r15, 2;
	@%p52 bra 	$L__BB19_33;

	ld.u8 	%r45, [%rd50+2];
	prmt.b32 	%r79, %r79, %r45, 8452;
	setp.lt.s32 	%p53, %r15, 4;
	@%p53 bra 	$L__BB19_33;

	ld.u8 	%r46, [%rd50+3];
	prmt.b32 	%r79, %r79, %r46, 8452;

$L__BB19_33:
	setp.lt.u32 	%p54, %r79, 128;
	mov.u32 	%r80, %r79;
	@%p54 bra 	$L__BB19_40;

	setp.lt.u32 	%p55, %r79, 57344;
	@%p55 bra 	$L__BB19_39;
	bra.uni 	$L__BB19_35;

$L__BB19_39:
	and.b32  	%r63, %r79, 7936;
	shr.u32 	%r64, %r63, 2;
	and.b32  	%r65, %r79, 63;
	or.b32  	%r80, %r64, %r65;
	bra.uni 	$L__BB19_40;

$L__BB19_35:
	setp.lt.u32 	%p56, %r79, 15728640;
	@%p56 bra 	$L__BB19_38;
	bra.uni 	$L__BB19_36;

$L__BB19_38:
	and.b32  	%r57, %r79, 983040;
	shr.u32 	%r58, %r57, 4;
	and.b32  	%r59, %r79, 16128;
	shr.u32 	%r60, %r59, 2;
	and.b32  	%r61, %r79, 63;
	or.b32  	%r62, %r60, %r61;
	or.b32  	%r80, %r62, %r58;
	bra.uni 	$L__BB19_40;

$L__BB19_36:
	setp.gt.u32 	%p57, %r79, -134217728;
	mov.u32 	%r80, 0;
	@%p57 bra 	$L__BB19_40;

	and.b32  	%r48, %r79, 50331648;
	shr.u32 	%r49, %r48, 6;
	and.b32  	%r50, %r79, 4128768;
	shr.u32 	%r51, %r50, 4;
	and.b32  	%r52, %r79, 16128;
	shr.u32 	%r53, %r52, 2;
	and.b32  	%r54, %r79, 63;
	or.b32  	%r55, %r51, %r54;
	or.b32  	%r56, %r55, %r49;
	or.b32  	%r80, %r56, %r53;

$L__BB19_40:
	setp.gt.u32 	%p58, %r80, 65535;
	mov.u32 	%r81, 0;
	@%p58 bra 	$L__BB19_42;

	cvt.u64.u32 	%rd102, %r80;
	add.s64 	%rd103, %rd52, %rd102;
	ld.u8 	%r81, [%rd103];

$L__BB19_42:
	and.b32  	%r67, %r81, 127;
	setp.ne.s32 	%p59, %r67, 0;
	setp.eq.s32 	%p60, %r81, 0;
	or.pred  	%p61, %p60, %p59;
	bfe.u32 	%r68, %r81, 3, 1;
	cvt.u16.u32 	%rs46, %r68;
	selp.b16 	%rs50, %rs46, %rs50, %p61;
	and.b16  	%rs47, %rs50, 255;
	selp.u32 	%r69, 1, 0, %p61;
	add.s32 	%r82, %r82, %r69;
	add.s32 	%r70, %r15, %r76;
	setp.lt.s32 	%p62, %r76, %r31;
	selp.b32 	%r76, %r70, %r76, %p62;
	add.s32 	%r75, %r75, 1;
	setp.ne.s16 	%p63, %rs47, 0;
	mov.u16 	%rs51, %rs45;
	@%p63 bra 	$L__BB19_10;

$L__BB19_43:
	setp.ne.s32 	%p64, %r82, 0;
	and.b16  	%rs48, %rs51, 255;
	setp.ne.s16 	%p65, %rs48, 0;
	and.pred  	%p66, %p65, %p64;
	selp.u16 	%rs49, 1, 0, %p66;
	st.u8 	[%rd51], %rs49;
	st.param.b32 	[func_retval0+0], %r34;
	ret;

}
	// .globl	pycount
.visible .func  (.param .b32 func_retval0) pycount(
	.param .b64 pycount_param_0,
	.param .b64 pycount_param_1,
	.param .b64 pycount_param_2
)
{
	.reg .pred 	%p<141>;
	.reg .b16 	%rs<101>;
	.reg .b32 	%r<109>;
	.reg .b64 	%rd<279>;


	ld.param.u64 	%rd124, [pycount_param_0];
	ld.param.u64 	%rd125, [pycount_param_1];
	ld.param.u64 	%rd126, [pycount_param_2];
	ld.u64 	%rd1, [%rd125];
	ld.v2.u32 	{%r47, %r90}, [%rd125+8];
	ld.u64 	%rd2, [%rd126];
	ld.v2.u32 	{%r49, %r89}, [%rd126+8];
	setp.ne.s32 	%p2, %r89, -1;
	@%p2 bra 	$L__BB20_9;

	setp.eq.s64 	%p3, %rd2, 0;
	setp.eq.s32 	%p4, %r49, 0;
	mov.u32 	%r89, 0;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB20_9;

	cvt.s64.s32 	%rd3, %r49;
	add.s64 	%rd129, %rd3, -1;
	and.b64  	%rd227, %rd3, 3;
	setp.lt.u64 	%p6, %rd129, 3;
	mov.u64 	%rd228, 0;
	mov.u64 	%rd224, %rd2;
	@%p6 bra 	$L__BB20_5;

	sub.s64 	%rd221, %rd3, %rd227;
	mov.u64 	%rd224, %rd2;

$L__BB20_4:
	ld.u8 	%rs5, [%rd224];
	and.b16  	%rs6, %rs5, 192;
	setp.ne.s16 	%p7, %rs6, 128;
	selp.u64 	%rd131, 1, 0, %p7;
	add.s64 	%rd132, %rd228, %rd131;
	ld.u8 	%rs7, [%rd224+1];
	and.b16  	%rs8, %rs7, 192;
	setp.ne.s16 	%p8, %rs8, 128;
	selp.u64 	%rd133, 1, 0, %p8;
	add.s64 	%rd134, %rd132, %rd133;
	ld.u8 	%rs9, [%rd224+2];
	and.b16  	%rs10, %rs9, 192;
	setp.ne.s16 	%p9, %rs10, 128;
	selp.u64 	%rd135, 1, 0, %p9;
	add.s64 	%rd136, %rd134, %rd135;
	ld.u8 	%rs11, [%rd224+3];
	and.b16  	%rs12, %rs11, 192;
	setp.ne.s16 	%p10, %rs12, 128;
	selp.u64 	%rd137, 1, 0, %p10;
	add.s64 	%rd228, %rd136, %rd137;
	add.s64 	%rd224, %rd224, 4;
	add.s64 	%rd221, %rd221, -4;
	setp.ne.s64 	%p11, %rd221, 0;
	@%p11 bra 	$L__BB20_4;

$L__BB20_5:
	setp.eq.s64 	%p12, %rd227, 0;
	@%p12 bra 	$L__BB20_8;

$L__BB20_7:
	.pragma "nounroll";
	ld.u8 	%rs13, [%rd224];
	and.b16  	%rs14, %rs13, 192;
	setp.ne.s16 	%p13, %rs14, 128;
	selp.u64 	%rd138, 1, 0, %p13;
	add.s64 	%rd228, %rd228, %rd138;
	add.s64 	%rd224, %rd224, 1;
	add.s64 	%rd227, %rd227, -1;
	setp.ne.s64 	%p14, %rd227, 0;
	@%p14 bra 	$L__BB20_7;

$L__BB20_8:
	cvt.u32.u64 	%r89, %rd228;

$L__BB20_9:
	setp.eq.s32 	%p15, %r89, 0;
	mov.u32 	%r52, 0;
	mov.u32 	%r108, %r52;
	@%p15 bra 	$L__BB20_84;

	setp.ne.s32 	%p16, %r90, -1;
	@%p16 bra 	$L__BB20_19;

	setp.eq.s64 	%p17, %rd1, 0;
	setp.eq.s32 	%p18, %r47, 0;
	mov.u32 	%r90, 0;
	or.pred  	%p19, %p17, %p18;
	@%p19 bra 	$L__BB20_19;

	cvt.s64.s32 	%rd22, %r47;
	add.s64 	%rd141, %rd22, -1;
	and.b64  	%rd237, %rd22, 3;
	setp.lt.u64 	%p20, %rd141, 3;
	mov.u64 	%rd238, 0;
	mov.u64 	%rd234, %rd1;
	@%p20 bra 	$L__BB20_15;

	sub.s64 	%rd231, %rd22, %rd237;
	mov.u64 	%rd234, %rd1;

$L__BB20_14:
	ld.u8 	%rs15, [%rd234];
	and.b16  	%rs16, %rs15, 192;
	setp.ne.s16 	%p21, %rs16, 128;
	selp.u64 	%rd143, 1, 0, %p21;
	add.s64 	%rd144, %rd238, %rd143;
	ld.u8 	%rs17, [%rd234+1];
	and.b16  	%rs18, %rs17, 192;
	setp.ne.s16 	%p22, %rs18, 128;
	selp.u64 	%rd145, 1, 0, %p22;
	add.s64 	%rd146, %rd144, %rd145;
	ld.u8 	%rs19, [%rd234+2];
	and.b16  	%rs20, %rs19, 192;
	setp.ne.s16 	%p23, %rs20, 128;
	selp.u64 	%rd147, 1, 0, %p23;
	add.s64 	%rd148, %rd146, %rd147;
	ld.u8 	%rs21, [%rd234+3];
	and.b16  	%rs22, %rs21, 192;
	setp.ne.s16 	%p24, %rs22, 128;
	selp.u64 	%rd149, 1, 0, %p24;
	add.s64 	%rd238, %rd148, %rd149;
	add.s64 	%rd234, %rd234, 4;
	add.s64 	%rd231, %rd231, -4;
	setp.ne.s64 	%p25, %rd231, 0;
	@%p25 bra 	$L__BB20_14;

$L__BB20_15:
	setp.eq.s64 	%p26, %rd237, 0;
	@%p26 bra 	$L__BB20_18;

$L__BB20_17:
	.pragma "nounroll";
	ld.u8 	%rs23, [%rd234];
	and.b16  	%rs24, %rs23, 192;
	setp.ne.s16 	%p27, %rs24, 128;
	selp.u64 	%rd150, 1, 0, %p27;
	add.s64 	%rd238, %rd238, %rd150;
	add.s64 	%rd234, %rd234, 1;
	add.s64 	%rd237, %rd237, -1;
	setp.ne.s64 	%p28, %rd237, 0;
	@%p28 bra 	$L__BB20_17;

$L__BB20_18:
	cvt.u32.u64 	%r90, %rd238;

$L__BB20_19:
	setp.eq.s32 	%p29, %r47, 0;
	setp.eq.s64 	%p30, %rd1, 0;
	or.pred  	%p1, %p30, %p29;
	cvt.s64.s32 	%rd151, %r47;
	add.s64 	%rd41, %rd1, %rd151;
	add.s64 	%rd42, %rd151, -1;
	and.b64  	%rd43, %rd151, 3;
	sub.s64 	%rd44, %rd151, %rd43;
	setp.eq.s64 	%p32, %rd2, 0;
	mov.u32 	%r56, -1;
	mov.u16 	%rs71, 1;
	mov.u32 	%r91, %r52;
	mov.u32 	%r108, %r52;
	mov.u32 	%r106, %r90;

$L__BB20_20:
	setp.lt.s32 	%p31, %r91, 0;
	or.pred  	%p33, %p32, %p31;
	mov.u32 	%r107, %r56;
	@%p33 bra 	$L__BB20_83;

	setp.ne.s32 	%p34, %r106, -1;
	or.pred  	%p35, %p34, %p1;
	selp.b32 	%r94, %r106, 0, %p34;
	@%p35 bra 	$L__BB20_30;

	setp.lt.u64 	%p36, %rd42, 3;
	mov.u64 	%rd245, 0;
	mov.u64 	%rd244, %rd1;
	@%p36 bra 	$L__BB20_25;

	mov.u64 	%rd244, %rd1;
	mov.u64 	%rd241, %rd44;

$L__BB20_24:
	ld.u8 	%rs25, [%rd244];
	and.b16  	%rs26, %rs25, 192;
	setp.ne.s16 	%p37, %rs26, 128;
	selp.u64 	%rd155, 1, 0, %p37;
	add.s64 	%rd156, %rd245, %rd155;
	ld.u8 	%rs27, [%rd244+1];
	and.b16  	%rs28, %rs27, 192;
	setp.ne.s16 	%p38, %rs28, 128;
	selp.u64 	%rd157, 1, 0, %p38;
	add.s64 	%rd158, %rd156, %rd157;
	ld.u8 	%rs29, [%rd244+2];
	and.b16  	%rs30, %rs29, 192;
	setp.ne.s16 	%p39, %rs30, 128;
	selp.u64 	%rd159, 1, 0, %p39;
	add.s64 	%rd160, %rd158, %rd159;
	ld.u8 	%rs31, [%rd244+3];
	and.b16  	%rs32, %rs31, 192;
	setp.ne.s16 	%p40, %rs32, 128;
	selp.u64 	%rd161, 1, 0, %p40;
	add.s64 	%rd245, %rd160, %rd161;
	add.s64 	%rd244, %rd244, 4;
	add.s64 	%rd241, %rd241, -4;
	setp.ne.s64 	%p41, %rd241, 0;
	@%p41 bra 	$L__BB20_24;

$L__BB20_25:
	setp.eq.s64 	%p42, %rd43, 0;
	@%p42 bra 	$L__BB20_29;

	setp.eq.s64 	%p43, %rd43, 1;
	ld.u8 	%rs33, [%rd244];
	and.b16  	%rs34, %rs33, 192;
	setp.ne.s16 	%p44, %rs34, 128;
	selp.u64 	%rd162, 1, 0, %p44;
	add.s64 	%rd245, %rd245, %rd162;
	@%p43 bra 	$L__BB20_29;

	setp.eq.s64 	%p45, %rd43, 2;
	ld.u8 	%rs35, [%rd244+1];
	and.b16  	%rs36, %rs35, 192;
	setp.ne.s16 	%p46, %rs36, 128;
	selp.u64 	%rd163, 1, 0, %p46;
	add.s64 	%rd245, %rd245, %rd163;
	@%p45 bra 	$L__BB20_29;

	ld.u8 	%rs37, [%rd244+2];
	and.b16  	%rs38, %rs37, 192;
	setp.ne.s16 	%p47, %rs38, 128;
	selp.u64 	%rd164, 1, 0, %p47;
	add.s64 	%rd245, %rd245, %rd164;

$L__BB20_29:
	cvt.u32.u64 	%r94, %rd245;

$L__BB20_30:
	setp.ne.s32 	%p48, %r94, -1;
	or.pred  	%p49, %p48, %p1;
	selp.b32 	%r95, %r94, 0, %p48;
	@%p49 bra 	$L__BB20_39;

	setp.lt.u64 	%p50, %rd42, 3;
	mov.u64 	%rd252, 0;
	mov.u64 	%rd250, %rd1;
	@%p50 bra 	$L__BB20_34;

	mov.u64 	%rd250, %rd1;
	mov.u64 	%rd248, %rd44;

$L__BB20_33:
	ld.u8 	%rs39, [%rd250];
	and.b16  	%rs40, %rs39, 192;
	setp.ne.s16 	%p51, %rs40, 128;
	selp.u64 	%rd168, 1, 0, %p51;
	add.s64 	%rd169, %rd252, %rd168;
	ld.u8 	%rs41, [%rd250+1];
	and.b16  	%rs42, %rs41, 192;
	setp.ne.s16 	%p52, %rs42, 128;
	selp.u64 	%rd170, 1, 0, %p52;
	add.s64 	%rd171, %rd169, %rd170;
	ld.u8 	%rs43, [%rd250+2];
	and.b16  	%rs44, %rs43, 192;
	setp.ne.s16 	%p53, %rs44, 128;
	selp.u64 	%rd172, 1, 0, %p53;
	add.s64 	%rd173, %rd171, %rd172;
	ld.u8 	%rs45, [%rd250+3];
	and.b16  	%rs46, %rs45, 192;
	setp.ne.s16 	%p54, %rs46, 128;
	selp.u64 	%rd174, 1, 0, %p54;
	add.s64 	%rd252, %rd173, %rd174;
	add.s64 	%rd250, %rd250, 4;
	add.s64 	%rd248, %rd248, -4;
	setp.ne.s64 	%p55, %rd248, 0;
	@%p55 bra 	$L__BB20_33;

$L__BB20_34:
	setp.eq.s64 	%p56, %rd43, 0;
	@%p56 bra 	$L__BB20_38;

	setp.eq.s64 	%p57, %rd43, 1;
	ld.u8 	%rs47, [%rd250];
	and.b16  	%rs48, %rs47, 192;
	setp.ne.s16 	%p58, %rs48, 128;
	selp.u64 	%rd175, 1, 0, %p58;
	add.s64 	%rd252, %rd252, %rd175;
	@%p57 bra 	$L__BB20_38;

	setp.eq.s64 	%p59, %rd43, 2;
	ld.u8 	%rs49, [%rd250+1];
	and.b16  	%rs50, %rs49, 192;
	setp.ne.s16 	%p60, %rs50, 128;
	selp.u64 	%rd176, 1, 0, %p60;
	add.s64 	%rd252, %rd252, %rd176;
	@%p59 bra 	$L__BB20_38;

	ld.u8 	%rs51, [%rd250+2];
	and.b16  	%rs52, %rs51, 192;
	setp.ne.s16 	%p61, %rs52, 128;
	selp.u64 	%rd177, 1, 0, %p61;
	add.s64 	%rd252, %rd252, %rd177;

$L__BB20_38:
	cvt.u32.u64 	%r95, %rd252;

$L__BB20_39:
	setp.eq.s32 	%p62, %r95, %r47;
	mov.u32 	%r98, %r91;
	@%p62 bra 	$L__BB20_43;

	setp.lt.s32 	%p63, %r47, 1;
	mov.u32 	%r98, 0;
	setp.lt.s32 	%p64, %r91, 1;
	or.pred  	%p65, %p64, %p63;
	@%p65 bra 	$L__BB20_43;

	mov.u64 	%rd253, %rd1;
	mov.u32 	%r96, %r91;

$L__BB20_42:
	ld.u8 	%rs53, [%rd253];
	setp.gt.u16 	%p66, %rs53, 239;
	selp.b32 	%r59, 2, 1, %p66;
	setp.gt.u16 	%p67, %rs53, 223;
	selp.u32 	%r60, 1, 0, %p67;
	add.s32 	%r61, %r59, %r60;
	setp.gt.u16 	%p68, %rs53, 191;
	selp.u32 	%r62, 1, 0, %p68;
	add.s32 	%r63, %r61, %r62;
	and.b16  	%rs54, %rs53, 192;
	setp.eq.s16 	%p69, %rs54, 128;
	selp.b32 	%r64, -1, 0, %p69;
	add.s32 	%r65, %r63, %r64;
	setp.ne.s32 	%p70, %r65, 0;
	selp.b32 	%r66, -1, 0, %p70;
	add.s32 	%r96, %r96, %r66;
	add.s32 	%r98, %r65, %r98;
	setp.gt.s32 	%p71, %r96, 0;
	add.s64 	%rd253, %rd253, 1;
	setp.lt.u64 	%p72, %rd253, %rd41;
	and.pred  	%p73, %p71, %p72;
	@%p73 bra 	$L__BB20_42;

$L__BB20_43:
	sub.s32 	%r67, %r90, %r91;
	setp.lt.s32 	%p74, %r67, 0;
	selp.b32 	%r68, %r94, %r67, %p74;
	add.s32 	%r69, %r68, %r91;
	min.s32 	%r100, %r94, %r69;
	setp.ne.s32 	%p75, %r95, -1;
	or.pred  	%p76, %p75, %p1;
	selp.b32 	%r106, %r95, 0, %p75;
	@%p76 bra 	$L__BB20_52;

	setp.lt.u64 	%p77, %rd42, 3;
	mov.u64 	%rd260, 0;
	mov.u64 	%rd258, %rd1;
	@%p77 bra 	$L__BB20_47;

	mov.u64 	%rd258, %rd1;
	mov.u64 	%rd256, %rd44;

$L__BB20_46:
	ld.u8 	%rs55, [%rd258];
	and.b16  	%rs56, %rs55, 192;
	setp.ne.s16 	%p78, %rs56, 128;
	selp.u64 	%rd181, 1, 0, %p78;
	add.s64 	%rd182, %rd260, %rd181;
	ld.u8 	%rs57, [%rd258+1];
	and.b16  	%rs58, %rs57, 192;
	setp.ne.s16 	%p79, %rs58, 128;
	selp.u64 	%rd183, 1, 0, %p79;
	add.s64 	%rd184, %rd182, %rd183;
	ld.u8 	%rs59, [%rd258+2];
	and.b16  	%rs60, %rs59, 192;
	setp.ne.s16 	%p80, %rs60, 128;
	selp.u64 	%rd185, 1, 0, %p80;
	add.s64 	%rd186, %rd184, %rd185;
	ld.u8 	%rs61, [%rd258+3];
	and.b16  	%rs62, %rs61, 192;
	setp.ne.s16 	%p81, %rs62, 128;
	selp.u64 	%rd187, 1, 0, %p81;
	add.s64 	%rd260, %rd186, %rd187;
	add.s64 	%rd258, %rd258, 4;
	add.s64 	%rd256, %rd256, -4;
	setp.ne.s64 	%p82, %rd256, 0;
	@%p82 bra 	$L__BB20_46;

$L__BB20_47:
	setp.eq.s64 	%p83, %rd43, 0;
	@%p83 bra 	$L__BB20_51;

	setp.eq.s64 	%p84, %rd43, 1;
	ld.u8 	%rs63, [%rd258];
	and.b16  	%rs64, %rs63, 192;
	setp.ne.s16 	%p85, %rs64, 128;
	selp.u64 	%rd188, 1, 0, %p85;
	add.s64 	%rd260, %rd260, %rd188;
	@%p84 bra 	$L__BB20_51;

	setp.eq.s64 	%p86, %rd43, 2;
	ld.u8 	%rs65, [%rd258+1];
	and.b16  	%rs66, %rs65, 192;
	setp.ne.s16 	%p87, %rs66, 128;
	selp.u64 	%rd189, 1, 0, %p87;
	add.s64 	%rd260, %rd260, %rd189;
	@%p86 bra 	$L__BB20_51;

	ld.u8 	%rs67, [%rd258+2];
	and.b16  	%rs68, %rs67, 192;
	setp.ne.s16 	%p88, %rs68, 128;
	selp.u64 	%rd190, 1, 0, %p88;
	add.s64 	%rd260, %rd260, %rd190;

$L__BB20_51:
	cvt.u32.u64 	%r106, %rd260;

$L__BB20_52:
	setp.eq.s32 	%p89, %r106, %r47;
	mov.u32 	%r102, %r100;
	@%p89 bra 	$L__BB20_56;

	setp.lt.s32 	%p90, %r47, 1;
	mov.u32 	%r102, 0;
	setp.lt.s32 	%p91, %r100, 1;
	or.pred  	%p92, %p91, %p90;
	@%p92 bra 	$L__BB20_56;

	mov.u64 	%rd261, %rd1;

$L__BB20_55:
	ld.u8 	%rs69, [%rd261];
	setp.gt.u16 	%p93, %rs69, 239;
	selp.b32 	%r72, 2, 1, %p93;
	setp.gt.u16 	%p94, %rs69, 223;
	selp.u32 	%r73, 1, 0, %p94;
	add.s32 	%r74, %r72, %r73;
	setp.gt.u16 	%p95, %rs69, 191;
	selp.u32 	%r75, 1, 0, %p95;
	add.s32 	%r76, %r74, %r75;
	and.b16  	%rs70, %rs69, 192;
	setp.eq.s16 	%p96, %rs70, 128;
	selp.b32 	%r77, -1, 0, %p96;
	add.s32 	%r78, %r76, %r77;
	setp.ne.s32 	%p97, %r78, 0;
	selp.b32 	%r79, -1, 0, %p97;
	add.s32 	%r100, %r100, %r79;
	add.s32 	%r102, %r78, %r102;
	setp.gt.s32 	%p98, %r100, 0;
	add.s64 	%rd261, %rd261, 1;
	setp.lt.u64 	%p99, %rd261, %rd41;
	and.pred  	%p100, %p98, %p99;
	@%p100 bra 	$L__BB20_55;

$L__BB20_56:
	add.s32 	%r81, %r98, %r49;
	sub.s32 	%r32, %r102, %r81;
	cvt.s64.s32 	%rd191, %r98;
	add.s64 	%rd262, %rd1, %rd191;
	setp.lt.s32 	%p101, %r32, 0;
	mov.u32 	%r107, %r56;
	@%p101 bra 	$L__BB20_83;

	mov.u32 	%r103, 0;

$L__BB20_58:
	setp.lt.s32 	%p102, %r49, 1;
	mov.u16 	%rs100, %rs71;
	@%p102 bra 	$L__BB20_62;

	mov.u32 	%r104, 1;
	mov.u64 	%rd263, %rd262;
	mov.u64 	%rd264, %rd2;

$L__BB20_60:
	ld.u8 	%rs1, [%rd264];
	ld.u8 	%rs2, [%rd263];
	setp.eq.s16 	%p103, %rs2, %rs1;
	setp.lt.s32 	%p104, %r104, %r49;
	and.pred  	%p105, %p103, %p104;
	add.s64 	%rd264, %rd264, 1;
	add.s64 	%rd263, %rd263, 1;
	add.s32 	%r104, %r104, 1;
	@%p105 bra 	$L__BB20_60;

	selp.u16 	%rs100, 1, 0, %p103;

$L__BB20_62:
	setp.eq.s16 	%p107, %rs100, 0;
	@%p107 bra 	$L__BB20_82;
	bra.uni 	$L__BB20_63;

$L__BB20_82:
	add.s64 	%rd262, %rd262, 1;
	add.s32 	%r41, %r103, 1;
	setp.lt.s32 	%p138, %r103, %r32;
	mov.u32 	%r103, %r41;
	mov.u32 	%r107, %r56;
	@%p138 bra 	$L__BB20_58;
	bra.uni 	$L__BB20_83;

$L__BB20_63:
	add.s32 	%r36, %r103, %r98;
	setp.ne.s32 	%p108, %r106, -1;
	or.pred  	%p109, %p108, %p1;
	selp.b32 	%r105, %r106, 0, %p108;
	@%p109 bra 	$L__BB20_72;

	setp.lt.u64 	%p110, %rd42, 3;
	mov.u64 	%rd271, 0;
	mov.u64 	%rd269, %rd1;
	@%p110 bra 	$L__BB20_67;

	mov.u64 	%rd269, %rd1;
	mov.u64 	%rd267, %rd44;

$L__BB20_66:
	ld.u8 	%rs72, [%rd269];
	and.b16  	%rs73, %rs72, 192;
	setp.ne.s16 	%p111, %rs73, 128;
	selp.u64 	%rd195, 1, 0, %p111;
	add.s64 	%rd196, %rd271, %rd195;
	ld.u8 	%rs74, [%rd269+1];
	and.b16  	%rs75, %rs74, 192;
	setp.ne.s16 	%p112, %rs75, 128;
	selp.u64 	%rd197, 1, 0, %p112;
	add.s64 	%rd198, %rd196, %rd197;
	ld.u8 	%rs76, [%rd269+2];
	and.b16  	%rs77, %rs76, 192;
	setp.ne.s16 	%p113, %rs77, 128;
	selp.u64 	%rd199, 1, 0, %p113;
	add.s64 	%rd200, %rd198, %rd199;
	ld.u8 	%rs78, [%rd269+3];
	and.b16  	%rs79, %rs78, 192;
	setp.ne.s16 	%p114, %rs79, 128;
	selp.u64 	%rd201, 1, 0, %p114;
	add.s64 	%rd271, %rd200, %rd201;
	add.s64 	%rd269, %rd269, 4;
	add.s64 	%rd267, %rd267, -4;
	setp.ne.s64 	%p115, %rd267, 0;
	@%p115 bra 	$L__BB20_66;

$L__BB20_67:
	setp.eq.s64 	%p116, %rd43, 0;
	@%p116 bra 	$L__BB20_71;

	setp.eq.s64 	%p117, %rd43, 1;
	ld.u8 	%rs80, [%rd269];
	and.b16  	%rs81, %rs80, 192;
	setp.ne.s16 	%p118, %rs81, 128;
	selp.u64 	%rd202, 1, 0, %p118;
	add.s64 	%rd271, %rd271, %rd202;
	@%p117 bra 	$L__BB20_71;

	setp.eq.s64 	%p119, %rd43, 2;
	ld.u8 	%rs82, [%rd269+1];
	and.b16  	%rs83, %rs82, 192;
	setp.ne.s16 	%p120, %rs83, 128;
	selp.u64 	%rd203, 1, 0, %p120;
	add.s64 	%rd271, %rd271, %rd203;
	@%p119 bra 	$L__BB20_71;

	ld.u8 	%rs84, [%rd269+2];
	and.b16  	%rs85, %rs84, 192;
	setp.ne.s16 	%p121, %rs85, 128;
	selp.u64 	%rd204, 1, 0, %p121;
	add.s64 	%rd271, %rd271, %rd204;

$L__BB20_71:
	cvt.u32.u64 	%r105, %rd271;

$L__BB20_72:
	setp.eq.s32 	%p122, %r105, %r47;
	mov.u32 	%r106, %r47;
	mov.u32 	%r107, %r36;
	@%p122 bra 	$L__BB20_83;

	setp.eq.s32 	%p124, %r36, 0;
	mov.u32 	%r107, 0;
	or.pred  	%p125, %p30, %p124;
	mov.u32 	%r106, %r105;
	@%p125 bra 	$L__BB20_83;

	cvt.s64.s32 	%rd107, %r36;
	add.s64 	%rd207, %rd107, -1;
	and.b64  	%rd108, %rd107, 3;
	setp.lt.u64 	%p126, %rd207, 3;
	mov.u64 	%rd278, 0;
	mov.u64 	%rd276, %rd1;
	@%p126 bra 	$L__BB20_77;

	sub.s64 	%rd274, %rd107, %rd108;
	mov.u64 	%rd276, %rd1;

$L__BB20_76:
	ld.u8 	%rs86, [%rd276];
	and.b16  	%rs87, %rs86, 192;
	setp.ne.s16 	%p127, %rs87, 128;
	selp.u64 	%rd209, 1, 0, %p127;
	add.s64 	%rd210, %rd278, %rd209;
	ld.u8 	%rs88, [%rd276+1];
	and.b16  	%rs89, %rs88, 192;
	setp.ne.s16 	%p128, %rs89, 128;
	selp.u64 	%rd211, 1, 0, %p128;
	add.s64 	%rd212, %rd210, %rd211;
	ld.u8 	%rs90, [%rd276+2];
	and.b16  	%rs91, %rs90, 192;
	setp.ne.s16 	%p129, %rs91, 128;
	selp.u64 	%rd213, 1, 0, %p129;
	add.s64 	%rd214, %rd212, %rd213;
	ld.u8 	%rs92, [%rd276+3];
	and.b16  	%rs93, %rs92, 192;
	setp.ne.s16 	%p130, %rs93, 128;
	selp.u64 	%rd215, 1, 0, %p130;
	add.s64 	%rd278, %rd214, %rd215;
	add.s64 	%rd276, %rd276, 4;
	add.s64 	%rd274, %rd274, -4;
	setp.ne.s64 	%p131, %rd274, 0;
	@%p131 bra 	$L__BB20_76;

$L__BB20_77:
	setp.eq.s64 	%p132, %rd108, 0;
	@%p132 bra 	$L__BB20_81;

	ld.u8 	%rs94, [%rd276];
	and.b16  	%rs95, %rs94, 192;
	setp.ne.s16 	%p133, %rs95, 128;
	selp.u64 	%rd216, 1, 0, %p133;
	add.s64 	%rd278, %rd278, %rd216;
	setp.eq.s64 	%p134, %rd108, 1;
	@%p134 bra 	$L__BB20_81;

	ld.u8 	%rs96, [%rd276+1];
	and.b16  	%rs97, %rs96, 192;
	setp.ne.s16 	%p135, %rs97, 128;
	selp.u64 	%rd217, 1, 0, %p135;
	add.s64 	%rd278, %rd278, %rd217;
	setp.eq.s64 	%p136, %rd108, 2;
	@%p136 bra 	$L__BB20_81;

	ld.u8 	%rs98, [%rd276+2];
	and.b16  	%rs99, %rs98, 192;
	setp.ne.s16 	%p137, %rs99, 128;
	selp.u64 	%rd218, 1, 0, %p137;
	add.s64 	%rd278, %rd278, %rd218;

$L__BB20_81:
	cvt.u32.u64 	%r107, %rd278;
	mov.u32 	%r106, %r105;

$L__BB20_83:
	setp.ne.s32 	%p139, %r107, -1;
	selp.u32 	%r86, 1, 0, %p139;
	add.s32 	%r108, %r108, %r86;
	add.s32 	%r87, %r107, %r89;
	selp.b32 	%r91, %r87, -1, %p139;
	setp.ne.s32 	%p140, %r91, -1;
	@%p140 bra 	$L__BB20_20;

$L__BB20_84:
	st.u32 	[%rd124], %r108;
	st.param.b32 	[func_retval0+0], %r52;
	ret;

}
	// .weak	_ZN3cub11EmptyKernelIvEEvv
.weak .entry _ZN3cub11EmptyKernelIvEEvv()
{



	ret;

}

